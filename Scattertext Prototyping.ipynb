{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pygal\n",
    "import scattertext as st\n",
    "import imp\n",
    "#imp.reload('scattertext')\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from scipy.stats import rankdata, hmean\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convention_df = st.SampleCorpora.ConventionData2012.get_data()\n",
    "convention_df['parsed'] = convention_df['text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromParsedDocuments(convention_df, category_col='party', parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "\n",
    "class OLSUngarStyle(object):\n",
    "\t@staticmethod    \n",
    "\tdef get_scores_and_p_values(tdm, category):\n",
    "\t\t'''\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\ttdm: TermDocMatrix\n",
    "\t\tcategory: str, category name\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpd.DataFrame(['coef', 'p-val'])\n",
    "\t\t'''\n",
    "\t\tX = tdm._X\n",
    "\t\ty = (tdm.get_category_names_by_row() == category).astype(int) * 2 - 1.\n",
    "\t\tpX = X/X.sum(axis=1)\n",
    "\t\tansX = pX.copy()\n",
    "\t\tansX = 2 * np.sqrt(np.array(ansX) + 3./8)\n",
    "\t\tres = OLS(y, ansX).fit()\n",
    "\t\treturn res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = corpus._X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'democrat'\n",
    "y = (corpus.get_category_names_by_row() == category).astype(int) * 2 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ansX = 2* np.sqrt(X/X.sum(axis=1) + 3./8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((189,), (189, 62257))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, ansX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from statsmodels.regression.linear_model import OLS\n",
    "#res = OLS(y, 2* np.sqrt(pX + 3./8)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "reg = OLS(y, ansX).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps = reg.pvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#def zero_rank(data):\n",
    "\n",
    "data = np.array([5, -2, 3, 10, 16, 1, -1])\n",
    "neg_rank = scaled_rank(data[data < 0])*0.5\n",
    "pos_ranks = scaled_rank(data[data >= 0])*0.5 + 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaled_rank(data):\n",
    "    return scaled_frequency(rankdata(data, 'ordinal'))\n",
    "\n",
    "def rank_zero_at_half(data):\n",
    "    data_out = np.copy(data)\n",
    "    data_out[data < 0] = scaled_rank(data[data < 0])*0.5\n",
    "    data_out[data >= 0] = scaled_rank(data[data >= 0])*0.5 + 0.5\n",
    "    return data_out\n",
    "\n",
    "def scaled_frequency(ranks):\n",
    "    return (ranks.astype(float) - ranks.min())/(ranks.max() - ranks.min())\n",
    "\n",
    "def scaled_log_freq(ranks):\n",
    "    ranks = np.log(ranks+1)\n",
    "    return (ranks.astype(float) - ranks.min())/(ranks.max() - ranks.min())\n",
    "\n",
    "def find_most_common_pos(parsed_col, corpus, word):\n",
    "    c = Counter()\n",
    "    for parsed in corpus.search(word)['parsed']:\n",
    "        for w in parsed:\n",
    "            if w.lower_ == word:\n",
    "                c[w.pos_] += 1\n",
    "    return c.most_common(1)[0][0]\n",
    "\n",
    "def get_should_be_there_but_isnt(corpus, \n",
    "                                 min_background_corpus_rank= 0.9, \n",
    "                                 min_corpus_rank = 0.5, \n",
    "                                 max_background_rank = 0.999, \n",
    "                                 min_similarity_rank = 0.7,\n",
    "                                 size_of_distictive_set = 10, \n",
    "                                 filter_proper_nouns = True):\n",
    "    assert type(corpus) == st.ParsedCorpus\n",
    "    unigram_corpus = corpus.get_unigram_corpus()\n",
    "    background_freq_df = unigram_corpus.get_scaled_f_scores_vs_background()\n",
    "    background_freq_df = background_freq_df.loc[unigram_corpus.get_term_freq_df().index].fillna(0)\n",
    "    background_freq_df['corpus_rank'] = scaled_rank(background_freq_df['corpus'])\n",
    "    background_freq_df['background_rank'] = scaled_rank(background_freq_df['background'])\n",
    "    most_distinctive_terms = ([nlp(term) \n",
    "                              for term \n",
    "                              in (background_freq_df[background_freq_df['background'] > 0]\n",
    "                                  .sort_values(by='Scaled f-score', ascending=False)\n",
    "                                  .iloc[:size_of_distictive_set*10]\n",
    "                                  .index)\n",
    "                              if not filter_proper_nouns \n",
    "                               or find_most_common_pos('parsed', corpus, term) != 'PROPN']\n",
    "                              [:size_of_distictive_set])\n",
    "    background_freq_df['semantic_similarity'] = [np.max([nlp(term).similarity(distinctive_term) for distinctive_term in most_distinctive_terms]) \n",
    "                                                 for term in background_freq_df.index]\n",
    "    background_freq_df['semantic_similarity_rank'] = ((rankdata(background_freq_df['semantic_similarity'])+1.)/(len(background_freq_df)+1.))\n",
    "\n",
    "    new_stop_list = (set(background_freq_df[((background_freq_df['background_rank'] < min_background_corpus_rank)\n",
    "                                             & (background_freq_df['corpus_rank'] < min_corpus_rank))].index) \n",
    "                     | set(t for t in background_freq_df.index if len(t) == 1)\n",
    "                     | set(background_freq_df[background_freq_df['background'] == 0].index)\n",
    "                     | set(background_freq_df[background_freq_df['background_rank'] > max_background_rank].index)\n",
    "                     | set(background_freq_df[background_freq_df['semantic_similarity_rank'] < min_similarity_rank].index)\n",
    "                    )\n",
    "    print(len(new_stop_list), len(background_freq_df))\n",
    "    unigram_corpus = unigram_corpus.remove_terms(new_stop_list)\n",
    "    #unigram_corpus = corpus.get_unigram_corpus()\n",
    "    #background_freq_df = background_freq_df.loc[set(background_freq_df.index) - new_stop_list]\n",
    "    background_freq_df = unigram_corpus.get_scaled_f_scores_vs_background()\n",
    "    background_freq_df = background_freq_df.loc[unigram_corpus.get_term_freq_df().index].fillna(0)\n",
    "    background_freq_df['corpus_rank'] = scaled_rank(background_freq_df['corpus'])\n",
    "    background_freq_df['background_rank'] = scaled_rank(background_freq_df['background'])\n",
    "    background_freq_df['semantic_similarity'] = [np.max([nlp(term).similarity(distinctive_term) \n",
    "                                                         for distinctive_term in most_distinctive_terms])\n",
    "                                                 for term in background_freq_df.index]\n",
    "    #background_freq_df = background_freq_df.sort_values(by='corpus_background_delta_pctl', ascending=False)\n",
    "    background_freq_df['background_delta'] = background_freq_df['background_rank'] - background_freq_df['corpus_rank']\n",
    "    background_freq_df['background_delta_rank'] = rank_zero_at_half(background_freq_df['background_rank'] - background_freq_df['corpus_rank'])\n",
    "    background_freq_df['semantic_similarity_rank'] = ((rankdata(background_freq_df['semantic_similarity'])+1.)/(len(background_freq_df)+1.))\n",
    "\n",
    "    #background_freq_df['should_be_there'] = hmean([background_freq_df['background_delta_rank'], background_freq_df['semantic_similarity_rank']])\n",
    "    background_freq_df['should_be_there'] = hmean([background_freq_df['background_delta_rank']+0.0001, background_freq_df['semantic_similarity_rank']+0.0001])\n",
    "\n",
    "    print(len(background_freq_df))\n",
    "    return unigram_corpus, background_freq_df, most_distinctive_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6658 8160\n",
      "1502\n",
      "[hardworking, bailouts, autoworkers, grandkids, billionaires, millionaires, noches, bless, dreamers, outcompete]\n"
     ]
    }
   ],
   "source": [
    "unigram_corpus, background_freq_df, most_distinctive_terms = get_should_be_there_but_isnt(corpus)\n",
    "print(most_distinctive_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1502"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#background_freq_df.sort_values(by='semantic_similarity', ascending=False) \n",
    "from scipy.stats import hmean, rankdata\n",
    "\n",
    "#corpus_background_delta_pctl = 1-(np.argsort(background_freq_df['background_rank'] - background_freq_df['corpus_rank'])+1.)/(len(background_freq_df)+1)\n",
    "#background_freq_df['should_be_there_score'] = hmean([semantic_sim_pctl, corpus_background_delta_pctl])\n",
    "#background_freq_df['corpus_background_delta_pctl'] = corpus_background_delta_pctl\n",
    "len(background_freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1400\"\n",
       "            height=\"1000\"\n",
       "            src=\"demo_expected_vs_actual.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a55be0b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = st.produce_scattertext_explorer(unigram_corpus,\n",
    "                                       category='democrat',\n",
    "                                       category_name='Characteristic',\n",
    "                                       not_category_name='Not Characteristic',\n",
    "                                       minimum_term_frequency=0,\n",
    "                                       width_in_pixels=1200,\n",
    "                                       x_coords=background_freq_df['background_delta_rank'],\n",
    "                                       y_coords=background_freq_df['semantic_similarity_rank'],\n",
    "                                       scores=1.-background_freq_df['should_be_there'],\n",
    "                                       sort_by_dist=False,\n",
    "                                       show_characteristic=False,\n",
    "                                       metadata=convention_df['speaker'],\n",
    "                                       x_label='Background Delta',\n",
    "                                       y_label='Similarity to Characteristic Terms')\n",
    "fn = 'demo_expected_vs_actual.html'\n",
    "open(fn, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=fn, width = 1400, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debate_df = pd.read_csv('../scattertext/scattertext/data/presidential_debates_2016.csv.gz')\n",
    "debate_df = debate_df[debate_df['party'].isin(['Democratic', 'Republican'])]\n",
    "debate_df['parsed'] = debate_df['statement'].apply(nlp)\n",
    "debate_corpus = st.CorpusFromParsedDocuments(debate_df, category_col='party', parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096 3748\n",
      "652\n",
      "[obama, outsmarted, barack, twitter, trump, kaine, hillary, frisk, renegotiate, slobs]\n"
     ]
    }
   ],
   "source": [
    "debate_unigram_corpus, debate_background_freq_df, distinctive_terms = get_should_be_there_but_isnt(debate_corpus, filter_proper_nouns=False)\n",
    "print(distinctive_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1400\"\n",
       "            height=\"1000\"\n",
       "            src=\"demo_expected_vs_actual.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x152f3cf98>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = st.produce_scattertext_explorer(debate_unigram_corpus,\n",
    "                                       category='Democratic',\n",
    "                                       category_name='Characteristic',\n",
    "                                       not_category_name='Not Characteristic',\n",
    "                                       minimum_term_frequency=0,\n",
    "                                       width_in_pixels=1200,\n",
    "                                       x_coords=debate_background_freq_df['background_delta_rank'],\n",
    "                                       y_coords=debate_background_freq_df['semantic_similarity_rank'],\n",
    "                                       scores=1.-debate_background_freq_df['should_be_there'],\n",
    "                                       sort_by_dist=False,\n",
    "                                       show_characteristic=False,\n",
    "                                       metadata=debate_df['speaker and debate'],\n",
    "                                       x_label='Background Delta',\n",
    "                                       y_label='Similarity to Characteristic Terms')\n",
    "fn = 'demo_expected_vs_actual.html'\n",
    "open(fn, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=fn, width = 1400, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
