<!-- some code adapted from www.degeneratestate.org/static/metal_lyrics/metal_line.html -->
<!-- <!DOCTYPE html>
<meta content="utf-8"> -->
<style> /* set the CSS */

body {
  font: 12px Arial;
}


svg {
  font: 12px Helvetica;
}

path {
  stroke: steelblue;
  stroke-width: 2;
  fill: none;
}

.axis path,
.axis lineper {
  fill: none;
  stroke: grey;
  stroke-width: 1;
  shape-rendering: crispEdges;
}

div.tooltip {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 28px;
  padding: 2px;
  font: 12px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

div.tooltipscore {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 42px;
  padding: 2px;
  font: 10px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}


.category_header {
  font: 12px sans-serif;
  font-weight: bolder;
  text-decoration: underline;
}

div.label {
  color: rgb(252, 251, 253);
  color: rgb(63, 0, 125);
  color: rgb(158, 155, 201);

  position: absolute;
  text-align: left;
  padding: 1px;
  border-spacing: 1px;
  font: 10px sans-serif;
  font-family: Sans-Serif;
  border: 0;
  pointer-events: none;
}

input {
  border: 1px dotted #ccc;
  background: white;
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

.alert {
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

ul.top_terms li {
  padding-right: 20px;
  font-size: 30pt;
  color: red;
}

input:focus {
  background-color: lightyellow;
  outline: none;
}

.snippet {
  padding-bottom: 10px;
  padding-left: 5px;
  padding-right: 5px;
  white-space: pre-wrap;
}

.snippet_header {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  font-weight: bolder;
  #text-decoration: underline;
  text-align: center;
  border-bottom-width: 10px;
  border-bottom-color: #888888;
  padding-bottom: 10px;
}


#title-div {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
}


.text_header {
  font: 18px sans-serif;
  font-size: 18px;
  font-family: Helvetica, Arial, Sans-Serif;

  font-weight: bolder;
  text-decoration: underline;
  text-align: center;
  color: darkblue;
  padding-bottom: 10px;
}

.text_subheader {
  font-size: 14px;
  font-family: Helvetica, Arial, Sans-Serif;

  text-align: center;
}


.snippet_meta {
  border-top: 3px solid #4588ba;
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
color: darkblue;
}

.left_contexts {
  width: 45%;
  float: left;
}

.right_contexts {
  width: 45%;
  float: left;
}

.scattertext {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.label {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}


.small_label {
  font-size: 10px;
}

#corpus-stats {
  text-align: center;
}

#cat {
}

#notcat {
}

</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.6.0/d3.min.js" charset="utf-8"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js" charset="utf-8"></script>


<span id="title-div"></span>
<div class="scattertext" id="d3-div-1"> </div>
<div id="corpus-stats"> </div>
<form name="termForm" onSubmit="handleSearch(); return false;">
  <input name="Submit" type="submit" value="Search for term">
  <input type="text" id="searchTerm" placeholder="Type a word or two&hellip;">
  <span id="alertMessage" class="alert"></span>
</form>
<a name="snippets"></a>
<a name="snippetsalt"></a>
<div id="termstats"></div>
<div id="d3-div-2">
 <div class="snippet_header left_contexts" id="cathead"></div>
 <div class="snippet_header right_contexts" id="notcathead"></div>
 <div class="snippet left_contexts" id="cat"></div>
 <div class="snippet right_contexts" id="notcat"></div>
</div>
<script charset="utf-8">
  // Created using Cozy: github.com/uwplse/cozy
function Rectangle(ax1, ay1, ax2, ay2) {
    this.ax1 = ax1;
    this.ay1 = ay1;
    this.ax2 = ax2;
    this.ay2 = ay2;
    this._left7 = undefined;
    this._right8 = undefined;
    this._parent9 = undefined;
    this._min_ax12 = undefined;
    this._min_ay13 = undefined;
    this._max_ay24 = undefined;
    this._height10 = undefined;
}
function RectangleHolder() {
    this.my_size = 0;
    (this)._root1 = null;
}
RectangleHolder.prototype.size = function () {
    return this.my_size;
};
RectangleHolder.prototype.add = function (x) {
    ++this.my_size;
    var _idx69 = (x).ax2;
    (x)._left7 = null;
    (x)._right8 = null;
    (x)._min_ax12 = (x).ax1;
    (x)._min_ay13 = (x).ay1;
    (x)._max_ay24 = (x).ay2;
    (x)._height10 = 0;
    var _previous70 = null;
    var _current71 = (this)._root1;
    var _is_left72 = false;
    while (!((_current71) == null)) {
        _previous70 = _current71;
        if ((_idx69) < ((_current71).ax2)) {
            _current71 = (_current71)._left7;
            _is_left72 = true;
        } else {
            _current71 = (_current71)._right8;
            _is_left72 = false;
        }
    }
    if ((_previous70) == null) {
        (this)._root1 = x;
    } else {
        (x)._parent9 = _previous70;
        if (_is_left72) {
            (_previous70)._left7 = x;
        } else {
            (_previous70)._right8 = x;
        }
    }
    var _cursor73 = (x)._parent9;
    var _changed74 = true;
    while ((_changed74) && (!((_cursor73) == (null)))) {
        var _old__min_ax1275 = (_cursor73)._min_ax12;
        var _old__min_ay1376 = (_cursor73)._min_ay13;
        var _old__max_ay2477 = (_cursor73)._max_ay24;
        var _old_height78 = (_cursor73)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval79 = (_cursor73).ax1;
        var _child80 = (_cursor73)._left7;
        if (!((_child80) == null)) {
            var _val81 = (_child80)._min_ax12;
            _augval79 = ((_augval79) < (_val81)) ? (_augval79) : (_val81);
        }
        var _child82 = (_cursor73)._right8;
        if (!((_child82) == null)) {
            var _val83 = (_child82)._min_ax12;
            _augval79 = ((_augval79) < (_val83)) ? (_augval79) : (_val83);
        }
        (_cursor73)._min_ax12 = _augval79;
        /* _min_ay13 is min of ay1 */
        var _augval84 = (_cursor73).ay1;
        var _child85 = (_cursor73)._left7;
        if (!((_child85) == null)) {
            var _val86 = (_child85)._min_ay13;
            _augval84 = ((_augval84) < (_val86)) ? (_augval84) : (_val86);
        }
        var _child87 = (_cursor73)._right8;
        if (!((_child87) == null)) {
            var _val88 = (_child87)._min_ay13;
            _augval84 = ((_augval84) < (_val88)) ? (_augval84) : (_val88);
        }
        (_cursor73)._min_ay13 = _augval84;
        /* _max_ay24 is max of ay2 */
        var _augval89 = (_cursor73).ay2;
        var _child90 = (_cursor73)._left7;
        if (!((_child90) == null)) {
            var _val91 = (_child90)._max_ay24;
            _augval89 = ((_augval89) < (_val91)) ? (_val91) : (_augval89);
        }
        var _child92 = (_cursor73)._right8;
        if (!((_child92) == null)) {
            var _val93 = (_child92)._max_ay24;
            _augval89 = ((_augval89) < (_val93)) ? (_val93) : (_augval89);
        }
        (_cursor73)._max_ay24 = _augval89;
        (_cursor73)._height10 = 1 + ((((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) > ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10))) ? ((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) : ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10)));
        _changed74 = false;
        _changed74 = (_changed74) || (!((_old__min_ax1275) == ((_cursor73)._min_ax12)));
        _changed74 = (_changed74) || (!((_old__min_ay1376) == ((_cursor73)._min_ay13)));
        _changed74 = (_changed74) || (!((_old__max_ay2477) == ((_cursor73)._max_ay24)));
        _changed74 = (_changed74) || (!((_old_height78) == ((_cursor73)._height10)));
        _cursor73 = (_cursor73)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor94 = x;
    var _imbalance95;
    while (!(((_cursor94)._parent9) == null)) {
        _cursor94 = (_cursor94)._parent9;
        (_cursor94)._height10 = 1 + ((((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) > ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10))) ? ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) : ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10)));
        _imbalance95 = ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) - ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10));
        if ((_imbalance95) > (1)) {
            if ((((((_cursor94)._left7)._left7) == null) ? (-1) : ((((_cursor94)._left7)._left7)._height10)) < (((((_cursor94)._left7)._right8) == null) ? (-1) : ((((_cursor94)._left7)._right8)._height10))) {
                /* rotate ((_cursor94)._left7)._right8 */
                var _a96 = (_cursor94)._left7;
                var _b97 = (_a96)._right8;
                var _c98 = (_b97)._left7;
                /* replace _a96 with _b97 in (_a96)._parent9 */
                if (!(((_a96)._parent9) == null)) {
                    if ((((_a96)._parent9)._left7) == (_a96)) {
                        ((_a96)._parent9)._left7 = _b97;
                    } else {
                        ((_a96)._parent9)._right8 = _b97;
                    }
                }
                if (!((_b97) == null)) {
                    (_b97)._parent9 = (_a96)._parent9;
                }
                /* replace _c98 with _a96 in _b97 */
                (_b97)._left7 = _a96;
                if (!((_a96) == null)) {
                    (_a96)._parent9 = _b97;
                }
                /* replace _b97 with _c98 in _a96 */
                (_a96)._right8 = _c98;
                if (!((_c98) == null)) {
                    (_c98)._parent9 = _a96;
                }
                /* _min_ax12 is min of ax1 */
                var _augval99 = (_a96).ax1;
                var _child100 = (_a96)._left7;
                if (!((_child100) == null)) {
                    var _val101 = (_child100)._min_ax12;
                    _augval99 = ((_augval99) < (_val101)) ? (_augval99) : (_val101);
                }
                var _child102 = (_a96)._right8;
                if (!((_child102) == null)) {
                    var _val103 = (_child102)._min_ax12;
                    _augval99 = ((_augval99) < (_val103)) ? (_augval99) : (_val103);
                }
                (_a96)._min_ax12 = _augval99;
                /* _min_ay13 is min of ay1 */
                var _augval104 = (_a96).ay1;
                var _child105 = (_a96)._left7;
                if (!((_child105) == null)) {
                    var _val106 = (_child105)._min_ay13;
                    _augval104 = ((_augval104) < (_val106)) ? (_augval104) : (_val106);
                }
                var _child107 = (_a96)._right8;
                if (!((_child107) == null)) {
                    var _val108 = (_child107)._min_ay13;
                    _augval104 = ((_augval104) < (_val108)) ? (_augval104) : (_val108);
                }
                (_a96)._min_ay13 = _augval104;
                /* _max_ay24 is max of ay2 */
                var _augval109 = (_a96).ay2;
                var _child110 = (_a96)._left7;
                if (!((_child110) == null)) {
                    var _val111 = (_child110)._max_ay24;
                    _augval109 = ((_augval109) < (_val111)) ? (_val111) : (_augval109);
                }
                var _child112 = (_a96)._right8;
                if (!((_child112) == null)) {
                    var _val113 = (_child112)._max_ay24;
                    _augval109 = ((_augval109) < (_val113)) ? (_val113) : (_augval109);
                }
                (_a96)._max_ay24 = _augval109;
                (_a96)._height10 = 1 + ((((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) > ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10))) ? ((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) : ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval114 = (_b97).ax1;
                var _child115 = (_b97)._left7;
                if (!((_child115) == null)) {
                    var _val116 = (_child115)._min_ax12;
                    _augval114 = ((_augval114) < (_val116)) ? (_augval114) : (_val116);
                }
                var _child117 = (_b97)._right8;
                if (!((_child117) == null)) {
                    var _val118 = (_child117)._min_ax12;
                    _augval114 = ((_augval114) < (_val118)) ? (_augval114) : (_val118);
                }
                (_b97)._min_ax12 = _augval114;
                /* _min_ay13 is min of ay1 */
                var _augval119 = (_b97).ay1;
                var _child120 = (_b97)._left7;
                if (!((_child120) == null)) {
                    var _val121 = (_child120)._min_ay13;
                    _augval119 = ((_augval119) < (_val121)) ? (_augval119) : (_val121);
                }
                var _child122 = (_b97)._right8;
                if (!((_child122) == null)) {
                    var _val123 = (_child122)._min_ay13;
                    _augval119 = ((_augval119) < (_val123)) ? (_augval119) : (_val123);
                }
                (_b97)._min_ay13 = _augval119;
                /* _max_ay24 is max of ay2 */
                var _augval124 = (_b97).ay2;
                var _child125 = (_b97)._left7;
                if (!((_child125) == null)) {
                    var _val126 = (_child125)._max_ay24;
                    _augval124 = ((_augval124) < (_val126)) ? (_val126) : (_augval124);
                }
                var _child127 = (_b97)._right8;
                if (!((_child127) == null)) {
                    var _val128 = (_child127)._max_ay24;
                    _augval124 = ((_augval124) < (_val128)) ? (_val128) : (_augval124);
                }
                (_b97)._max_ay24 = _augval124;
                (_b97)._height10 = 1 + ((((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) > ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10))) ? ((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) : ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10)));
                if (!(((_b97)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval129 = ((_b97)._parent9).ax1;
                    var _child130 = ((_b97)._parent9)._left7;
                    if (!((_child130) == null)) {
                        var _val131 = (_child130)._min_ax12;
                        _augval129 = ((_augval129) < (_val131)) ? (_augval129) : (_val131);
                    }
                    var _child132 = ((_b97)._parent9)._right8;
                    if (!((_child132) == null)) {
                        var _val133 = (_child132)._min_ax12;
                        _augval129 = ((_augval129) < (_val133)) ? (_augval129) : (_val133);
                    }
                    ((_b97)._parent9)._min_ax12 = _augval129;
                    /* _min_ay13 is min of ay1 */
                    var _augval134 = ((_b97)._parent9).ay1;
                    var _child135 = ((_b97)._parent9)._left7;
                    if (!((_child135) == null)) {
                        var _val136 = (_child135)._min_ay13;
                        _augval134 = ((_augval134) < (_val136)) ? (_augval134) : (_val136);
                    }
                    var _child137 = ((_b97)._parent9)._right8;
                    if (!((_child137) == null)) {
                        var _val138 = (_child137)._min_ay13;
                        _augval134 = ((_augval134) < (_val138)) ? (_augval134) : (_val138);
                    }
                    ((_b97)._parent9)._min_ay13 = _augval134;
                    /* _max_ay24 is max of ay2 */
                    var _augval139 = ((_b97)._parent9).ay2;
                    var _child140 = ((_b97)._parent9)._left7;
                    if (!((_child140) == null)) {
                        var _val141 = (_child140)._max_ay24;
                        _augval139 = ((_augval139) < (_val141)) ? (_val141) : (_augval139);
                    }
                    var _child142 = ((_b97)._parent9)._right8;
                    if (!((_child142) == null)) {
                        var _val143 = (_child142)._max_ay24;
                        _augval139 = ((_augval139) < (_val143)) ? (_val143) : (_augval139);
                    }
                    ((_b97)._parent9)._max_ay24 = _augval139;
                    ((_b97)._parent9)._height10 = 1 + (((((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) > (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10))) ? (((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) : (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b97;
                }
            }
            /* rotate (_cursor94)._left7 */
            var _a144 = _cursor94;
            var _b145 = (_a144)._left7;
            var _c146 = (_b145)._right8;
            /* replace _a144 with _b145 in (_a144)._parent9 */
            if (!(((_a144)._parent9) == null)) {
                if ((((_a144)._parent9)._left7) == (_a144)) {
                    ((_a144)._parent9)._left7 = _b145;
                } else {
                    ((_a144)._parent9)._right8 = _b145;
                }
            }
            if (!((_b145) == null)) {
                (_b145)._parent9 = (_a144)._parent9;
            }
            /* replace _c146 with _a144 in _b145 */
            (_b145)._right8 = _a144;
            if (!((_a144) == null)) {
                (_a144)._parent9 = _b145;
            }
            /* replace _b145 with _c146 in _a144 */
            (_a144)._left7 = _c146;
            if (!((_c146) == null)) {
                (_c146)._parent9 = _a144;
            }
            /* _min_ax12 is min of ax1 */
            var _augval147 = (_a144).ax1;
            var _child148 = (_a144)._left7;
            if (!((_child148) == null)) {
                var _val149 = (_child148)._min_ax12;
                _augval147 = ((_augval147) < (_val149)) ? (_augval147) : (_val149);
            }
            var _child150 = (_a144)._right8;
            if (!((_child150) == null)) {
                var _val151 = (_child150)._min_ax12;
                _augval147 = ((_augval147) < (_val151)) ? (_augval147) : (_val151);
            }
            (_a144)._min_ax12 = _augval147;
            /* _min_ay13 is min of ay1 */
            var _augval152 = (_a144).ay1;
            var _child153 = (_a144)._left7;
            if (!((_child153) == null)) {
                var _val154 = (_child153)._min_ay13;
                _augval152 = ((_augval152) < (_val154)) ? (_augval152) : (_val154);
            }
            var _child155 = (_a144)._right8;
            if (!((_child155) == null)) {
                var _val156 = (_child155)._min_ay13;
                _augval152 = ((_augval152) < (_val156)) ? (_augval152) : (_val156);
            }
            (_a144)._min_ay13 = _augval152;
            /* _max_ay24 is max of ay2 */
            var _augval157 = (_a144).ay2;
            var _child158 = (_a144)._left7;
            if (!((_child158) == null)) {
                var _val159 = (_child158)._max_ay24;
                _augval157 = ((_augval157) < (_val159)) ? (_val159) : (_augval157);
            }
            var _child160 = (_a144)._right8;
            if (!((_child160) == null)) {
                var _val161 = (_child160)._max_ay24;
                _augval157 = ((_augval157) < (_val161)) ? (_val161) : (_augval157);
            }
            (_a144)._max_ay24 = _augval157;
            (_a144)._height10 = 1 + ((((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) > ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10))) ? ((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) : ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval162 = (_b145).ax1;
            var _child163 = (_b145)._left7;
            if (!((_child163) == null)) {
                var _val164 = (_child163)._min_ax12;
                _augval162 = ((_augval162) < (_val164)) ? (_augval162) : (_val164);
            }
            var _child165 = (_b145)._right8;
            if (!((_child165) == null)) {
                var _val166 = (_child165)._min_ax12;
                _augval162 = ((_augval162) < (_val166)) ? (_augval162) : (_val166);
            }
            (_b145)._min_ax12 = _augval162;
            /* _min_ay13 is min of ay1 */
            var _augval167 = (_b145).ay1;
            var _child168 = (_b145)._left7;
            if (!((_child168) == null)) {
                var _val169 = (_child168)._min_ay13;
                _augval167 = ((_augval167) < (_val169)) ? (_augval167) : (_val169);
            }
            var _child170 = (_b145)._right8;
            if (!((_child170) == null)) {
                var _val171 = (_child170)._min_ay13;
                _augval167 = ((_augval167) < (_val171)) ? (_augval167) : (_val171);
            }
            (_b145)._min_ay13 = _augval167;
            /* _max_ay24 is max of ay2 */
            var _augval172 = (_b145).ay2;
            var _child173 = (_b145)._left7;
            if (!((_child173) == null)) {
                var _val174 = (_child173)._max_ay24;
                _augval172 = ((_augval172) < (_val174)) ? (_val174) : (_augval172);
            }
            var _child175 = (_b145)._right8;
            if (!((_child175) == null)) {
                var _val176 = (_child175)._max_ay24;
                _augval172 = ((_augval172) < (_val176)) ? (_val176) : (_augval172);
            }
            (_b145)._max_ay24 = _augval172;
            (_b145)._height10 = 1 + ((((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) > ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10))) ? ((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) : ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10)));
            if (!(((_b145)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval177 = ((_b145)._parent9).ax1;
                var _child178 = ((_b145)._parent9)._left7;
                if (!((_child178) == null)) {
                    var _val179 = (_child178)._min_ax12;
                    _augval177 = ((_augval177) < (_val179)) ? (_augval177) : (_val179);
                }
                var _child180 = ((_b145)._parent9)._right8;
                if (!((_child180) == null)) {
                    var _val181 = (_child180)._min_ax12;
                    _augval177 = ((_augval177) < (_val181)) ? (_augval177) : (_val181);
                }
                ((_b145)._parent9)._min_ax12 = _augval177;
                /* _min_ay13 is min of ay1 */
                var _augval182 = ((_b145)._parent9).ay1;
                var _child183 = ((_b145)._parent9)._left7;
                if (!((_child183) == null)) {
                    var _val184 = (_child183)._min_ay13;
                    _augval182 = ((_augval182) < (_val184)) ? (_augval182) : (_val184);
                }
                var _child185 = ((_b145)._parent9)._right8;
                if (!((_child185) == null)) {
                    var _val186 = (_child185)._min_ay13;
                    _augval182 = ((_augval182) < (_val186)) ? (_augval182) : (_val186);
                }
                ((_b145)._parent9)._min_ay13 = _augval182;
                /* _max_ay24 is max of ay2 */
                var _augval187 = ((_b145)._parent9).ay2;
                var _child188 = ((_b145)._parent9)._left7;
                if (!((_child188) == null)) {
                    var _val189 = (_child188)._max_ay24;
                    _augval187 = ((_augval187) < (_val189)) ? (_val189) : (_augval187);
                }
                var _child190 = ((_b145)._parent9)._right8;
                if (!((_child190) == null)) {
                    var _val191 = (_child190)._max_ay24;
                    _augval187 = ((_augval187) < (_val191)) ? (_val191) : (_augval187);
                }
                ((_b145)._parent9)._max_ay24 = _augval187;
                ((_b145)._parent9)._height10 = 1 + (((((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) > (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10))) ? (((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) : (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b145;
            }
            _cursor94 = (_cursor94)._parent9;
        } else if ((_imbalance95) < (-1)) {
            if ((((((_cursor94)._right8)._left7) == null) ? (-1) : ((((_cursor94)._right8)._left7)._height10)) > (((((_cursor94)._right8)._right8) == null) ? (-1) : ((((_cursor94)._right8)._right8)._height10))) {
                /* rotate ((_cursor94)._right8)._left7 */
                var _a192 = (_cursor94)._right8;
                var _b193 = (_a192)._left7;
                var _c194 = (_b193)._right8;
                /* replace _a192 with _b193 in (_a192)._parent9 */
                if (!(((_a192)._parent9) == null)) {
                    if ((((_a192)._parent9)._left7) == (_a192)) {
                        ((_a192)._parent9)._left7 = _b193;
                    } else {
                        ((_a192)._parent9)._right8 = _b193;
                    }
                }
                if (!((_b193) == null)) {
                    (_b193)._parent9 = (_a192)._parent9;
                }
                /* replace _c194 with _a192 in _b193 */
                (_b193)._right8 = _a192;
                if (!((_a192) == null)) {
                    (_a192)._parent9 = _b193;
                }
                /* replace _b193 with _c194 in _a192 */
                (_a192)._left7 = _c194;
                if (!((_c194) == null)) {
                    (_c194)._parent9 = _a192;
                }
                /* _min_ax12 is min of ax1 */
                var _augval195 = (_a192).ax1;
                var _child196 = (_a192)._left7;
                if (!((_child196) == null)) {
                    var _val197 = (_child196)._min_ax12;
                    _augval195 = ((_augval195) < (_val197)) ? (_augval195) : (_val197);
                }
                var _child198 = (_a192)._right8;
                if (!((_child198) == null)) {
                    var _val199 = (_child198)._min_ax12;
                    _augval195 = ((_augval195) < (_val199)) ? (_augval195) : (_val199);
                }
                (_a192)._min_ax12 = _augval195;
                /* _min_ay13 is min of ay1 */
                var _augval200 = (_a192).ay1;
                var _child201 = (_a192)._left7;
                if (!((_child201) == null)) {
                    var _val202 = (_child201)._min_ay13;
                    _augval200 = ((_augval200) < (_val202)) ? (_augval200) : (_val202);
                }
                var _child203 = (_a192)._right8;
                if (!((_child203) == null)) {
                    var _val204 = (_child203)._min_ay13;
                    _augval200 = ((_augval200) < (_val204)) ? (_augval200) : (_val204);
                }
                (_a192)._min_ay13 = _augval200;
                /* _max_ay24 is max of ay2 */
                var _augval205 = (_a192).ay2;
                var _child206 = (_a192)._left7;
                if (!((_child206) == null)) {
                    var _val207 = (_child206)._max_ay24;
                    _augval205 = ((_augval205) < (_val207)) ? (_val207) : (_augval205);
                }
                var _child208 = (_a192)._right8;
                if (!((_child208) == null)) {
                    var _val209 = (_child208)._max_ay24;
                    _augval205 = ((_augval205) < (_val209)) ? (_val209) : (_augval205);
                }
                (_a192)._max_ay24 = _augval205;
                (_a192)._height10 = 1 + ((((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) > ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10))) ? ((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) : ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval210 = (_b193).ax1;
                var _child211 = (_b193)._left7;
                if (!((_child211) == null)) {
                    var _val212 = (_child211)._min_ax12;
                    _augval210 = ((_augval210) < (_val212)) ? (_augval210) : (_val212);
                }
                var _child213 = (_b193)._right8;
                if (!((_child213) == null)) {
                    var _val214 = (_child213)._min_ax12;
                    _augval210 = ((_augval210) < (_val214)) ? (_augval210) : (_val214);
                }
                (_b193)._min_ax12 = _augval210;
                /* _min_ay13 is min of ay1 */
                var _augval215 = (_b193).ay1;
                var _child216 = (_b193)._left7;
                if (!((_child216) == null)) {
                    var _val217 = (_child216)._min_ay13;
                    _augval215 = ((_augval215) < (_val217)) ? (_augval215) : (_val217);
                }
                var _child218 = (_b193)._right8;
                if (!((_child218) == null)) {
                    var _val219 = (_child218)._min_ay13;
                    _augval215 = ((_augval215) < (_val219)) ? (_augval215) : (_val219);
                }
                (_b193)._min_ay13 = _augval215;
                /* _max_ay24 is max of ay2 */
                var _augval220 = (_b193).ay2;
                var _child221 = (_b193)._left7;
                if (!((_child221) == null)) {
                    var _val222 = (_child221)._max_ay24;
                    _augval220 = ((_augval220) < (_val222)) ? (_val222) : (_augval220);
                }
                var _child223 = (_b193)._right8;
                if (!((_child223) == null)) {
                    var _val224 = (_child223)._max_ay24;
                    _augval220 = ((_augval220) < (_val224)) ? (_val224) : (_augval220);
                }
                (_b193)._max_ay24 = _augval220;
                (_b193)._height10 = 1 + ((((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) > ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10))) ? ((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) : ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10)));
                if (!(((_b193)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval225 = ((_b193)._parent9).ax1;
                    var _child226 = ((_b193)._parent9)._left7;
                    if (!((_child226) == null)) {
                        var _val227 = (_child226)._min_ax12;
                        _augval225 = ((_augval225) < (_val227)) ? (_augval225) : (_val227);
                    }
                    var _child228 = ((_b193)._parent9)._right8;
                    if (!((_child228) == null)) {
                        var _val229 = (_child228)._min_ax12;
                        _augval225 = ((_augval225) < (_val229)) ? (_augval225) : (_val229);
                    }
                    ((_b193)._parent9)._min_ax12 = _augval225;
                    /* _min_ay13 is min of ay1 */
                    var _augval230 = ((_b193)._parent9).ay1;
                    var _child231 = ((_b193)._parent9)._left7;
                    if (!((_child231) == null)) {
                        var _val232 = (_child231)._min_ay13;
                        _augval230 = ((_augval230) < (_val232)) ? (_augval230) : (_val232);
                    }
                    var _child233 = ((_b193)._parent9)._right8;
                    if (!((_child233) == null)) {
                        var _val234 = (_child233)._min_ay13;
                        _augval230 = ((_augval230) < (_val234)) ? (_augval230) : (_val234);
                    }
                    ((_b193)._parent9)._min_ay13 = _augval230;
                    /* _max_ay24 is max of ay2 */
                    var _augval235 = ((_b193)._parent9).ay2;
                    var _child236 = ((_b193)._parent9)._left7;
                    if (!((_child236) == null)) {
                        var _val237 = (_child236)._max_ay24;
                        _augval235 = ((_augval235) < (_val237)) ? (_val237) : (_augval235);
                    }
                    var _child238 = ((_b193)._parent9)._right8;
                    if (!((_child238) == null)) {
                        var _val239 = (_child238)._max_ay24;
                        _augval235 = ((_augval235) < (_val239)) ? (_val239) : (_augval235);
                    }
                    ((_b193)._parent9)._max_ay24 = _augval235;
                    ((_b193)._parent9)._height10 = 1 + (((((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) > (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10))) ? (((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) : (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b193;
                }
            }
            /* rotate (_cursor94)._right8 */
            var _a240 = _cursor94;
            var _b241 = (_a240)._right8;
            var _c242 = (_b241)._left7;
            /* replace _a240 with _b241 in (_a240)._parent9 */
            if (!(((_a240)._parent9) == null)) {
                if ((((_a240)._parent9)._left7) == (_a240)) {
                    ((_a240)._parent9)._left7 = _b241;
                } else {
                    ((_a240)._parent9)._right8 = _b241;
                }
            }
            if (!((_b241) == null)) {
                (_b241)._parent9 = (_a240)._parent9;
            }
            /* replace _c242 with _a240 in _b241 */
            (_b241)._left7 = _a240;
            if (!((_a240) == null)) {
                (_a240)._parent9 = _b241;
            }
            /* replace _b241 with _c242 in _a240 */
            (_a240)._right8 = _c242;
            if (!((_c242) == null)) {
                (_c242)._parent9 = _a240;
            }
            /* _min_ax12 is min of ax1 */
            var _augval243 = (_a240).ax1;
            var _child244 = (_a240)._left7;
            if (!((_child244) == null)) {
                var _val245 = (_child244)._min_ax12;
                _augval243 = ((_augval243) < (_val245)) ? (_augval243) : (_val245);
            }
            var _child246 = (_a240)._right8;
            if (!((_child246) == null)) {
                var _val247 = (_child246)._min_ax12;
                _augval243 = ((_augval243) < (_val247)) ? (_augval243) : (_val247);
            }
            (_a240)._min_ax12 = _augval243;
            /* _min_ay13 is min of ay1 */
            var _augval248 = (_a240).ay1;
            var _child249 = (_a240)._left7;
            if (!((_child249) == null)) {
                var _val250 = (_child249)._min_ay13;
                _augval248 = ((_augval248) < (_val250)) ? (_augval248) : (_val250);
            }
            var _child251 = (_a240)._right8;
            if (!((_child251) == null)) {
                var _val252 = (_child251)._min_ay13;
                _augval248 = ((_augval248) < (_val252)) ? (_augval248) : (_val252);
            }
            (_a240)._min_ay13 = _augval248;
            /* _max_ay24 is max of ay2 */
            var _augval253 = (_a240).ay2;
            var _child254 = (_a240)._left7;
            if (!((_child254) == null)) {
                var _val255 = (_child254)._max_ay24;
                _augval253 = ((_augval253) < (_val255)) ? (_val255) : (_augval253);
            }
            var _child256 = (_a240)._right8;
            if (!((_child256) == null)) {
                var _val257 = (_child256)._max_ay24;
                _augval253 = ((_augval253) < (_val257)) ? (_val257) : (_augval253);
            }
            (_a240)._max_ay24 = _augval253;
            (_a240)._height10 = 1 + ((((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) > ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10))) ? ((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) : ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval258 = (_b241).ax1;
            var _child259 = (_b241)._left7;
            if (!((_child259) == null)) {
                var _val260 = (_child259)._min_ax12;
                _augval258 = ((_augval258) < (_val260)) ? (_augval258) : (_val260);
            }
            var _child261 = (_b241)._right8;
            if (!((_child261) == null)) {
                var _val262 = (_child261)._min_ax12;
                _augval258 = ((_augval258) < (_val262)) ? (_augval258) : (_val262);
            }
            (_b241)._min_ax12 = _augval258;
            /* _min_ay13 is min of ay1 */
            var _augval263 = (_b241).ay1;
            var _child264 = (_b241)._left7;
            if (!((_child264) == null)) {
                var _val265 = (_child264)._min_ay13;
                _augval263 = ((_augval263) < (_val265)) ? (_augval263) : (_val265);
            }
            var _child266 = (_b241)._right8;
            if (!((_child266) == null)) {
                var _val267 = (_child266)._min_ay13;
                _augval263 = ((_augval263) < (_val267)) ? (_augval263) : (_val267);
            }
            (_b241)._min_ay13 = _augval263;
            /* _max_ay24 is max of ay2 */
            var _augval268 = (_b241).ay2;
            var _child269 = (_b241)._left7;
            if (!((_child269) == null)) {
                var _val270 = (_child269)._max_ay24;
                _augval268 = ((_augval268) < (_val270)) ? (_val270) : (_augval268);
            }
            var _child271 = (_b241)._right8;
            if (!((_child271) == null)) {
                var _val272 = (_child271)._max_ay24;
                _augval268 = ((_augval268) < (_val272)) ? (_val272) : (_augval268);
            }
            (_b241)._max_ay24 = _augval268;
            (_b241)._height10 = 1 + ((((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) > ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10))) ? ((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) : ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10)));
            if (!(((_b241)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval273 = ((_b241)._parent9).ax1;
                var _child274 = ((_b241)._parent9)._left7;
                if (!((_child274) == null)) {
                    var _val275 = (_child274)._min_ax12;
                    _augval273 = ((_augval273) < (_val275)) ? (_augval273) : (_val275);
                }
                var _child276 = ((_b241)._parent9)._right8;
                if (!((_child276) == null)) {
                    var _val277 = (_child276)._min_ax12;
                    _augval273 = ((_augval273) < (_val277)) ? (_augval273) : (_val277);
                }
                ((_b241)._parent9)._min_ax12 = _augval273;
                /* _min_ay13 is min of ay1 */
                var _augval278 = ((_b241)._parent9).ay1;
                var _child279 = ((_b241)._parent9)._left7;
                if (!((_child279) == null)) {
                    var _val280 = (_child279)._min_ay13;
                    _augval278 = ((_augval278) < (_val280)) ? (_augval278) : (_val280);
                }
                var _child281 = ((_b241)._parent9)._right8;
                if (!((_child281) == null)) {
                    var _val282 = (_child281)._min_ay13;
                    _augval278 = ((_augval278) < (_val282)) ? (_augval278) : (_val282);
                }
                ((_b241)._parent9)._min_ay13 = _augval278;
                /* _max_ay24 is max of ay2 */
                var _augval283 = ((_b241)._parent9).ay2;
                var _child284 = ((_b241)._parent9)._left7;
                if (!((_child284) == null)) {
                    var _val285 = (_child284)._max_ay24;
                    _augval283 = ((_augval283) < (_val285)) ? (_val285) : (_augval283);
                }
                var _child286 = ((_b241)._parent9)._right8;
                if (!((_child286) == null)) {
                    var _val287 = (_child286)._max_ay24;
                    _augval283 = ((_augval283) < (_val287)) ? (_val287) : (_augval283);
                }
                ((_b241)._parent9)._max_ay24 = _augval283;
                ((_b241)._parent9)._height10 = 1 + (((((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) > (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10))) ? (((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) : (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b241;
            }
            _cursor94 = (_cursor94)._parent9;
        }
    }
};
RectangleHolder.prototype.remove = function (x) {
    --this.my_size;
    var _parent288 = (x)._parent9;
    var _left289 = (x)._left7;
    var _right290 = (x)._right8;
    var _new_x291;
    if (((_left289) == null) && ((_right290) == null)) {
        _new_x291 = null;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if ((!((_left289) == null)) && ((_right290) == null)) {
        _new_x291 = _left289;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if (((_left289) == null) && (!((_right290) == null))) {
        _new_x291 = _right290;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else {
        var _root292 = (x)._right8;
        var _x293 = _root292;
        var _descend294 = true;
        var _from_left295 = true;
        while (true) {
            if ((_x293) == null) {
                _x293 = null;
                break;
            }
            if (_descend294) {
                /* too small? */
                if (false) {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                } else if ((!(((_x293)._left7) == null)) && (true)) {
                    _x293 = (_x293)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x293) == (_root292)) {
                    _root292 = (_x293)._right8;
                    _x293 = (_x293)._right8;
                } else {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                }
            } else if (_from_left295) {
                if (false) {
                    _x293 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x293)._right8) == null)) && (true)) {
                    _descend294 = true;
                    if ((_x293) == (_root292)) {
                        _root292 = (_x293)._right8;
                    }
                    _x293 = (_x293)._right8;
                } else if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            } else {
                if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            }
        }
        _new_x291 = _x293;
        var _mp296 = (_x293)._parent9;
        var _mr297 = (_x293)._right8;
        /* replace _x293 with _mr297 in _mp296 */
        if (!((_mp296) == null)) {
            if (((_mp296)._left7) == (_x293)) {
                (_mp296)._left7 = _mr297;
            } else {
                (_mp296)._right8 = _mr297;
            }
        }
        if (!((_mr297) == null)) {
            (_mr297)._parent9 = _mp296;
        }
        /* replace x with _x293 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _x293;
            } else {
                (_parent288)._right8 = _x293;
            }
        }
        if (!((_x293) == null)) {
            (_x293)._parent9 = _parent288;
        }
        /* replace null with _left289 in _x293 */
        (_x293)._left7 = _left289;
        if (!((_left289) == null)) {
            (_left289)._parent9 = _x293;
        }
        /* replace _mr297 with (x)._right8 in _x293 */
        (_x293)._right8 = (x)._right8;
        if (!(((x)._right8) == null)) {
            ((x)._right8)._parent9 = _x293;
        }
        /* _min_ax12 is min of ax1 */
        var _augval298 = (_x293).ax1;
        var _child299 = (_x293)._left7;
        if (!((_child299) == null)) {
            var _val300 = (_child299)._min_ax12;
            _augval298 = ((_augval298) < (_val300)) ? (_augval298) : (_val300);
        }
        var _child301 = (_x293)._right8;
        if (!((_child301) == null)) {
            var _val302 = (_child301)._min_ax12;
            _augval298 = ((_augval298) < (_val302)) ? (_augval298) : (_val302);
        }
        (_x293)._min_ax12 = _augval298;
        /* _min_ay13 is min of ay1 */
        var _augval303 = (_x293).ay1;
        var _child304 = (_x293)._left7;
        if (!((_child304) == null)) {
            var _val305 = (_child304)._min_ay13;
            _augval303 = ((_augval303) < (_val305)) ? (_augval303) : (_val305);
        }
        var _child306 = (_x293)._right8;
        if (!((_child306) == null)) {
            var _val307 = (_child306)._min_ay13;
            _augval303 = ((_augval303) < (_val307)) ? (_augval303) : (_val307);
        }
        (_x293)._min_ay13 = _augval303;
        /* _max_ay24 is max of ay2 */
        var _augval308 = (_x293).ay2;
        var _child309 = (_x293)._left7;
        if (!((_child309) == null)) {
            var _val310 = (_child309)._max_ay24;
            _augval308 = ((_augval308) < (_val310)) ? (_val310) : (_augval308);
        }
        var _child311 = (_x293)._right8;
        if (!((_child311) == null)) {
            var _val312 = (_child311)._max_ay24;
            _augval308 = ((_augval308) < (_val312)) ? (_val312) : (_augval308);
        }
        (_x293)._max_ay24 = _augval308;
        (_x293)._height10 = 1 + ((((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) > ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10))) ? ((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) : ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10)));
        var _cursor313 = _mp296;
        var _changed314 = true;
        while ((_changed314) && (!((_cursor313) == (_parent288)))) {
            var _old__min_ax12315 = (_cursor313)._min_ax12;
            var _old__min_ay13316 = (_cursor313)._min_ay13;
            var _old__max_ay24317 = (_cursor313)._max_ay24;
            var _old_height318 = (_cursor313)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval319 = (_cursor313).ax1;
            var _child320 = (_cursor313)._left7;
            if (!((_child320) == null)) {
                var _val321 = (_child320)._min_ax12;
                _augval319 = ((_augval319) < (_val321)) ? (_augval319) : (_val321);
            }
            var _child322 = (_cursor313)._right8;
            if (!((_child322) == null)) {
                var _val323 = (_child322)._min_ax12;
                _augval319 = ((_augval319) < (_val323)) ? (_augval319) : (_val323);
            }
            (_cursor313)._min_ax12 = _augval319;
            /* _min_ay13 is min of ay1 */
            var _augval324 = (_cursor313).ay1;
            var _child325 = (_cursor313)._left7;
            if (!((_child325) == null)) {
                var _val326 = (_child325)._min_ay13;
                _augval324 = ((_augval324) < (_val326)) ? (_augval324) : (_val326);
            }
            var _child327 = (_cursor313)._right8;
            if (!((_child327) == null)) {
                var _val328 = (_child327)._min_ay13;
                _augval324 = ((_augval324) < (_val328)) ? (_augval324) : (_val328);
            }
            (_cursor313)._min_ay13 = _augval324;
            /* _max_ay24 is max of ay2 */
            var _augval329 = (_cursor313).ay2;
            var _child330 = (_cursor313)._left7;
            if (!((_child330) == null)) {
                var _val331 = (_child330)._max_ay24;
                _augval329 = ((_augval329) < (_val331)) ? (_val331) : (_augval329);
            }
            var _child332 = (_cursor313)._right8;
            if (!((_child332) == null)) {
                var _val333 = (_child332)._max_ay24;
                _augval329 = ((_augval329) < (_val333)) ? (_val333) : (_augval329);
            }
            (_cursor313)._max_ay24 = _augval329;
            (_cursor313)._height10 = 1 + ((((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) > ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10))) ? ((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) : ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10)));
            _changed314 = false;
            _changed314 = (_changed314) || (!((_old__min_ax12315) == ((_cursor313)._min_ax12)));
            _changed314 = (_changed314) || (!((_old__min_ay13316) == ((_cursor313)._min_ay13)));
            _changed314 = (_changed314) || (!((_old__max_ay24317) == ((_cursor313)._max_ay24)));
            _changed314 = (_changed314) || (!((_old_height318) == ((_cursor313)._height10)));
            _cursor313 = (_cursor313)._parent9;
        }
    }
    var _cursor334 = _parent288;
    var _changed335 = true;
    while ((_changed335) && (!((_cursor334) == (null)))) {
        var _old__min_ax12336 = (_cursor334)._min_ax12;
        var _old__min_ay13337 = (_cursor334)._min_ay13;
        var _old__max_ay24338 = (_cursor334)._max_ay24;
        var _old_height339 = (_cursor334)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval340 = (_cursor334).ax1;
        var _child341 = (_cursor334)._left7;
        if (!((_child341) == null)) {
            var _val342 = (_child341)._min_ax12;
            _augval340 = ((_augval340) < (_val342)) ? (_augval340) : (_val342);
        }
        var _child343 = (_cursor334)._right8;
        if (!((_child343) == null)) {
            var _val344 = (_child343)._min_ax12;
            _augval340 = ((_augval340) < (_val344)) ? (_augval340) : (_val344);
        }
        (_cursor334)._min_ax12 = _augval340;
        /* _min_ay13 is min of ay1 */
        var _augval345 = (_cursor334).ay1;
        var _child346 = (_cursor334)._left7;
        if (!((_child346) == null)) {
            var _val347 = (_child346)._min_ay13;
            _augval345 = ((_augval345) < (_val347)) ? (_augval345) : (_val347);
        }
        var _child348 = (_cursor334)._right8;
        if (!((_child348) == null)) {
            var _val349 = (_child348)._min_ay13;
            _augval345 = ((_augval345) < (_val349)) ? (_augval345) : (_val349);
        }
        (_cursor334)._min_ay13 = _augval345;
        /* _max_ay24 is max of ay2 */
        var _augval350 = (_cursor334).ay2;
        var _child351 = (_cursor334)._left7;
        if (!((_child351) == null)) {
            var _val352 = (_child351)._max_ay24;
            _augval350 = ((_augval350) < (_val352)) ? (_val352) : (_augval350);
        }
        var _child353 = (_cursor334)._right8;
        if (!((_child353) == null)) {
            var _val354 = (_child353)._max_ay24;
            _augval350 = ((_augval350) < (_val354)) ? (_val354) : (_augval350);
        }
        (_cursor334)._max_ay24 = _augval350;
        (_cursor334)._height10 = 1 + ((((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) > ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10))) ? ((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) : ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10)));
        _changed335 = false;
        _changed335 = (_changed335) || (!((_old__min_ax12336) == ((_cursor334)._min_ax12)));
        _changed335 = (_changed335) || (!((_old__min_ay13337) == ((_cursor334)._min_ay13)));
        _changed335 = (_changed335) || (!((_old__max_ay24338) == ((_cursor334)._max_ay24)));
        _changed335 = (_changed335) || (!((_old_height339) == ((_cursor334)._height10)));
        _cursor334 = (_cursor334)._parent9;
    }
    if (((this)._root1) == (x)) {
        (this)._root1 = _new_x291;
    }
};
RectangleHolder.prototype.updateAx1 = function (__x, new_val) {
    if ((__x).ax1 != new_val) {
        /* _min_ax12 is min of ax1 */
        var _augval355 = new_val;
        var _child356 = (__x)._left7;
        if (!((_child356) == null)) {
            var _val357 = (_child356)._min_ax12;
            _augval355 = ((_augval355) < (_val357)) ? (_augval355) : (_val357);
        }
        var _child358 = (__x)._right8;
        if (!((_child358) == null)) {
            var _val359 = (_child358)._min_ax12;
            _augval355 = ((_augval355) < (_val359)) ? (_augval355) : (_val359);
        }
        (__x)._min_ax12 = _augval355;
        var _cursor360 = (__x)._parent9;
        var _changed361 = true;
        while ((_changed361) && (!((_cursor360) == (null)))) {
            var _old__min_ax12362 = (_cursor360)._min_ax12;
            var _old_height363 = (_cursor360)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval364 = (_cursor360).ax1;
            var _child365 = (_cursor360)._left7;
            if (!((_child365) == null)) {
                var _val366 = (_child365)._min_ax12;
                _augval364 = ((_augval364) < (_val366)) ? (_augval364) : (_val366);
            }
            var _child367 = (_cursor360)._right8;
            if (!((_child367) == null)) {
                var _val368 = (_child367)._min_ax12;
                _augval364 = ((_augval364) < (_val368)) ? (_augval364) : (_val368);
            }
            (_cursor360)._min_ax12 = _augval364;
            (_cursor360)._height10 = 1 + ((((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) > ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10))) ? ((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) : ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10)));
            _changed361 = false;
            _changed361 = (_changed361) || (!((_old__min_ax12362) == ((_cursor360)._min_ax12)));
            _changed361 = (_changed361) || (!((_old_height363) == ((_cursor360)._height10)));
            _cursor360 = (_cursor360)._parent9;
        }
        (__x).ax1 = new_val;
    }
}
RectangleHolder.prototype.updateAy1 = function (__x, new_val) {
    if ((__x).ay1 != new_val) {
        /* _min_ay13 is min of ay1 */
        var _augval369 = new_val;
        var _child370 = (__x)._left7;
        if (!((_child370) == null)) {
            var _val371 = (_child370)._min_ay13;
            _augval369 = ((_augval369) < (_val371)) ? (_augval369) : (_val371);
        }
        var _child372 = (__x)._right8;
        if (!((_child372) == null)) {
            var _val373 = (_child372)._min_ay13;
            _augval369 = ((_augval369) < (_val373)) ? (_augval369) : (_val373);
        }
        (__x)._min_ay13 = _augval369;
        var _cursor374 = (__x)._parent9;
        var _changed375 = true;
        while ((_changed375) && (!((_cursor374) == (null)))) {
            var _old__min_ay13376 = (_cursor374)._min_ay13;
            var _old_height377 = (_cursor374)._height10;
            /* _min_ay13 is min of ay1 */
            var _augval378 = (_cursor374).ay1;
            var _child379 = (_cursor374)._left7;
            if (!((_child379) == null)) {
                var _val380 = (_child379)._min_ay13;
                _augval378 = ((_augval378) < (_val380)) ? (_augval378) : (_val380);
            }
            var _child381 = (_cursor374)._right8;
            if (!((_child381) == null)) {
                var _val382 = (_child381)._min_ay13;
                _augval378 = ((_augval378) < (_val382)) ? (_augval378) : (_val382);
            }
            (_cursor374)._min_ay13 = _augval378;
            (_cursor374)._height10 = 1 + ((((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) > ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10))) ? ((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) : ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10)));
            _changed375 = false;
            _changed375 = (_changed375) || (!((_old__min_ay13376) == ((_cursor374)._min_ay13)));
            _changed375 = (_changed375) || (!((_old_height377) == ((_cursor374)._height10)));
            _cursor374 = (_cursor374)._parent9;
        }
        (__x).ay1 = new_val;
    }
}
RectangleHolder.prototype.updateAx2 = function (__x, new_val) {
    if ((__x).ax2 != new_val) {
        var _parent383 = (__x)._parent9;
        var _left384 = (__x)._left7;
        var _right385 = (__x)._right8;
        var _new_x386;
        if (((_left384) == null) && ((_right385) == null)) {
            _new_x386 = null;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if ((!((_left384) == null)) && ((_right385) == null)) {
            _new_x386 = _left384;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if (((_left384) == null) && (!((_right385) == null))) {
            _new_x386 = _right385;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else {
            var _root387 = (__x)._right8;
            var _x388 = _root387;
            var _descend389 = true;
            var _from_left390 = true;
            while (true) {
                if ((_x388) == null) {
                    _x388 = null;
                    break;
                }
                if (_descend389) {
                    /* too small? */
                    if (false) {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    } else if ((!(((_x388)._left7) == null)) && (true)) {
                        _x388 = (_x388)._left7;
                        /* too large? */
                    } else if (false) {
                        if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                        /* node ok? */
                    } else if (true) {
                        break;
                    } else if ((_x388) == (_root387)) {
                        _root387 = (_x388)._right8;
                        _x388 = (_x388)._right8;
                    } else {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    }
                } else if (_from_left390) {
                    if (false) {
                        _x388 = null;
                        break;
                    } else if (true) {
                        break;
                    } else if ((!(((_x388)._right8) == null)) && (true)) {
                        _descend389 = true;
                        if ((_x388) == (_root387)) {
                            _root387 = (_x388)._right8;
                        }
                        _x388 = (_x388)._right8;
                    } else if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                } else {
                    if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                }
            }
            _new_x386 = _x388;
            var _mp391 = (_x388)._parent9;
            var _mr392 = (_x388)._right8;
            /* replace _x388 with _mr392 in _mp391 */
            if (!((_mp391) == null)) {
                if (((_mp391)._left7) == (_x388)) {
                    (_mp391)._left7 = _mr392;
                } else {
                    (_mp391)._right8 = _mr392;
                }
            }
            if (!((_mr392) == null)) {
                (_mr392)._parent9 = _mp391;
            }
            /* replace __x with _x388 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _x388;
                } else {
                    (_parent383)._right8 = _x388;
                }
            }
            if (!((_x388) == null)) {
                (_x388)._parent9 = _parent383;
            }
            /* replace null with _left384 in _x388 */
            (_x388)._left7 = _left384;
            if (!((_left384) == null)) {
                (_left384)._parent9 = _x388;
            }
            /* replace _mr392 with (__x)._right8 in _x388 */
            (_x388)._right8 = (__x)._right8;
            if (!(((__x)._right8) == null)) {
                ((__x)._right8)._parent9 = _x388;
            }
            /* _min_ax12 is min of ax1 */
            var _augval393 = (_x388).ax1;
            var _child394 = (_x388)._left7;
            if (!((_child394) == null)) {
                var _val395 = (_child394)._min_ax12;
                _augval393 = ((_augval393) < (_val395)) ? (_augval393) : (_val395);
            }
            var _child396 = (_x388)._right8;
            if (!((_child396) == null)) {
                var _val397 = (_child396)._min_ax12;
                _augval393 = ((_augval393) < (_val397)) ? (_augval393) : (_val397);
            }
            (_x388)._min_ax12 = _augval393;
            /* _min_ay13 is min of ay1 */
            var _augval398 = (_x388).ay1;
            var _child399 = (_x388)._left7;
            if (!((_child399) == null)) {
                var _val400 = (_child399)._min_ay13;
                _augval398 = ((_augval398) < (_val400)) ? (_augval398) : (_val400);
            }
            var _child401 = (_x388)._right8;
            if (!((_child401) == null)) {
                var _val402 = (_child401)._min_ay13;
                _augval398 = ((_augval398) < (_val402)) ? (_augval398) : (_val402);
            }
            (_x388)._min_ay13 = _augval398;
            /* _max_ay24 is max of ay2 */
            var _augval403 = (_x388).ay2;
            var _child404 = (_x388)._left7;
            if (!((_child404) == null)) {
                var _val405 = (_child404)._max_ay24;
                _augval403 = ((_augval403) < (_val405)) ? (_val405) : (_augval403);
            }
            var _child406 = (_x388)._right8;
            if (!((_child406) == null)) {
                var _val407 = (_child406)._max_ay24;
                _augval403 = ((_augval403) < (_val407)) ? (_val407) : (_augval403);
            }
            (_x388)._max_ay24 = _augval403;
            (_x388)._height10 = 1 + ((((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) > ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10))) ? ((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) : ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10)));
            var _cursor408 = _mp391;
            var _changed409 = true;
            while ((_changed409) && (!((_cursor408) == (_parent383)))) {
                var _old__min_ax12410 = (_cursor408)._min_ax12;
                var _old__min_ay13411 = (_cursor408)._min_ay13;
                var _old__max_ay24412 = (_cursor408)._max_ay24;
                var _old_height413 = (_cursor408)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval414 = (_cursor408).ax1;
                var _child415 = (_cursor408)._left7;
                if (!((_child415) == null)) {
                    var _val416 = (_child415)._min_ax12;
                    _augval414 = ((_augval414) < (_val416)) ? (_augval414) : (_val416);
                }
                var _child417 = (_cursor408)._right8;
                if (!((_child417) == null)) {
                    var _val418 = (_child417)._min_ax12;
                    _augval414 = ((_augval414) < (_val418)) ? (_augval414) : (_val418);
                }
                (_cursor408)._min_ax12 = _augval414;
                /* _min_ay13 is min of ay1 */
                var _augval419 = (_cursor408).ay1;
                var _child420 = (_cursor408)._left7;
                if (!((_child420) == null)) {
                    var _val421 = (_child420)._min_ay13;
                    _augval419 = ((_augval419) < (_val421)) ? (_augval419) : (_val421);
                }
                var _child422 = (_cursor408)._right8;
                if (!((_child422) == null)) {
                    var _val423 = (_child422)._min_ay13;
                    _augval419 = ((_augval419) < (_val423)) ? (_augval419) : (_val423);
                }
                (_cursor408)._min_ay13 = _augval419;
                /* _max_ay24 is max of ay2 */
                var _augval424 = (_cursor408).ay2;
                var _child425 = (_cursor408)._left7;
                if (!((_child425) == null)) {
                    var _val426 = (_child425)._max_ay24;
                    _augval424 = ((_augval424) < (_val426)) ? (_val426) : (_augval424);
                }
                var _child427 = (_cursor408)._right8;
                if (!((_child427) == null)) {
                    var _val428 = (_child427)._max_ay24;
                    _augval424 = ((_augval424) < (_val428)) ? (_val428) : (_augval424);
                }
                (_cursor408)._max_ay24 = _augval424;
                (_cursor408)._height10 = 1 + ((((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) > ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10))) ? ((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) : ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10)));
                _changed409 = false;
                _changed409 = (_changed409) || (!((_old__min_ax12410) == ((_cursor408)._min_ax12)));
                _changed409 = (_changed409) || (!((_old__min_ay13411) == ((_cursor408)._min_ay13)));
                _changed409 = (_changed409) || (!((_old__max_ay24412) == ((_cursor408)._max_ay24)));
                _changed409 = (_changed409) || (!((_old_height413) == ((_cursor408)._height10)));
                _cursor408 = (_cursor408)._parent9;
            }
        }
        var _cursor429 = _parent383;
        var _changed430 = true;
        while ((_changed430) && (!((_cursor429) == (null)))) {
            var _old__min_ax12431 = (_cursor429)._min_ax12;
            var _old__min_ay13432 = (_cursor429)._min_ay13;
            var _old__max_ay24433 = (_cursor429)._max_ay24;
            var _old_height434 = (_cursor429)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval435 = (_cursor429).ax1;
            var _child436 = (_cursor429)._left7;
            if (!((_child436) == null)) {
                var _val437 = (_child436)._min_ax12;
                _augval435 = ((_augval435) < (_val437)) ? (_augval435) : (_val437);
            }
            var _child438 = (_cursor429)._right8;
            if (!((_child438) == null)) {
                var _val439 = (_child438)._min_ax12;
                _augval435 = ((_augval435) < (_val439)) ? (_augval435) : (_val439);
            }
            (_cursor429)._min_ax12 = _augval435;
            /* _min_ay13 is min of ay1 */
            var _augval440 = (_cursor429).ay1;
            var _child441 = (_cursor429)._left7;
            if (!((_child441) == null)) {
                var _val442 = (_child441)._min_ay13;
                _augval440 = ((_augval440) < (_val442)) ? (_augval440) : (_val442);
            }
            var _child443 = (_cursor429)._right8;
            if (!((_child443) == null)) {
                var _val444 = (_child443)._min_ay13;
                _augval440 = ((_augval440) < (_val444)) ? (_augval440) : (_val444);
            }
            (_cursor429)._min_ay13 = _augval440;
            /* _max_ay24 is max of ay2 */
            var _augval445 = (_cursor429).ay2;
            var _child446 = (_cursor429)._left7;
            if (!((_child446) == null)) {
                var _val447 = (_child446)._max_ay24;
                _augval445 = ((_augval445) < (_val447)) ? (_val447) : (_augval445);
            }
            var _child448 = (_cursor429)._right8;
            if (!((_child448) == null)) {
                var _val449 = (_child448)._max_ay24;
                _augval445 = ((_augval445) < (_val449)) ? (_val449) : (_augval445);
            }
            (_cursor429)._max_ay24 = _augval445;
            (_cursor429)._height10 = 1 + ((((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) > ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10))) ? ((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) : ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10)));
            _changed430 = false;
            _changed430 = (_changed430) || (!((_old__min_ax12431) == ((_cursor429)._min_ax12)));
            _changed430 = (_changed430) || (!((_old__min_ay13432) == ((_cursor429)._min_ay13)));
            _changed430 = (_changed430) || (!((_old__max_ay24433) == ((_cursor429)._max_ay24)));
            _changed430 = (_changed430) || (!((_old_height434) == ((_cursor429)._height10)));
            _cursor429 = (_cursor429)._parent9;
        }
        if (((this)._root1) == (__x)) {
            (this)._root1 = _new_x386;
        }
        (__x)._left7 = null;
        (__x)._right8 = null;
        (__x)._min_ax12 = (__x).ax1;
        (__x)._min_ay13 = (__x).ay1;
        (__x)._max_ay24 = (__x).ay2;
        (__x)._height10 = 0;
        var _previous450 = null;
        var _current451 = (this)._root1;
        var _is_left452 = false;
        while (!((_current451) == null)) {
            _previous450 = _current451;
            if ((new_val) < ((_current451).ax2)) {
                _current451 = (_current451)._left7;
                _is_left452 = true;
            } else {
                _current451 = (_current451)._right8;
                _is_left452 = false;
            }
        }
        if ((_previous450) == null) {
            (this)._root1 = __x;
        } else {
            (__x)._parent9 = _previous450;
            if (_is_left452) {
                (_previous450)._left7 = __x;
            } else {
                (_previous450)._right8 = __x;
            }
        }
        var _cursor453 = (__x)._parent9;
        var _changed454 = true;
        while ((_changed454) && (!((_cursor453) == (null)))) {
            var _old__min_ax12455 = (_cursor453)._min_ax12;
            var _old__min_ay13456 = (_cursor453)._min_ay13;
            var _old__max_ay24457 = (_cursor453)._max_ay24;
            var _old_height458 = (_cursor453)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval459 = (_cursor453).ax1;
            var _child460 = (_cursor453)._left7;
            if (!((_child460) == null)) {
                var _val461 = (_child460)._min_ax12;
                _augval459 = ((_augval459) < (_val461)) ? (_augval459) : (_val461);
            }
            var _child462 = (_cursor453)._right8;
            if (!((_child462) == null)) {
                var _val463 = (_child462)._min_ax12;
                _augval459 = ((_augval459) < (_val463)) ? (_augval459) : (_val463);
            }
            (_cursor453)._min_ax12 = _augval459;
            /* _min_ay13 is min of ay1 */
            var _augval464 = (_cursor453).ay1;
            var _child465 = (_cursor453)._left7;
            if (!((_child465) == null)) {
                var _val466 = (_child465)._min_ay13;
                _augval464 = ((_augval464) < (_val466)) ? (_augval464) : (_val466);
            }
            var _child467 = (_cursor453)._right8;
            if (!((_child467) == null)) {
                var _val468 = (_child467)._min_ay13;
                _augval464 = ((_augval464) < (_val468)) ? (_augval464) : (_val468);
            }
            (_cursor453)._min_ay13 = _augval464;
            /* _max_ay24 is max of ay2 */
            var _augval469 = (_cursor453).ay2;
            var _child470 = (_cursor453)._left7;
            if (!((_child470) == null)) {
                var _val471 = (_child470)._max_ay24;
                _augval469 = ((_augval469) < (_val471)) ? (_val471) : (_augval469);
            }
            var _child472 = (_cursor453)._right8;
            if (!((_child472) == null)) {
                var _val473 = (_child472)._max_ay24;
                _augval469 = ((_augval469) < (_val473)) ? (_val473) : (_augval469);
            }
            (_cursor453)._max_ay24 = _augval469;
            (_cursor453)._height10 = 1 + ((((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) > ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10))) ? ((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) : ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10)));
            _changed454 = false;
            _changed454 = (_changed454) || (!((_old__min_ax12455) == ((_cursor453)._min_ax12)));
            _changed454 = (_changed454) || (!((_old__min_ay13456) == ((_cursor453)._min_ay13)));
            _changed454 = (_changed454) || (!((_old__max_ay24457) == ((_cursor453)._max_ay24)));
            _changed454 = (_changed454) || (!((_old_height458) == ((_cursor453)._height10)));
            _cursor453 = (_cursor453)._parent9;
        }
        /* rebalance AVL tree */
        var _cursor474 = __x;
        var _imbalance475;
        while (!(((_cursor474)._parent9) == null)) {
            _cursor474 = (_cursor474)._parent9;
            (_cursor474)._height10 = 1 + ((((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) > ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10))) ? ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) : ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10)));
            _imbalance475 = ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) - ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10));
            if ((_imbalance475) > (1)) {
                if ((((((_cursor474)._left7)._left7) == null) ? (-1) : ((((_cursor474)._left7)._left7)._height10)) < (((((_cursor474)._left7)._right8) == null) ? (-1) : ((((_cursor474)._left7)._right8)._height10))) {
                    /* rotate ((_cursor474)._left7)._right8 */
                    var _a476 = (_cursor474)._left7;
                    var _b477 = (_a476)._right8;
                    var _c478 = (_b477)._left7;
                    /* replace _a476 with _b477 in (_a476)._parent9 */
                    if (!(((_a476)._parent9) == null)) {
                        if ((((_a476)._parent9)._left7) == (_a476)) {
                            ((_a476)._parent9)._left7 = _b477;
                        } else {
                            ((_a476)._parent9)._right8 = _b477;
                        }
                    }
                    if (!((_b477) == null)) {
                        (_b477)._parent9 = (_a476)._parent9;
                    }
                    /* replace _c478 with _a476 in _b477 */
                    (_b477)._left7 = _a476;
                    if (!((_a476) == null)) {
                        (_a476)._parent9 = _b477;
                    }
                    /* replace _b477 with _c478 in _a476 */
                    (_a476)._right8 = _c478;
                    if (!((_c478) == null)) {
                        (_c478)._parent9 = _a476;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval479 = (_a476).ax1;
                    var _child480 = (_a476)._left7;
                    if (!((_child480) == null)) {
                        var _val481 = (_child480)._min_ax12;
                        _augval479 = ((_augval479) < (_val481)) ? (_augval479) : (_val481);
                    }
                    var _child482 = (_a476)._right8;
                    if (!((_child482) == null)) {
                        var _val483 = (_child482)._min_ax12;
                        _augval479 = ((_augval479) < (_val483)) ? (_augval479) : (_val483);
                    }
                    (_a476)._min_ax12 = _augval479;
                    /* _min_ay13 is min of ay1 */
                    var _augval484 = (_a476).ay1;
                    var _child485 = (_a476)._left7;
                    if (!((_child485) == null)) {
                        var _val486 = (_child485)._min_ay13;
                        _augval484 = ((_augval484) < (_val486)) ? (_augval484) : (_val486);
                    }
                    var _child487 = (_a476)._right8;
                    if (!((_child487) == null)) {
                        var _val488 = (_child487)._min_ay13;
                        _augval484 = ((_augval484) < (_val488)) ? (_augval484) : (_val488);
                    }
                    (_a476)._min_ay13 = _augval484;
                    /* _max_ay24 is max of ay2 */
                    var _augval489 = (_a476).ay2;
                    var _child490 = (_a476)._left7;
                    if (!((_child490) == null)) {
                        var _val491 = (_child490)._max_ay24;
                        _augval489 = ((_augval489) < (_val491)) ? (_val491) : (_augval489);
                    }
                    var _child492 = (_a476)._right8;
                    if (!((_child492) == null)) {
                        var _val493 = (_child492)._max_ay24;
                        _augval489 = ((_augval489) < (_val493)) ? (_val493) : (_augval489);
                    }
                    (_a476)._max_ay24 = _augval489;
                    (_a476)._height10 = 1 + ((((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) > ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10))) ? ((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) : ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval494 = (_b477).ax1;
                    var _child495 = (_b477)._left7;
                    if (!((_child495) == null)) {
                        var _val496 = (_child495)._min_ax12;
                        _augval494 = ((_augval494) < (_val496)) ? (_augval494) : (_val496);
                    }
                    var _child497 = (_b477)._right8;
                    if (!((_child497) == null)) {
                        var _val498 = (_child497)._min_ax12;
                        _augval494 = ((_augval494) < (_val498)) ? (_augval494) : (_val498);
                    }
                    (_b477)._min_ax12 = _augval494;
                    /* _min_ay13 is min of ay1 */
                    var _augval499 = (_b477).ay1;
                    var _child500 = (_b477)._left7;
                    if (!((_child500) == null)) {
                        var _val501 = (_child500)._min_ay13;
                        _augval499 = ((_augval499) < (_val501)) ? (_augval499) : (_val501);
                    }
                    var _child502 = (_b477)._right8;
                    if (!((_child502) == null)) {
                        var _val503 = (_child502)._min_ay13;
                        _augval499 = ((_augval499) < (_val503)) ? (_augval499) : (_val503);
                    }
                    (_b477)._min_ay13 = _augval499;
                    /* _max_ay24 is max of ay2 */
                    var _augval504 = (_b477).ay2;
                    var _child505 = (_b477)._left7;
                    if (!((_child505) == null)) {
                        var _val506 = (_child505)._max_ay24;
                        _augval504 = ((_augval504) < (_val506)) ? (_val506) : (_augval504);
                    }
                    var _child507 = (_b477)._right8;
                    if (!((_child507) == null)) {
                        var _val508 = (_child507)._max_ay24;
                        _augval504 = ((_augval504) < (_val508)) ? (_val508) : (_augval504);
                    }
                    (_b477)._max_ay24 = _augval504;
                    (_b477)._height10 = 1 + ((((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) > ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10))) ? ((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) : ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10)));
                    if (!(((_b477)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval509 = ((_b477)._parent9).ax1;
                        var _child510 = ((_b477)._parent9)._left7;
                        if (!((_child510) == null)) {
                            var _val511 = (_child510)._min_ax12;
                            _augval509 = ((_augval509) < (_val511)) ? (_augval509) : (_val511);
                        }
                        var _child512 = ((_b477)._parent9)._right8;
                        if (!((_child512) == null)) {
                            var _val513 = (_child512)._min_ax12;
                            _augval509 = ((_augval509) < (_val513)) ? (_augval509) : (_val513);
                        }
                        ((_b477)._parent9)._min_ax12 = _augval509;
                        /* _min_ay13 is min of ay1 */
                        var _augval514 = ((_b477)._parent9).ay1;
                        var _child515 = ((_b477)._parent9)._left7;
                        if (!((_child515) == null)) {
                            var _val516 = (_child515)._min_ay13;
                            _augval514 = ((_augval514) < (_val516)) ? (_augval514) : (_val516);
                        }
                        var _child517 = ((_b477)._parent9)._right8;
                        if (!((_child517) == null)) {
                            var _val518 = (_child517)._min_ay13;
                            _augval514 = ((_augval514) < (_val518)) ? (_augval514) : (_val518);
                        }
                        ((_b477)._parent9)._min_ay13 = _augval514;
                        /* _max_ay24 is max of ay2 */
                        var _augval519 = ((_b477)._parent9).ay2;
                        var _child520 = ((_b477)._parent9)._left7;
                        if (!((_child520) == null)) {
                            var _val521 = (_child520)._max_ay24;
                            _augval519 = ((_augval519) < (_val521)) ? (_val521) : (_augval519);
                        }
                        var _child522 = ((_b477)._parent9)._right8;
                        if (!((_child522) == null)) {
                            var _val523 = (_child522)._max_ay24;
                            _augval519 = ((_augval519) < (_val523)) ? (_val523) : (_augval519);
                        }
                        ((_b477)._parent9)._max_ay24 = _augval519;
                        ((_b477)._parent9)._height10 = 1 + (((((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) > (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10))) ? (((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) : (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b477;
                    }
                }
                /* rotate (_cursor474)._left7 */
                var _a524 = _cursor474;
                var _b525 = (_a524)._left7;
                var _c526 = (_b525)._right8;
                /* replace _a524 with _b525 in (_a524)._parent9 */
                if (!(((_a524)._parent9) == null)) {
                    if ((((_a524)._parent9)._left7) == (_a524)) {
                        ((_a524)._parent9)._left7 = _b525;
                    } else {
                        ((_a524)._parent9)._right8 = _b525;
                    }
                }
                if (!((_b525) == null)) {
                    (_b525)._parent9 = (_a524)._parent9;
                }
                /* replace _c526 with _a524 in _b525 */
                (_b525)._right8 = _a524;
                if (!((_a524) == null)) {
                    (_a524)._parent9 = _b525;
                }
                /* replace _b525 with _c526 in _a524 */
                (_a524)._left7 = _c526;
                if (!((_c526) == null)) {
                    (_c526)._parent9 = _a524;
                }
                /* _min_ax12 is min of ax1 */
                var _augval527 = (_a524).ax1;
                var _child528 = (_a524)._left7;
                if (!((_child528) == null)) {
                    var _val529 = (_child528)._min_ax12;
                    _augval527 = ((_augval527) < (_val529)) ? (_augval527) : (_val529);
                }
                var _child530 = (_a524)._right8;
                if (!((_child530) == null)) {
                    var _val531 = (_child530)._min_ax12;
                    _augval527 = ((_augval527) < (_val531)) ? (_augval527) : (_val531);
                }
                (_a524)._min_ax12 = _augval527;
                /* _min_ay13 is min of ay1 */
                var _augval532 = (_a524).ay1;
                var _child533 = (_a524)._left7;
                if (!((_child533) == null)) {
                    var _val534 = (_child533)._min_ay13;
                    _augval532 = ((_augval532) < (_val534)) ? (_augval532) : (_val534);
                }
                var _child535 = (_a524)._right8;
                if (!((_child535) == null)) {
                    var _val536 = (_child535)._min_ay13;
                    _augval532 = ((_augval532) < (_val536)) ? (_augval532) : (_val536);
                }
                (_a524)._min_ay13 = _augval532;
                /* _max_ay24 is max of ay2 */
                var _augval537 = (_a524).ay2;
                var _child538 = (_a524)._left7;
                if (!((_child538) == null)) {
                    var _val539 = (_child538)._max_ay24;
                    _augval537 = ((_augval537) < (_val539)) ? (_val539) : (_augval537);
                }
                var _child540 = (_a524)._right8;
                if (!((_child540) == null)) {
                    var _val541 = (_child540)._max_ay24;
                    _augval537 = ((_augval537) < (_val541)) ? (_val541) : (_augval537);
                }
                (_a524)._max_ay24 = _augval537;
                (_a524)._height10 = 1 + ((((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) > ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10))) ? ((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) : ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval542 = (_b525).ax1;
                var _child543 = (_b525)._left7;
                if (!((_child543) == null)) {
                    var _val544 = (_child543)._min_ax12;
                    _augval542 = ((_augval542) < (_val544)) ? (_augval542) : (_val544);
                }
                var _child545 = (_b525)._right8;
                if (!((_child545) == null)) {
                    var _val546 = (_child545)._min_ax12;
                    _augval542 = ((_augval542) < (_val546)) ? (_augval542) : (_val546);
                }
                (_b525)._min_ax12 = _augval542;
                /* _min_ay13 is min of ay1 */
                var _augval547 = (_b525).ay1;
                var _child548 = (_b525)._left7;
                if (!((_child548) == null)) {
                    var _val549 = (_child548)._min_ay13;
                    _augval547 = ((_augval547) < (_val549)) ? (_augval547) : (_val549);
                }
                var _child550 = (_b525)._right8;
                if (!((_child550) == null)) {
                    var _val551 = (_child550)._min_ay13;
                    _augval547 = ((_augval547) < (_val551)) ? (_augval547) : (_val551);
                }
                (_b525)._min_ay13 = _augval547;
                /* _max_ay24 is max of ay2 */
                var _augval552 = (_b525).ay2;
                var _child553 = (_b525)._left7;
                if (!((_child553) == null)) {
                    var _val554 = (_child553)._max_ay24;
                    _augval552 = ((_augval552) < (_val554)) ? (_val554) : (_augval552);
                }
                var _child555 = (_b525)._right8;
                if (!((_child555) == null)) {
                    var _val556 = (_child555)._max_ay24;
                    _augval552 = ((_augval552) < (_val556)) ? (_val556) : (_augval552);
                }
                (_b525)._max_ay24 = _augval552;
                (_b525)._height10 = 1 + ((((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) > ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10))) ? ((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) : ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10)));
                if (!(((_b525)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval557 = ((_b525)._parent9).ax1;
                    var _child558 = ((_b525)._parent9)._left7;
                    if (!((_child558) == null)) {
                        var _val559 = (_child558)._min_ax12;
                        _augval557 = ((_augval557) < (_val559)) ? (_augval557) : (_val559);
                    }
                    var _child560 = ((_b525)._parent9)._right8;
                    if (!((_child560) == null)) {
                        var _val561 = (_child560)._min_ax12;
                        _augval557 = ((_augval557) < (_val561)) ? (_augval557) : (_val561);
                    }
                    ((_b525)._parent9)._min_ax12 = _augval557;
                    /* _min_ay13 is min of ay1 */
                    var _augval562 = ((_b525)._parent9).ay1;
                    var _child563 = ((_b525)._parent9)._left7;
                    if (!((_child563) == null)) {
                        var _val564 = (_child563)._min_ay13;
                        _augval562 = ((_augval562) < (_val564)) ? (_augval562) : (_val564);
                    }
                    var _child565 = ((_b525)._parent9)._right8;
                    if (!((_child565) == null)) {
                        var _val566 = (_child565)._min_ay13;
                        _augval562 = ((_augval562) < (_val566)) ? (_augval562) : (_val566);
                    }
                    ((_b525)._parent9)._min_ay13 = _augval562;
                    /* _max_ay24 is max of ay2 */
                    var _augval567 = ((_b525)._parent9).ay2;
                    var _child568 = ((_b525)._parent9)._left7;
                    if (!((_child568) == null)) {
                        var _val569 = (_child568)._max_ay24;
                        _augval567 = ((_augval567) < (_val569)) ? (_val569) : (_augval567);
                    }
                    var _child570 = ((_b525)._parent9)._right8;
                    if (!((_child570) == null)) {
                        var _val571 = (_child570)._max_ay24;
                        _augval567 = ((_augval567) < (_val571)) ? (_val571) : (_augval567);
                    }
                    ((_b525)._parent9)._max_ay24 = _augval567;
                    ((_b525)._parent9)._height10 = 1 + (((((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) > (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10))) ? (((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) : (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b525;
                }
                _cursor474 = (_cursor474)._parent9;
            } else if ((_imbalance475) < (-1)) {
                if ((((((_cursor474)._right8)._left7) == null) ? (-1) : ((((_cursor474)._right8)._left7)._height10)) > (((((_cursor474)._right8)._right8) == null) ? (-1) : ((((_cursor474)._right8)._right8)._height10))) {
                    /* rotate ((_cursor474)._right8)._left7 */
                    var _a572 = (_cursor474)._right8;
                    var _b573 = (_a572)._left7;
                    var _c574 = (_b573)._right8;
                    /* replace _a572 with _b573 in (_a572)._parent9 */
                    if (!(((_a572)._parent9) == null)) {
                        if ((((_a572)._parent9)._left7) == (_a572)) {
                            ((_a572)._parent9)._left7 = _b573;
                        } else {
                            ((_a572)._parent9)._right8 = _b573;
                        }
                    }
                    if (!((_b573) == null)) {
                        (_b573)._parent9 = (_a572)._parent9;
                    }
                    /* replace _c574 with _a572 in _b573 */
                    (_b573)._right8 = _a572;
                    if (!((_a572) == null)) {
                        (_a572)._parent9 = _b573;
                    }
                    /* replace _b573 with _c574 in _a572 */
                    (_a572)._left7 = _c574;
                    if (!((_c574) == null)) {
                        (_c574)._parent9 = _a572;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval575 = (_a572).ax1;
                    var _child576 = (_a572)._left7;
                    if (!((_child576) == null)) {
                        var _val577 = (_child576)._min_ax12;
                        _augval575 = ((_augval575) < (_val577)) ? (_augval575) : (_val577);
                    }
                    var _child578 = (_a572)._right8;
                    if (!((_child578) == null)) {
                        var _val579 = (_child578)._min_ax12;
                        _augval575 = ((_augval575) < (_val579)) ? (_augval575) : (_val579);
                    }
                    (_a572)._min_ax12 = _augval575;
                    /* _min_ay13 is min of ay1 */
                    var _augval580 = (_a572).ay1;
                    var _child581 = (_a572)._left7;
                    if (!((_child581) == null)) {
                        var _val582 = (_child581)._min_ay13;
                        _augval580 = ((_augval580) < (_val582)) ? (_augval580) : (_val582);
                    }
                    var _child583 = (_a572)._right8;
                    if (!((_child583) == null)) {
                        var _val584 = (_child583)._min_ay13;
                        _augval580 = ((_augval580) < (_val584)) ? (_augval580) : (_val584);
                    }
                    (_a572)._min_ay13 = _augval580;
                    /* _max_ay24 is max of ay2 */
                    var _augval585 = (_a572).ay2;
                    var _child586 = (_a572)._left7;
                    if (!((_child586) == null)) {
                        var _val587 = (_child586)._max_ay24;
                        _augval585 = ((_augval585) < (_val587)) ? (_val587) : (_augval585);
                    }
                    var _child588 = (_a572)._right8;
                    if (!((_child588) == null)) {
                        var _val589 = (_child588)._max_ay24;
                        _augval585 = ((_augval585) < (_val589)) ? (_val589) : (_augval585);
                    }
                    (_a572)._max_ay24 = _augval585;
                    (_a572)._height10 = 1 + ((((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) > ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10))) ? ((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) : ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval590 = (_b573).ax1;
                    var _child591 = (_b573)._left7;
                    if (!((_child591) == null)) {
                        var _val592 = (_child591)._min_ax12;
                        _augval590 = ((_augval590) < (_val592)) ? (_augval590) : (_val592);
                    }
                    var _child593 = (_b573)._right8;
                    if (!((_child593) == null)) {
                        var _val594 = (_child593)._min_ax12;
                        _augval590 = ((_augval590) < (_val594)) ? (_augval590) : (_val594);
                    }
                    (_b573)._min_ax12 = _augval590;
                    /* _min_ay13 is min of ay1 */
                    var _augval595 = (_b573).ay1;
                    var _child596 = (_b573)._left7;
                    if (!((_child596) == null)) {
                        var _val597 = (_child596)._min_ay13;
                        _augval595 = ((_augval595) < (_val597)) ? (_augval595) : (_val597);
                    }
                    var _child598 = (_b573)._right8;
                    if (!((_child598) == null)) {
                        var _val599 = (_child598)._min_ay13;
                        _augval595 = ((_augval595) < (_val599)) ? (_augval595) : (_val599);
                    }
                    (_b573)._min_ay13 = _augval595;
                    /* _max_ay24 is max of ay2 */
                    var _augval600 = (_b573).ay2;
                    var _child601 = (_b573)._left7;
                    if (!((_child601) == null)) {
                        var _val602 = (_child601)._max_ay24;
                        _augval600 = ((_augval600) < (_val602)) ? (_val602) : (_augval600);
                    }
                    var _child603 = (_b573)._right8;
                    if (!((_child603) == null)) {
                        var _val604 = (_child603)._max_ay24;
                        _augval600 = ((_augval600) < (_val604)) ? (_val604) : (_augval600);
                    }
                    (_b573)._max_ay24 = _augval600;
                    (_b573)._height10 = 1 + ((((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) > ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10))) ? ((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) : ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10)));
                    if (!(((_b573)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval605 = ((_b573)._parent9).ax1;
                        var _child606 = ((_b573)._parent9)._left7;
                        if (!((_child606) == null)) {
                            var _val607 = (_child606)._min_ax12;
                            _augval605 = ((_augval605) < (_val607)) ? (_augval605) : (_val607);
                        }
                        var _child608 = ((_b573)._parent9)._right8;
                        if (!((_child608) == null)) {
                            var _val609 = (_child608)._min_ax12;
                            _augval605 = ((_augval605) < (_val609)) ? (_augval605) : (_val609);
                        }
                        ((_b573)._parent9)._min_ax12 = _augval605;
                        /* _min_ay13 is min of ay1 */
                        var _augval610 = ((_b573)._parent9).ay1;
                        var _child611 = ((_b573)._parent9)._left7;
                        if (!((_child611) == null)) {
                            var _val612 = (_child611)._min_ay13;
                            _augval610 = ((_augval610) < (_val612)) ? (_augval610) : (_val612);
                        }
                        var _child613 = ((_b573)._parent9)._right8;
                        if (!((_child613) == null)) {
                            var _val614 = (_child613)._min_ay13;
                            _augval610 = ((_augval610) < (_val614)) ? (_augval610) : (_val614);
                        }
                        ((_b573)._parent9)._min_ay13 = _augval610;
                        /* _max_ay24 is max of ay2 */
                        var _augval615 = ((_b573)._parent9).ay2;
                        var _child616 = ((_b573)._parent9)._left7;
                        if (!((_child616) == null)) {
                            var _val617 = (_child616)._max_ay24;
                            _augval615 = ((_augval615) < (_val617)) ? (_val617) : (_augval615);
                        }
                        var _child618 = ((_b573)._parent9)._right8;
                        if (!((_child618) == null)) {
                            var _val619 = (_child618)._max_ay24;
                            _augval615 = ((_augval615) < (_val619)) ? (_val619) : (_augval615);
                        }
                        ((_b573)._parent9)._max_ay24 = _augval615;
                        ((_b573)._parent9)._height10 = 1 + (((((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) > (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10))) ? (((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) : (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b573;
                    }
                }
                /* rotate (_cursor474)._right8 */
                var _a620 = _cursor474;
                var _b621 = (_a620)._right8;
                var _c622 = (_b621)._left7;
                /* replace _a620 with _b621 in (_a620)._parent9 */
                if (!(((_a620)._parent9) == null)) {
                    if ((((_a620)._parent9)._left7) == (_a620)) {
                        ((_a620)._parent9)._left7 = _b621;
                    } else {
                        ((_a620)._parent9)._right8 = _b621;
                    }
                }
                if (!((_b621) == null)) {
                    (_b621)._parent9 = (_a620)._parent9;
                }
                /* replace _c622 with _a620 in _b621 */
                (_b621)._left7 = _a620;
                if (!((_a620) == null)) {
                    (_a620)._parent9 = _b621;
                }
                /* replace _b621 with _c622 in _a620 */
                (_a620)._right8 = _c622;
                if (!((_c622) == null)) {
                    (_c622)._parent9 = _a620;
                }
                /* _min_ax12 is min of ax1 */
                var _augval623 = (_a620).ax1;
                var _child624 = (_a620)._left7;
                if (!((_child624) == null)) {
                    var _val625 = (_child624)._min_ax12;
                    _augval623 = ((_augval623) < (_val625)) ? (_augval623) : (_val625);
                }
                var _child626 = (_a620)._right8;
                if (!((_child626) == null)) {
                    var _val627 = (_child626)._min_ax12;
                    _augval623 = ((_augval623) < (_val627)) ? (_augval623) : (_val627);
                }
                (_a620)._min_ax12 = _augval623;
                /* _min_ay13 is min of ay1 */
                var _augval628 = (_a620).ay1;
                var _child629 = (_a620)._left7;
                if (!((_child629) == null)) {
                    var _val630 = (_child629)._min_ay13;
                    _augval628 = ((_augval628) < (_val630)) ? (_augval628) : (_val630);
                }
                var _child631 = (_a620)._right8;
                if (!((_child631) == null)) {
                    var _val632 = (_child631)._min_ay13;
                    _augval628 = ((_augval628) < (_val632)) ? (_augval628) : (_val632);
                }
                (_a620)._min_ay13 = _augval628;
                /* _max_ay24 is max of ay2 */
                var _augval633 = (_a620).ay2;
                var _child634 = (_a620)._left7;
                if (!((_child634) == null)) {
                    var _val635 = (_child634)._max_ay24;
                    _augval633 = ((_augval633) < (_val635)) ? (_val635) : (_augval633);
                }
                var _child636 = (_a620)._right8;
                if (!((_child636) == null)) {
                    var _val637 = (_child636)._max_ay24;
                    _augval633 = ((_augval633) < (_val637)) ? (_val637) : (_augval633);
                }
                (_a620)._max_ay24 = _augval633;
                (_a620)._height10 = 1 + ((((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) > ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10))) ? ((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) : ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval638 = (_b621).ax1;
                var _child639 = (_b621)._left7;
                if (!((_child639) == null)) {
                    var _val640 = (_child639)._min_ax12;
                    _augval638 = ((_augval638) < (_val640)) ? (_augval638) : (_val640);
                }
                var _child641 = (_b621)._right8;
                if (!((_child641) == null)) {
                    var _val642 = (_child641)._min_ax12;
                    _augval638 = ((_augval638) < (_val642)) ? (_augval638) : (_val642);
                }
                (_b621)._min_ax12 = _augval638;
                /* _min_ay13 is min of ay1 */
                var _augval643 = (_b621).ay1;
                var _child644 = (_b621)._left7;
                if (!((_child644) == null)) {
                    var _val645 = (_child644)._min_ay13;
                    _augval643 = ((_augval643) < (_val645)) ? (_augval643) : (_val645);
                }
                var _child646 = (_b621)._right8;
                if (!((_child646) == null)) {
                    var _val647 = (_child646)._min_ay13;
                    _augval643 = ((_augval643) < (_val647)) ? (_augval643) : (_val647);
                }
                (_b621)._min_ay13 = _augval643;
                /* _max_ay24 is max of ay2 */
                var _augval648 = (_b621).ay2;
                var _child649 = (_b621)._left7;
                if (!((_child649) == null)) {
                    var _val650 = (_child649)._max_ay24;
                    _augval648 = ((_augval648) < (_val650)) ? (_val650) : (_augval648);
                }
                var _child651 = (_b621)._right8;
                if (!((_child651) == null)) {
                    var _val652 = (_child651)._max_ay24;
                    _augval648 = ((_augval648) < (_val652)) ? (_val652) : (_augval648);
                }
                (_b621)._max_ay24 = _augval648;
                (_b621)._height10 = 1 + ((((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) > ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10))) ? ((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) : ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10)));
                if (!(((_b621)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval653 = ((_b621)._parent9).ax1;
                    var _child654 = ((_b621)._parent9)._left7;
                    if (!((_child654) == null)) {
                        var _val655 = (_child654)._min_ax12;
                        _augval653 = ((_augval653) < (_val655)) ? (_augval653) : (_val655);
                    }
                    var _child656 = ((_b621)._parent9)._right8;
                    if (!((_child656) == null)) {
                        var _val657 = (_child656)._min_ax12;
                        _augval653 = ((_augval653) < (_val657)) ? (_augval653) : (_val657);
                    }
                    ((_b621)._parent9)._min_ax12 = _augval653;
                    /* _min_ay13 is min of ay1 */
                    var _augval658 = ((_b621)._parent9).ay1;
                    var _child659 = ((_b621)._parent9)._left7;
                    if (!((_child659) == null)) {
                        var _val660 = (_child659)._min_ay13;
                        _augval658 = ((_augval658) < (_val660)) ? (_augval658) : (_val660);
                    }
                    var _child661 = ((_b621)._parent9)._right8;
                    if (!((_child661) == null)) {
                        var _val662 = (_child661)._min_ay13;
                        _augval658 = ((_augval658) < (_val662)) ? (_augval658) : (_val662);
                    }
                    ((_b621)._parent9)._min_ay13 = _augval658;
                    /* _max_ay24 is max of ay2 */
                    var _augval663 = ((_b621)._parent9).ay2;
                    var _child664 = ((_b621)._parent9)._left7;
                    if (!((_child664) == null)) {
                        var _val665 = (_child664)._max_ay24;
                        _augval663 = ((_augval663) < (_val665)) ? (_val665) : (_augval663);
                    }
                    var _child666 = ((_b621)._parent9)._right8;
                    if (!((_child666) == null)) {
                        var _val667 = (_child666)._max_ay24;
                        _augval663 = ((_augval663) < (_val667)) ? (_val667) : (_augval663);
                    }
                    ((_b621)._parent9)._max_ay24 = _augval663;
                    ((_b621)._parent9)._height10 = 1 + (((((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) > (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10))) ? (((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) : (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b621;
                }
                _cursor474 = (_cursor474)._parent9;
            }
        }
        (__x).ax2 = new_val;
    }
}
RectangleHolder.prototype.updateAy2 = function (__x, new_val) {
    if ((__x).ay2 != new_val) {
        /* _max_ay24 is max of ay2 */
        var _augval668 = new_val;
        var _child669 = (__x)._left7;
        if (!((_child669) == null)) {
            var _val670 = (_child669)._max_ay24;
            _augval668 = ((_augval668) < (_val670)) ? (_val670) : (_augval668);
        }
        var _child671 = (__x)._right8;
        if (!((_child671) == null)) {
            var _val672 = (_child671)._max_ay24;
            _augval668 = ((_augval668) < (_val672)) ? (_val672) : (_augval668);
        }
        (__x)._max_ay24 = _augval668;
        var _cursor673 = (__x)._parent9;
        var _changed674 = true;
        while ((_changed674) && (!((_cursor673) == (null)))) {
            var _old__max_ay24675 = (_cursor673)._max_ay24;
            var _old_height676 = (_cursor673)._height10;
            /* _max_ay24 is max of ay2 */
            var _augval677 = (_cursor673).ay2;
            var _child678 = (_cursor673)._left7;
            if (!((_child678) == null)) {
                var _val679 = (_child678)._max_ay24;
                _augval677 = ((_augval677) < (_val679)) ? (_val679) : (_augval677);
            }
            var _child680 = (_cursor673)._right8;
            if (!((_child680) == null)) {
                var _val681 = (_child680)._max_ay24;
                _augval677 = ((_augval677) < (_val681)) ? (_val681) : (_augval677);
            }
            (_cursor673)._max_ay24 = _augval677;
            (_cursor673)._height10 = 1 + ((((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) > ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10))) ? ((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) : ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10)));
            _changed674 = false;
            _changed674 = (_changed674) || (!((_old__max_ay24675) == ((_cursor673)._max_ay24)));
            _changed674 = (_changed674) || (!((_old_height676) == ((_cursor673)._height10)));
            _cursor673 = (_cursor673)._parent9;
        }
        (__x).ay2 = new_val;
    }
}
RectangleHolder.prototype.update = function (__x, ax1, ay1, ax2, ay2) {
    var _parent682 = (__x)._parent9;
    var _left683 = (__x)._left7;
    var _right684 = (__x)._right8;
    var _new_x685;
    if (((_left683) == null) && ((_right684) == null)) {
        _new_x685 = null;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if ((!((_left683) == null)) && ((_right684) == null)) {
        _new_x685 = _left683;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if (((_left683) == null) && (!((_right684) == null))) {
        _new_x685 = _right684;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else {
        var _root686 = (__x)._right8;
        var _x687 = _root686;
        var _descend688 = true;
        var _from_left689 = true;
        while (true) {
            if ((_x687) == null) {
                _x687 = null;
                break;
            }
            if (_descend688) {
                /* too small? */
                if (false) {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                } else if ((!(((_x687)._left7) == null)) && (true)) {
                    _x687 = (_x687)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x687) == (_root686)) {
                    _root686 = (_x687)._right8;
                    _x687 = (_x687)._right8;
                } else {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                }
            } else if (_from_left689) {
                if (false) {
                    _x687 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x687)._right8) == null)) && (true)) {
                    _descend688 = true;
                    if ((_x687) == (_root686)) {
                        _root686 = (_x687)._right8;
                    }
                    _x687 = (_x687)._right8;
                } else if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            } else {
                if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            }
        }
        _new_x685 = _x687;
        var _mp690 = (_x687)._parent9;
        var _mr691 = (_x687)._right8;
        /* replace _x687 with _mr691 in _mp690 */
        if (!((_mp690) == null)) {
            if (((_mp690)._left7) == (_x687)) {
                (_mp690)._left7 = _mr691;
            } else {
                (_mp690)._right8 = _mr691;
            }
        }
        if (!((_mr691) == null)) {
            (_mr691)._parent9 = _mp690;
        }
        /* replace __x with _x687 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _x687;
            } else {
                (_parent682)._right8 = _x687;
            }
        }
        if (!((_x687) == null)) {
            (_x687)._parent9 = _parent682;
        }
        /* replace null with _left683 in _x687 */
        (_x687)._left7 = _left683;
        if (!((_left683) == null)) {
            (_left683)._parent9 = _x687;
        }
        /* replace _mr691 with (__x)._right8 in _x687 */
        (_x687)._right8 = (__x)._right8;
        if (!(((__x)._right8) == null)) {
            ((__x)._right8)._parent9 = _x687;
        }
        /* _min_ax12 is min of ax1 */
        var _augval692 = (_x687).ax1;
        var _child693 = (_x687)._left7;
        if (!((_child693) == null)) {
            var _val694 = (_child693)._min_ax12;
            _augval692 = ((_augval692) < (_val694)) ? (_augval692) : (_val694);
        }
        var _child695 = (_x687)._right8;
        if (!((_child695) == null)) {
            var _val696 = (_child695)._min_ax12;
            _augval692 = ((_augval692) < (_val696)) ? (_augval692) : (_val696);
        }
        (_x687)._min_ax12 = _augval692;
        /* _min_ay13 is min of ay1 */
        var _augval697 = (_x687).ay1;
        var _child698 = (_x687)._left7;
        if (!((_child698) == null)) {
            var _val699 = (_child698)._min_ay13;
            _augval697 = ((_augval697) < (_val699)) ? (_augval697) : (_val699);
        }
        var _child700 = (_x687)._right8;
        if (!((_child700) == null)) {
            var _val701 = (_child700)._min_ay13;
            _augval697 = ((_augval697) < (_val701)) ? (_augval697) : (_val701);
        }
        (_x687)._min_ay13 = _augval697;
        /* _max_ay24 is max of ay2 */
        var _augval702 = (_x687).ay2;
        var _child703 = (_x687)._left7;
        if (!((_child703) == null)) {
            var _val704 = (_child703)._max_ay24;
            _augval702 = ((_augval702) < (_val704)) ? (_val704) : (_augval702);
        }
        var _child705 = (_x687)._right8;
        if (!((_child705) == null)) {
            var _val706 = (_child705)._max_ay24;
            _augval702 = ((_augval702) < (_val706)) ? (_val706) : (_augval702);
        }
        (_x687)._max_ay24 = _augval702;
        (_x687)._height10 = 1 + ((((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) > ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10))) ? ((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) : ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10)));
        var _cursor707 = _mp690;
        var _changed708 = true;
        while ((_changed708) && (!((_cursor707) == (_parent682)))) {
            var _old__min_ax12709 = (_cursor707)._min_ax12;
            var _old__min_ay13710 = (_cursor707)._min_ay13;
            var _old__max_ay24711 = (_cursor707)._max_ay24;
            var _old_height712 = (_cursor707)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval713 = (_cursor707).ax1;
            var _child714 = (_cursor707)._left7;
            if (!((_child714) == null)) {
                var _val715 = (_child714)._min_ax12;
                _augval713 = ((_augval713) < (_val715)) ? (_augval713) : (_val715);
            }
            var _child716 = (_cursor707)._right8;
            if (!((_child716) == null)) {
                var _val717 = (_child716)._min_ax12;
                _augval713 = ((_augval713) < (_val717)) ? (_augval713) : (_val717);
            }
            (_cursor707)._min_ax12 = _augval713;
            /* _min_ay13 is min of ay1 */
            var _augval718 = (_cursor707).ay1;
            var _child719 = (_cursor707)._left7;
            if (!((_child719) == null)) {
                var _val720 = (_child719)._min_ay13;
                _augval718 = ((_augval718) < (_val720)) ? (_augval718) : (_val720);
            }
            var _child721 = (_cursor707)._right8;
            if (!((_child721) == null)) {
                var _val722 = (_child721)._min_ay13;
                _augval718 = ((_augval718) < (_val722)) ? (_augval718) : (_val722);
            }
            (_cursor707)._min_ay13 = _augval718;
            /* _max_ay24 is max of ay2 */
            var _augval723 = (_cursor707).ay2;
            var _child724 = (_cursor707)._left7;
            if (!((_child724) == null)) {
                var _val725 = (_child724)._max_ay24;
                _augval723 = ((_augval723) < (_val725)) ? (_val725) : (_augval723);
            }
            var _child726 = (_cursor707)._right8;
            if (!((_child726) == null)) {
                var _val727 = (_child726)._max_ay24;
                _augval723 = ((_augval723) < (_val727)) ? (_val727) : (_augval723);
            }
            (_cursor707)._max_ay24 = _augval723;
            (_cursor707)._height10 = 1 + ((((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) > ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10))) ? ((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) : ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10)));
            _changed708 = false;
            _changed708 = (_changed708) || (!((_old__min_ax12709) == ((_cursor707)._min_ax12)));
            _changed708 = (_changed708) || (!((_old__min_ay13710) == ((_cursor707)._min_ay13)));
            _changed708 = (_changed708) || (!((_old__max_ay24711) == ((_cursor707)._max_ay24)));
            _changed708 = (_changed708) || (!((_old_height712) == ((_cursor707)._height10)));
            _cursor707 = (_cursor707)._parent9;
        }
    }
    var _cursor728 = _parent682;
    var _changed729 = true;
    while ((_changed729) && (!((_cursor728) == (null)))) {
        var _old__min_ax12730 = (_cursor728)._min_ax12;
        var _old__min_ay13731 = (_cursor728)._min_ay13;
        var _old__max_ay24732 = (_cursor728)._max_ay24;
        var _old_height733 = (_cursor728)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval734 = (_cursor728).ax1;
        var _child735 = (_cursor728)._left7;
        if (!((_child735) == null)) {
            var _val736 = (_child735)._min_ax12;
            _augval734 = ((_augval734) < (_val736)) ? (_augval734) : (_val736);
        }
        var _child737 = (_cursor728)._right8;
        if (!((_child737) == null)) {
            var _val738 = (_child737)._min_ax12;
            _augval734 = ((_augval734) < (_val738)) ? (_augval734) : (_val738);
        }
        (_cursor728)._min_ax12 = _augval734;
        /* _min_ay13 is min of ay1 */
        var _augval739 = (_cursor728).ay1;
        var _child740 = (_cursor728)._left7;
        if (!((_child740) == null)) {
            var _val741 = (_child740)._min_ay13;
            _augval739 = ((_augval739) < (_val741)) ? (_augval739) : (_val741);
        }
        var _child742 = (_cursor728)._right8;
        if (!((_child742) == null)) {
            var _val743 = (_child742)._min_ay13;
            _augval739 = ((_augval739) < (_val743)) ? (_augval739) : (_val743);
        }
        (_cursor728)._min_ay13 = _augval739;
        /* _max_ay24 is max of ay2 */
        var _augval744 = (_cursor728).ay2;
        var _child745 = (_cursor728)._left7;
        if (!((_child745) == null)) {
            var _val746 = (_child745)._max_ay24;
            _augval744 = ((_augval744) < (_val746)) ? (_val746) : (_augval744);
        }
        var _child747 = (_cursor728)._right8;
        if (!((_child747) == null)) {
            var _val748 = (_child747)._max_ay24;
            _augval744 = ((_augval744) < (_val748)) ? (_val748) : (_augval744);
        }
        (_cursor728)._max_ay24 = _augval744;
        (_cursor728)._height10 = 1 + ((((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) > ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10))) ? ((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) : ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10)));
        _changed729 = false;
        _changed729 = (_changed729) || (!((_old__min_ax12730) == ((_cursor728)._min_ax12)));
        _changed729 = (_changed729) || (!((_old__min_ay13731) == ((_cursor728)._min_ay13)));
        _changed729 = (_changed729) || (!((_old__max_ay24732) == ((_cursor728)._max_ay24)));
        _changed729 = (_changed729) || (!((_old_height733) == ((_cursor728)._height10)));
        _cursor728 = (_cursor728)._parent9;
    }
    if (((this)._root1) == (__x)) {
        (this)._root1 = _new_x685;
    }
    (__x)._left7 = null;
    (__x)._right8 = null;
    (__x)._min_ax12 = (__x).ax1;
    (__x)._min_ay13 = (__x).ay1;
    (__x)._max_ay24 = (__x).ay2;
    (__x)._height10 = 0;
    var _previous749 = null;
    var _current750 = (this)._root1;
    var _is_left751 = false;
    while (!((_current750) == null)) {
        _previous749 = _current750;
        if ((ax2) < ((_current750).ax2)) {
            _current750 = (_current750)._left7;
            _is_left751 = true;
        } else {
            _current750 = (_current750)._right8;
            _is_left751 = false;
        }
    }
    if ((_previous749) == null) {
        (this)._root1 = __x;
    } else {
        (__x)._parent9 = _previous749;
        if (_is_left751) {
            (_previous749)._left7 = __x;
        } else {
            (_previous749)._right8 = __x;
        }
    }
    var _cursor752 = (__x)._parent9;
    var _changed753 = true;
    while ((_changed753) && (!((_cursor752) == (null)))) {
        var _old__min_ax12754 = (_cursor752)._min_ax12;
        var _old__min_ay13755 = (_cursor752)._min_ay13;
        var _old__max_ay24756 = (_cursor752)._max_ay24;
        var _old_height757 = (_cursor752)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval758 = (_cursor752).ax1;
        var _child759 = (_cursor752)._left7;
        if (!((_child759) == null)) {
            var _val760 = (_child759)._min_ax12;
            _augval758 = ((_augval758) < (_val760)) ? (_augval758) : (_val760);
        }
        var _child761 = (_cursor752)._right8;
        if (!((_child761) == null)) {
            var _val762 = (_child761)._min_ax12;
            _augval758 = ((_augval758) < (_val762)) ? (_augval758) : (_val762);
        }
        (_cursor752)._min_ax12 = _augval758;
        /* _min_ay13 is min of ay1 */
        var _augval763 = (_cursor752).ay1;
        var _child764 = (_cursor752)._left7;
        if (!((_child764) == null)) {
            var _val765 = (_child764)._min_ay13;
            _augval763 = ((_augval763) < (_val765)) ? (_augval763) : (_val765);
        }
        var _child766 = (_cursor752)._right8;
        if (!((_child766) == null)) {
            var _val767 = (_child766)._min_ay13;
            _augval763 = ((_augval763) < (_val767)) ? (_augval763) : (_val767);
        }
        (_cursor752)._min_ay13 = _augval763;
        /* _max_ay24 is max of ay2 */
        var _augval768 = (_cursor752).ay2;
        var _child769 = (_cursor752)._left7;
        if (!((_child769) == null)) {
            var _val770 = (_child769)._max_ay24;
            _augval768 = ((_augval768) < (_val770)) ? (_val770) : (_augval768);
        }
        var _child771 = (_cursor752)._right8;
        if (!((_child771) == null)) {
            var _val772 = (_child771)._max_ay24;
            _augval768 = ((_augval768) < (_val772)) ? (_val772) : (_augval768);
        }
        (_cursor752)._max_ay24 = _augval768;
        (_cursor752)._height10 = 1 + ((((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) > ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10))) ? ((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) : ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10)));
        _changed753 = false;
        _changed753 = (_changed753) || (!((_old__min_ax12754) == ((_cursor752)._min_ax12)));
        _changed753 = (_changed753) || (!((_old__min_ay13755) == ((_cursor752)._min_ay13)));
        _changed753 = (_changed753) || (!((_old__max_ay24756) == ((_cursor752)._max_ay24)));
        _changed753 = (_changed753) || (!((_old_height757) == ((_cursor752)._height10)));
        _cursor752 = (_cursor752)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor773 = __x;
    var _imbalance774;
    while (!(((_cursor773)._parent9) == null)) {
        _cursor773 = (_cursor773)._parent9;
        (_cursor773)._height10 = 1 + ((((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) > ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10))) ? ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) : ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10)));
        _imbalance774 = ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) - ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10));
        if ((_imbalance774) > (1)) {
            if ((((((_cursor773)._left7)._left7) == null) ? (-1) : ((((_cursor773)._left7)._left7)._height10)) < (((((_cursor773)._left7)._right8) == null) ? (-1) : ((((_cursor773)._left7)._right8)._height10))) {
                /* rotate ((_cursor773)._left7)._right8 */
                var _a775 = (_cursor773)._left7;
                var _b776 = (_a775)._right8;
                var _c777 = (_b776)._left7;
                /* replace _a775 with _b776 in (_a775)._parent9 */
                if (!(((_a775)._parent9) == null)) {
                    if ((((_a775)._parent9)._left7) == (_a775)) {
                        ((_a775)._parent9)._left7 = _b776;
                    } else {
                        ((_a775)._parent9)._right8 = _b776;
                    }
                }
                if (!((_b776) == null)) {
                    (_b776)._parent9 = (_a775)._parent9;
                }
                /* replace _c777 with _a775 in _b776 */
                (_b776)._left7 = _a775;
                if (!((_a775) == null)) {
                    (_a775)._parent9 = _b776;
                }
                /* replace _b776 with _c777 in _a775 */
                (_a775)._right8 = _c777;
                if (!((_c777) == null)) {
                    (_c777)._parent9 = _a775;
                }
                /* _min_ax12 is min of ax1 */
                var _augval778 = (_a775).ax1;
                var _child779 = (_a775)._left7;
                if (!((_child779) == null)) {
                    var _val780 = (_child779)._min_ax12;
                    _augval778 = ((_augval778) < (_val780)) ? (_augval778) : (_val780);
                }
                var _child781 = (_a775)._right8;
                if (!((_child781) == null)) {
                    var _val782 = (_child781)._min_ax12;
                    _augval778 = ((_augval778) < (_val782)) ? (_augval778) : (_val782);
                }
                (_a775)._min_ax12 = _augval778;
                /* _min_ay13 is min of ay1 */
                var _augval783 = (_a775).ay1;
                var _child784 = (_a775)._left7;
                if (!((_child784) == null)) {
                    var _val785 = (_child784)._min_ay13;
                    _augval783 = ((_augval783) < (_val785)) ? (_augval783) : (_val785);
                }
                var _child786 = (_a775)._right8;
                if (!((_child786) == null)) {
                    var _val787 = (_child786)._min_ay13;
                    _augval783 = ((_augval783) < (_val787)) ? (_augval783) : (_val787);
                }
                (_a775)._min_ay13 = _augval783;
                /* _max_ay24 is max of ay2 */
                var _augval788 = (_a775).ay2;
                var _child789 = (_a775)._left7;
                if (!((_child789) == null)) {
                    var _val790 = (_child789)._max_ay24;
                    _augval788 = ((_augval788) < (_val790)) ? (_val790) : (_augval788);
                }
                var _child791 = (_a775)._right8;
                if (!((_child791) == null)) {
                    var _val792 = (_child791)._max_ay24;
                    _augval788 = ((_augval788) < (_val792)) ? (_val792) : (_augval788);
                }
                (_a775)._max_ay24 = _augval788;
                (_a775)._height10 = 1 + ((((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) > ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10))) ? ((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) : ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval793 = (_b776).ax1;
                var _child794 = (_b776)._left7;
                if (!((_child794) == null)) {
                    var _val795 = (_child794)._min_ax12;
                    _augval793 = ((_augval793) < (_val795)) ? (_augval793) : (_val795);
                }
                var _child796 = (_b776)._right8;
                if (!((_child796) == null)) {
                    var _val797 = (_child796)._min_ax12;
                    _augval793 = ((_augval793) < (_val797)) ? (_augval793) : (_val797);
                }
                (_b776)._min_ax12 = _augval793;
                /* _min_ay13 is min of ay1 */
                var _augval798 = (_b776).ay1;
                var _child799 = (_b776)._left7;
                if (!((_child799) == null)) {
                    var _val800 = (_child799)._min_ay13;
                    _augval798 = ((_augval798) < (_val800)) ? (_augval798) : (_val800);
                }
                var _child801 = (_b776)._right8;
                if (!((_child801) == null)) {
                    var _val802 = (_child801)._min_ay13;
                    _augval798 = ((_augval798) < (_val802)) ? (_augval798) : (_val802);
                }
                (_b776)._min_ay13 = _augval798;
                /* _max_ay24 is max of ay2 */
                var _augval803 = (_b776).ay2;
                var _child804 = (_b776)._left7;
                if (!((_child804) == null)) {
                    var _val805 = (_child804)._max_ay24;
                    _augval803 = ((_augval803) < (_val805)) ? (_val805) : (_augval803);
                }
                var _child806 = (_b776)._right8;
                if (!((_child806) == null)) {
                    var _val807 = (_child806)._max_ay24;
                    _augval803 = ((_augval803) < (_val807)) ? (_val807) : (_augval803);
                }
                (_b776)._max_ay24 = _augval803;
                (_b776)._height10 = 1 + ((((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) > ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10))) ? ((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) : ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10)));
                if (!(((_b776)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval808 = ((_b776)._parent9).ax1;
                    var _child809 = ((_b776)._parent9)._left7;
                    if (!((_child809) == null)) {
                        var _val810 = (_child809)._min_ax12;
                        _augval808 = ((_augval808) < (_val810)) ? (_augval808) : (_val810);
                    }
                    var _child811 = ((_b776)._parent9)._right8;
                    if (!((_child811) == null)) {
                        var _val812 = (_child811)._min_ax12;
                        _augval808 = ((_augval808) < (_val812)) ? (_augval808) : (_val812);
                    }
                    ((_b776)._parent9)._min_ax12 = _augval808;
                    /* _min_ay13 is min of ay1 */
                    var _augval813 = ((_b776)._parent9).ay1;
                    var _child814 = ((_b776)._parent9)._left7;
                    if (!((_child814) == null)) {
                        var _val815 = (_child814)._min_ay13;
                        _augval813 = ((_augval813) < (_val815)) ? (_augval813) : (_val815);
                    }
                    var _child816 = ((_b776)._parent9)._right8;
                    if (!((_child816) == null)) {
                        var _val817 = (_child816)._min_ay13;
                        _augval813 = ((_augval813) < (_val817)) ? (_augval813) : (_val817);
                    }
                    ((_b776)._parent9)._min_ay13 = _augval813;
                    /* _max_ay24 is max of ay2 */
                    var _augval818 = ((_b776)._parent9).ay2;
                    var _child819 = ((_b776)._parent9)._left7;
                    if (!((_child819) == null)) {
                        var _val820 = (_child819)._max_ay24;
                        _augval818 = ((_augval818) < (_val820)) ? (_val820) : (_augval818);
                    }
                    var _child821 = ((_b776)._parent9)._right8;
                    if (!((_child821) == null)) {
                        var _val822 = (_child821)._max_ay24;
                        _augval818 = ((_augval818) < (_val822)) ? (_val822) : (_augval818);
                    }
                    ((_b776)._parent9)._max_ay24 = _augval818;
                    ((_b776)._parent9)._height10 = 1 + (((((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) > (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10))) ? (((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) : (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b776;
                }
            }
            /* rotate (_cursor773)._left7 */
            var _a823 = _cursor773;
            var _b824 = (_a823)._left7;
            var _c825 = (_b824)._right8;
            /* replace _a823 with _b824 in (_a823)._parent9 */
            if (!(((_a823)._parent9) == null)) {
                if ((((_a823)._parent9)._left7) == (_a823)) {
                    ((_a823)._parent9)._left7 = _b824;
                } else {
                    ((_a823)._parent9)._right8 = _b824;
                }
            }
            if (!((_b824) == null)) {
                (_b824)._parent9 = (_a823)._parent9;
            }
            /* replace _c825 with _a823 in _b824 */
            (_b824)._right8 = _a823;
            if (!((_a823) == null)) {
                (_a823)._parent9 = _b824;
            }
            /* replace _b824 with _c825 in _a823 */
            (_a823)._left7 = _c825;
            if (!((_c825) == null)) {
                (_c825)._parent9 = _a823;
            }
            /* _min_ax12 is min of ax1 */
            var _augval826 = (_a823).ax1;
            var _child827 = (_a823)._left7;
            if (!((_child827) == null)) {
                var _val828 = (_child827)._min_ax12;
                _augval826 = ((_augval826) < (_val828)) ? (_augval826) : (_val828);
            }
            var _child829 = (_a823)._right8;
            if (!((_child829) == null)) {
                var _val830 = (_child829)._min_ax12;
                _augval826 = ((_augval826) < (_val830)) ? (_augval826) : (_val830);
            }
            (_a823)._min_ax12 = _augval826;
            /* _min_ay13 is min of ay1 */
            var _augval831 = (_a823).ay1;
            var _child832 = (_a823)._left7;
            if (!((_child832) == null)) {
                var _val833 = (_child832)._min_ay13;
                _augval831 = ((_augval831) < (_val833)) ? (_augval831) : (_val833);
            }
            var _child834 = (_a823)._right8;
            if (!((_child834) == null)) {
                var _val835 = (_child834)._min_ay13;
                _augval831 = ((_augval831) < (_val835)) ? (_augval831) : (_val835);
            }
            (_a823)._min_ay13 = _augval831;
            /* _max_ay24 is max of ay2 */
            var _augval836 = (_a823).ay2;
            var _child837 = (_a823)._left7;
            if (!((_child837) == null)) {
                var _val838 = (_child837)._max_ay24;
                _augval836 = ((_augval836) < (_val838)) ? (_val838) : (_augval836);
            }
            var _child839 = (_a823)._right8;
            if (!((_child839) == null)) {
                var _val840 = (_child839)._max_ay24;
                _augval836 = ((_augval836) < (_val840)) ? (_val840) : (_augval836);
            }
            (_a823)._max_ay24 = _augval836;
            (_a823)._height10 = 1 + ((((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) > ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10))) ? ((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) : ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval841 = (_b824).ax1;
            var _child842 = (_b824)._left7;
            if (!((_child842) == null)) {
                var _val843 = (_child842)._min_ax12;
                _augval841 = ((_augval841) < (_val843)) ? (_augval841) : (_val843);
            }
            var _child844 = (_b824)._right8;
            if (!((_child844) == null)) {
                var _val845 = (_child844)._min_ax12;
                _augval841 = ((_augval841) < (_val845)) ? (_augval841) : (_val845);
            }
            (_b824)._min_ax12 = _augval841;
            /* _min_ay13 is min of ay1 */
            var _augval846 = (_b824).ay1;
            var _child847 = (_b824)._left7;
            if (!((_child847) == null)) {
                var _val848 = (_child847)._min_ay13;
                _augval846 = ((_augval846) < (_val848)) ? (_augval846) : (_val848);
            }
            var _child849 = (_b824)._right8;
            if (!((_child849) == null)) {
                var _val850 = (_child849)._min_ay13;
                _augval846 = ((_augval846) < (_val850)) ? (_augval846) : (_val850);
            }
            (_b824)._min_ay13 = _augval846;
            /* _max_ay24 is max of ay2 */
            var _augval851 = (_b824).ay2;
            var _child852 = (_b824)._left7;
            if (!((_child852) == null)) {
                var _val853 = (_child852)._max_ay24;
                _augval851 = ((_augval851) < (_val853)) ? (_val853) : (_augval851);
            }
            var _child854 = (_b824)._right8;
            if (!((_child854) == null)) {
                var _val855 = (_child854)._max_ay24;
                _augval851 = ((_augval851) < (_val855)) ? (_val855) : (_augval851);
            }
            (_b824)._max_ay24 = _augval851;
            (_b824)._height10 = 1 + ((((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) > ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10))) ? ((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) : ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10)));
            if (!(((_b824)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval856 = ((_b824)._parent9).ax1;
                var _child857 = ((_b824)._parent9)._left7;
                if (!((_child857) == null)) {
                    var _val858 = (_child857)._min_ax12;
                    _augval856 = ((_augval856) < (_val858)) ? (_augval856) : (_val858);
                }
                var _child859 = ((_b824)._parent9)._right8;
                if (!((_child859) == null)) {
                    var _val860 = (_child859)._min_ax12;
                    _augval856 = ((_augval856) < (_val860)) ? (_augval856) : (_val860);
                }
                ((_b824)._parent9)._min_ax12 = _augval856;
                /* _min_ay13 is min of ay1 */
                var _augval861 = ((_b824)._parent9).ay1;
                var _child862 = ((_b824)._parent9)._left7;
                if (!((_child862) == null)) {
                    var _val863 = (_child862)._min_ay13;
                    _augval861 = ((_augval861) < (_val863)) ? (_augval861) : (_val863);
                }
                var _child864 = ((_b824)._parent9)._right8;
                if (!((_child864) == null)) {
                    var _val865 = (_child864)._min_ay13;
                    _augval861 = ((_augval861) < (_val865)) ? (_augval861) : (_val865);
                }
                ((_b824)._parent9)._min_ay13 = _augval861;
                /* _max_ay24 is max of ay2 */
                var _augval866 = ((_b824)._parent9).ay2;
                var _child867 = ((_b824)._parent9)._left7;
                if (!((_child867) == null)) {
                    var _val868 = (_child867)._max_ay24;
                    _augval866 = ((_augval866) < (_val868)) ? (_val868) : (_augval866);
                }
                var _child869 = ((_b824)._parent9)._right8;
                if (!((_child869) == null)) {
                    var _val870 = (_child869)._max_ay24;
                    _augval866 = ((_augval866) < (_val870)) ? (_val870) : (_augval866);
                }
                ((_b824)._parent9)._max_ay24 = _augval866;
                ((_b824)._parent9)._height10 = 1 + (((((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) > (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10))) ? (((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) : (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b824;
            }
            _cursor773 = (_cursor773)._parent9;
        } else if ((_imbalance774) < (-1)) {
            if ((((((_cursor773)._right8)._left7) == null) ? (-1) : ((((_cursor773)._right8)._left7)._height10)) > (((((_cursor773)._right8)._right8) == null) ? (-1) : ((((_cursor773)._right8)._right8)._height10))) {
                /* rotate ((_cursor773)._right8)._left7 */
                var _a871 = (_cursor773)._right8;
                var _b872 = (_a871)._left7;
                var _c873 = (_b872)._right8;
                /* replace _a871 with _b872 in (_a871)._parent9 */
                if (!(((_a871)._parent9) == null)) {
                    if ((((_a871)._parent9)._left7) == (_a871)) {
                        ((_a871)._parent9)._left7 = _b872;
                    } else {
                        ((_a871)._parent9)._right8 = _b872;
                    }
                }
                if (!((_b872) == null)) {
                    (_b872)._parent9 = (_a871)._parent9;
                }
                /* replace _c873 with _a871 in _b872 */
                (_b872)._right8 = _a871;
                if (!((_a871) == null)) {
                    (_a871)._parent9 = _b872;
                }
                /* replace _b872 with _c873 in _a871 */
                (_a871)._left7 = _c873;
                if (!((_c873) == null)) {
                    (_c873)._parent9 = _a871;
                }
                /* _min_ax12 is min of ax1 */
                var _augval874 = (_a871).ax1;
                var _child875 = (_a871)._left7;
                if (!((_child875) == null)) {
                    var _val876 = (_child875)._min_ax12;
                    _augval874 = ((_augval874) < (_val876)) ? (_augval874) : (_val876);
                }
                var _child877 = (_a871)._right8;
                if (!((_child877) == null)) {
                    var _val878 = (_child877)._min_ax12;
                    _augval874 = ((_augval874) < (_val878)) ? (_augval874) : (_val878);
                }
                (_a871)._min_ax12 = _augval874;
                /* _min_ay13 is min of ay1 */
                var _augval879 = (_a871).ay1;
                var _child880 = (_a871)._left7;
                if (!((_child880) == null)) {
                    var _val881 = (_child880)._min_ay13;
                    _augval879 = ((_augval879) < (_val881)) ? (_augval879) : (_val881);
                }
                var _child882 = (_a871)._right8;
                if (!((_child882) == null)) {
                    var _val883 = (_child882)._min_ay13;
                    _augval879 = ((_augval879) < (_val883)) ? (_augval879) : (_val883);
                }
                (_a871)._min_ay13 = _augval879;
                /* _max_ay24 is max of ay2 */
                var _augval884 = (_a871).ay2;
                var _child885 = (_a871)._left7;
                if (!((_child885) == null)) {
                    var _val886 = (_child885)._max_ay24;
                    _augval884 = ((_augval884) < (_val886)) ? (_val886) : (_augval884);
                }
                var _child887 = (_a871)._right8;
                if (!((_child887) == null)) {
                    var _val888 = (_child887)._max_ay24;
                    _augval884 = ((_augval884) < (_val888)) ? (_val888) : (_augval884);
                }
                (_a871)._max_ay24 = _augval884;
                (_a871)._height10 = 1 + ((((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) > ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10))) ? ((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) : ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval889 = (_b872).ax1;
                var _child890 = (_b872)._left7;
                if (!((_child890) == null)) {
                    var _val891 = (_child890)._min_ax12;
                    _augval889 = ((_augval889) < (_val891)) ? (_augval889) : (_val891);
                }
                var _child892 = (_b872)._right8;
                if (!((_child892) == null)) {
                    var _val893 = (_child892)._min_ax12;
                    _augval889 = ((_augval889) < (_val893)) ? (_augval889) : (_val893);
                }
                (_b872)._min_ax12 = _augval889;
                /* _min_ay13 is min of ay1 */
                var _augval894 = (_b872).ay1;
                var _child895 = (_b872)._left7;
                if (!((_child895) == null)) {
                    var _val896 = (_child895)._min_ay13;
                    _augval894 = ((_augval894) < (_val896)) ? (_augval894) : (_val896);
                }
                var _child897 = (_b872)._right8;
                if (!((_child897) == null)) {
                    var _val898 = (_child897)._min_ay13;
                    _augval894 = ((_augval894) < (_val898)) ? (_augval894) : (_val898);
                }
                (_b872)._min_ay13 = _augval894;
                /* _max_ay24 is max of ay2 */
                var _augval899 = (_b872).ay2;
                var _child900 = (_b872)._left7;
                if (!((_child900) == null)) {
                    var _val901 = (_child900)._max_ay24;
                    _augval899 = ((_augval899) < (_val901)) ? (_val901) : (_augval899);
                }
                var _child902 = (_b872)._right8;
                if (!((_child902) == null)) {
                    var _val903 = (_child902)._max_ay24;
                    _augval899 = ((_augval899) < (_val903)) ? (_val903) : (_augval899);
                }
                (_b872)._max_ay24 = _augval899;
                (_b872)._height10 = 1 + ((((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) > ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10))) ? ((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) : ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10)));
                if (!(((_b872)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval904 = ((_b872)._parent9).ax1;
                    var _child905 = ((_b872)._parent9)._left7;
                    if (!((_child905) == null)) {
                        var _val906 = (_child905)._min_ax12;
                        _augval904 = ((_augval904) < (_val906)) ? (_augval904) : (_val906);
                    }
                    var _child907 = ((_b872)._parent9)._right8;
                    if (!((_child907) == null)) {
                        var _val908 = (_child907)._min_ax12;
                        _augval904 = ((_augval904) < (_val908)) ? (_augval904) : (_val908);
                    }
                    ((_b872)._parent9)._min_ax12 = _augval904;
                    /* _min_ay13 is min of ay1 */
                    var _augval909 = ((_b872)._parent9).ay1;
                    var _child910 = ((_b872)._parent9)._left7;
                    if (!((_child910) == null)) {
                        var _val911 = (_child910)._min_ay13;
                        _augval909 = ((_augval909) < (_val911)) ? (_augval909) : (_val911);
                    }
                    var _child912 = ((_b872)._parent9)._right8;
                    if (!((_child912) == null)) {
                        var _val913 = (_child912)._min_ay13;
                        _augval909 = ((_augval909) < (_val913)) ? (_augval909) : (_val913);
                    }
                    ((_b872)._parent9)._min_ay13 = _augval909;
                    /* _max_ay24 is max of ay2 */
                    var _augval914 = ((_b872)._parent9).ay2;
                    var _child915 = ((_b872)._parent9)._left7;
                    if (!((_child915) == null)) {
                        var _val916 = (_child915)._max_ay24;
                        _augval914 = ((_augval914) < (_val916)) ? (_val916) : (_augval914);
                    }
                    var _child917 = ((_b872)._parent9)._right8;
                    if (!((_child917) == null)) {
                        var _val918 = (_child917)._max_ay24;
                        _augval914 = ((_augval914) < (_val918)) ? (_val918) : (_augval914);
                    }
                    ((_b872)._parent9)._max_ay24 = _augval914;
                    ((_b872)._parent9)._height10 = 1 + (((((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) > (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10))) ? (((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) : (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b872;
                }
            }
            /* rotate (_cursor773)._right8 */
            var _a919 = _cursor773;
            var _b920 = (_a919)._right8;
            var _c921 = (_b920)._left7;
            /* replace _a919 with _b920 in (_a919)._parent9 */
            if (!(((_a919)._parent9) == null)) {
                if ((((_a919)._parent9)._left7) == (_a919)) {
                    ((_a919)._parent9)._left7 = _b920;
                } else {
                    ((_a919)._parent9)._right8 = _b920;
                }
            }
            if (!((_b920) == null)) {
                (_b920)._parent9 = (_a919)._parent9;
            }
            /* replace _c921 with _a919 in _b920 */
            (_b920)._left7 = _a919;
            if (!((_a919) == null)) {
                (_a919)._parent9 = _b920;
            }
            /* replace _b920 with _c921 in _a919 */
            (_a919)._right8 = _c921;
            if (!((_c921) == null)) {
                (_c921)._parent9 = _a919;
            }
            /* _min_ax12 is min of ax1 */
            var _augval922 = (_a919).ax1;
            var _child923 = (_a919)._left7;
            if (!((_child923) == null)) {
                var _val924 = (_child923)._min_ax12;
                _augval922 = ((_augval922) < (_val924)) ? (_augval922) : (_val924);
            }
            var _child925 = (_a919)._right8;
            if (!((_child925) == null)) {
                var _val926 = (_child925)._min_ax12;
                _augval922 = ((_augval922) < (_val926)) ? (_augval922) : (_val926);
            }
            (_a919)._min_ax12 = _augval922;
            /* _min_ay13 is min of ay1 */
            var _augval927 = (_a919).ay1;
            var _child928 = (_a919)._left7;
            if (!((_child928) == null)) {
                var _val929 = (_child928)._min_ay13;
                _augval927 = ((_augval927) < (_val929)) ? (_augval927) : (_val929);
            }
            var _child930 = (_a919)._right8;
            if (!((_child930) == null)) {
                var _val931 = (_child930)._min_ay13;
                _augval927 = ((_augval927) < (_val931)) ? (_augval927) : (_val931);
            }
            (_a919)._min_ay13 = _augval927;
            /* _max_ay24 is max of ay2 */
            var _augval932 = (_a919).ay2;
            var _child933 = (_a919)._left7;
            if (!((_child933) == null)) {
                var _val934 = (_child933)._max_ay24;
                _augval932 = ((_augval932) < (_val934)) ? (_val934) : (_augval932);
            }
            var _child935 = (_a919)._right8;
            if (!((_child935) == null)) {
                var _val936 = (_child935)._max_ay24;
                _augval932 = ((_augval932) < (_val936)) ? (_val936) : (_augval932);
            }
            (_a919)._max_ay24 = _augval932;
            (_a919)._height10 = 1 + ((((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) > ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10))) ? ((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) : ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval937 = (_b920).ax1;
            var _child938 = (_b920)._left7;
            if (!((_child938) == null)) {
                var _val939 = (_child938)._min_ax12;
                _augval937 = ((_augval937) < (_val939)) ? (_augval937) : (_val939);
            }
            var _child940 = (_b920)._right8;
            if (!((_child940) == null)) {
                var _val941 = (_child940)._min_ax12;
                _augval937 = ((_augval937) < (_val941)) ? (_augval937) : (_val941);
            }
            (_b920)._min_ax12 = _augval937;
            /* _min_ay13 is min of ay1 */
            var _augval942 = (_b920).ay1;
            var _child943 = (_b920)._left7;
            if (!((_child943) == null)) {
                var _val944 = (_child943)._min_ay13;
                _augval942 = ((_augval942) < (_val944)) ? (_augval942) : (_val944);
            }
            var _child945 = (_b920)._right8;
            if (!((_child945) == null)) {
                var _val946 = (_child945)._min_ay13;
                _augval942 = ((_augval942) < (_val946)) ? (_augval942) : (_val946);
            }
            (_b920)._min_ay13 = _augval942;
            /* _max_ay24 is max of ay2 */
            var _augval947 = (_b920).ay2;
            var _child948 = (_b920)._left7;
            if (!((_child948) == null)) {
                var _val949 = (_child948)._max_ay24;
                _augval947 = ((_augval947) < (_val949)) ? (_val949) : (_augval947);
            }
            var _child950 = (_b920)._right8;
            if (!((_child950) == null)) {
                var _val951 = (_child950)._max_ay24;
                _augval947 = ((_augval947) < (_val951)) ? (_val951) : (_augval947);
            }
            (_b920)._max_ay24 = _augval947;
            (_b920)._height10 = 1 + ((((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) > ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10))) ? ((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) : ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10)));
            if (!(((_b920)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval952 = ((_b920)._parent9).ax1;
                var _child953 = ((_b920)._parent9)._left7;
                if (!((_child953) == null)) {
                    var _val954 = (_child953)._min_ax12;
                    _augval952 = ((_augval952) < (_val954)) ? (_augval952) : (_val954);
                }
                var _child955 = ((_b920)._parent9)._right8;
                if (!((_child955) == null)) {
                    var _val956 = (_child955)._min_ax12;
                    _augval952 = ((_augval952) < (_val956)) ? (_augval952) : (_val956);
                }
                ((_b920)._parent9)._min_ax12 = _augval952;
                /* _min_ay13 is min of ay1 */
                var _augval957 = ((_b920)._parent9).ay1;
                var _child958 = ((_b920)._parent9)._left7;
                if (!((_child958) == null)) {
                    var _val959 = (_child958)._min_ay13;
                    _augval957 = ((_augval957) < (_val959)) ? (_augval957) : (_val959);
                }
                var _child960 = ((_b920)._parent9)._right8;
                if (!((_child960) == null)) {
                    var _val961 = (_child960)._min_ay13;
                    _augval957 = ((_augval957) < (_val961)) ? (_augval957) : (_val961);
                }
                ((_b920)._parent9)._min_ay13 = _augval957;
                /* _max_ay24 is max of ay2 */
                var _augval962 = ((_b920)._parent9).ay2;
                var _child963 = ((_b920)._parent9)._left7;
                if (!((_child963) == null)) {
                    var _val964 = (_child963)._max_ay24;
                    _augval962 = ((_augval962) < (_val964)) ? (_val964) : (_augval962);
                }
                var _child965 = ((_b920)._parent9)._right8;
                if (!((_child965) == null)) {
                    var _val966 = (_child965)._max_ay24;
                    _augval962 = ((_augval962) < (_val966)) ? (_val966) : (_augval962);
                }
                ((_b920)._parent9)._max_ay24 = _augval962;
                ((_b920)._parent9)._height10 = 1 + (((((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) > (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10))) ? (((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) : (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b920;
            }
            _cursor773 = (_cursor773)._parent9;
        }
    }
    (__x).ax1 = ax1;
    (__x).ay1 = ay1;
    (__x).ax2 = ax2;
    (__x).ay2 = ay2;
}
RectangleHolder.prototype.findMatchingRectangles = function (bx1, by1, bx2, by2, __callback) {
    var _root967 = (this)._root1;
    var _x968 = _root967;
    var _descend969 = true;
    var _from_left970 = true;
    while (true) {
        if ((_x968) == null) {
            _x968 = null;
            break;
        }
        if (_descend969) {
            /* too small? */
            if ((false) || (((_x968).ax2) <= (bx1))) {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            } else if ((!(((_x968)._left7) == null)) && ((((true) && ((((_x968)._left7)._min_ax12) < (bx2))) && ((((_x968)._left7)._min_ay13) < (by2))) && ((((_x968)._left7)._max_ay24) > (by1)))) {
                _x968 = (_x968)._left7;
                /* too large? */
            } else if (false) {
                if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
                /* node ok? */
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((_x968) == (_root967)) {
                _root967 = (_x968)._right8;
                _x968 = (_x968)._right8;
            } else {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            }
        } else if (_from_left970) {
            if (false) {
                _x968 = null;
                break;
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                _descend969 = true;
                if ((_x968) == (_root967)) {
                    _root967 = (_x968)._right8;
                }
                _x968 = (_x968)._right8;
            } else if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        } else {
            if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        }
    }
    var _prev_cursor5 = null;
    var _cursor6 = _x968;
    for (; ;) {
        if (!(!((_cursor6) == null))) break;
        var _name971 = _cursor6;
        /* ADVANCE */
        _prev_cursor5 = _cursor6;
        do {
            var _right_min972 = null;
            if ((!(((_cursor6)._right8) == null)) && ((((true) && ((((_cursor6)._right8)._min_ax12) < (bx2))) && ((((_cursor6)._right8)._min_ay13) < (by2))) && ((((_cursor6)._right8)._max_ay24) > (by1)))) {
                var _root973 = (_cursor6)._right8;
                var _x974 = _root973;
                var _descend975 = true;
                var _from_left976 = true;
                while (true) {
                    if ((_x974) == null) {
                        _x974 = null;
                        break;
                    }
                    if (_descend975) {
                        /* too small? */
                        if ((false) || (((_x974).ax2) <= (bx1))) {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        } else if ((!(((_x974)._left7) == null)) && ((((true) && ((((_x974)._left7)._min_ax12) < (bx2))) && ((((_x974)._left7)._min_ay13) < (by2))) && ((((_x974)._left7)._max_ay24) > (by1)))) {
                            _x974 = (_x974)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                            /* node ok? */
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((_x974) == (_root973)) {
                            _root973 = (_x974)._right8;
                            _x974 = (_x974)._right8;
                        } else {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        }
                    } else if (_from_left976) {
                        if (false) {
                            _x974 = null;
                            break;
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                            _descend975 = true;
                            if ((_x974) == (_root973)) {
                                _root973 = (_x974)._right8;
                            }
                            _x974 = (_x974)._right8;
                        } else if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    } else {
                        if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    }
                }
                _right_min972 = _x974;
            }
            if (!((_right_min972) == null)) {
                _cursor6 = _right_min972;
                break;
            } else {
                while ((!(((_cursor6)._parent9) == null)) && ((_cursor6) == (((_cursor6)._parent9)._right8))) {
                    _cursor6 = (_cursor6)._parent9;
                }
                _cursor6 = (_cursor6)._parent9;
                if ((!((_cursor6) == null)) && (false)) {
                    _cursor6 = null;
                }
            }
        } while ((!((_cursor6) == null)) && (!((((true) && (((_cursor6).ax1) < (bx2))) && (((_cursor6).ay1) < (by2))) && (((_cursor6).ay2) > (by1)))));
        if (__callback(_name971)) {
            var _to_remove977 = _prev_cursor5;
            var _parent978 = (_to_remove977)._parent9;
            var _left979 = (_to_remove977)._left7;
            var _right980 = (_to_remove977)._right8;
            var _new_x981;
            if (((_left979) == null) && ((_right980) == null)) {
                _new_x981 = null;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if ((!((_left979) == null)) && ((_right980) == null)) {
                _new_x981 = _left979;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if (((_left979) == null) && (!((_right980) == null))) {
                _new_x981 = _right980;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else {
                var _root982 = (_to_remove977)._right8;
                var _x983 = _root982;
                var _descend984 = true;
                var _from_left985 = true;
                while (true) {
                    if ((_x983) == null) {
                        _x983 = null;
                        break;
                    }
                    if (_descend984) {
                        /* too small? */
                        if (false) {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        } else if ((!(((_x983)._left7) == null)) && (true)) {
                            _x983 = (_x983)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                            /* node ok? */
                        } else if (true) {
                            break;
                        } else if ((_x983) == (_root982)) {
                            _root982 = (_x983)._right8;
                            _x983 = (_x983)._right8;
                        } else {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        }
                    } else if (_from_left985) {
                        if (false) {
                            _x983 = null;
                            break;
                        } else if (true) {
                            break;
                        } else if ((!(((_x983)._right8) == null)) && (true)) {
                            _descend984 = true;
                            if ((_x983) == (_root982)) {
                                _root982 = (_x983)._right8;
                            }
                            _x983 = (_x983)._right8;
                        } else if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    } else {
                        if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    }
                }
                _new_x981 = _x983;
                var _mp986 = (_x983)._parent9;
                var _mr987 = (_x983)._right8;
                /* replace _x983 with _mr987 in _mp986 */
                if (!((_mp986) == null)) {
                    if (((_mp986)._left7) == (_x983)) {
                        (_mp986)._left7 = _mr987;
                    } else {
                        (_mp986)._right8 = _mr987;
                    }
                }
                if (!((_mr987) == null)) {
                    (_mr987)._parent9 = _mp986;
                }
                /* replace _to_remove977 with _x983 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _x983;
                    } else {
                        (_parent978)._right8 = _x983;
                    }
                }
                if (!((_x983) == null)) {
                    (_x983)._parent9 = _parent978;
                }
                /* replace null with _left979 in _x983 */
                (_x983)._left7 = _left979;
                if (!((_left979) == null)) {
                    (_left979)._parent9 = _x983;
                }
                /* replace _mr987 with (_to_remove977)._right8 in _x983 */
                (_x983)._right8 = (_to_remove977)._right8;
                if (!(((_to_remove977)._right8) == null)) {
                    ((_to_remove977)._right8)._parent9 = _x983;
                }
                /* _min_ax12 is min of ax1 */
                var _augval988 = (_x983).ax1;
                var _child989 = (_x983)._left7;
                if (!((_child989) == null)) {
                    var _val990 = (_child989)._min_ax12;
                    _augval988 = ((_augval988) < (_val990)) ? (_augval988) : (_val990);
                }
                var _child991 = (_x983)._right8;
                if (!((_child991) == null)) {
                    var _val992 = (_child991)._min_ax12;
                    _augval988 = ((_augval988) < (_val992)) ? (_augval988) : (_val992);
                }
                (_x983)._min_ax12 = _augval988;
                /* _min_ay13 is min of ay1 */
                var _augval993 = (_x983).ay1;
                var _child994 = (_x983)._left7;
                if (!((_child994) == null)) {
                    var _val995 = (_child994)._min_ay13;
                    _augval993 = ((_augval993) < (_val995)) ? (_augval993) : (_val995);
                }
                var _child996 = (_x983)._right8;
                if (!((_child996) == null)) {
                    var _val997 = (_child996)._min_ay13;
                    _augval993 = ((_augval993) < (_val997)) ? (_augval993) : (_val997);
                }
                (_x983)._min_ay13 = _augval993;
                /* _max_ay24 is max of ay2 */
                var _augval998 = (_x983).ay2;
                var _child999 = (_x983)._left7;
                if (!((_child999) == null)) {
                    var _val1000 = (_child999)._max_ay24;
                    _augval998 = ((_augval998) < (_val1000)) ? (_val1000) : (_augval998);
                }
                var _child1001 = (_x983)._right8;
                if (!((_child1001) == null)) {
                    var _val1002 = (_child1001)._max_ay24;
                    _augval998 = ((_augval998) < (_val1002)) ? (_val1002) : (_augval998);
                }
                (_x983)._max_ay24 = _augval998;
                (_x983)._height10 = 1 + ((((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) > ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10))) ? ((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) : ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10)));
                var _cursor1003 = _mp986;
                var _changed1004 = true;
                while ((_changed1004) && (!((_cursor1003) == (_parent978)))) {
                    var _old__min_ax121005 = (_cursor1003)._min_ax12;
                    var _old__min_ay131006 = (_cursor1003)._min_ay13;
                    var _old__max_ay241007 = (_cursor1003)._max_ay24;
                    var _old_height1008 = (_cursor1003)._height10;
                    /* _min_ax12 is min of ax1 */
                    var _augval1009 = (_cursor1003).ax1;
                    var _child1010 = (_cursor1003)._left7;
                    if (!((_child1010) == null)) {
                        var _val1011 = (_child1010)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1011)) ? (_augval1009) : (_val1011);
                    }
                    var _child1012 = (_cursor1003)._right8;
                    if (!((_child1012) == null)) {
                        var _val1013 = (_child1012)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1013)) ? (_augval1009) : (_val1013);
                    }
                    (_cursor1003)._min_ax12 = _augval1009;
                    /* _min_ay13 is min of ay1 */
                    var _augval1014 = (_cursor1003).ay1;
                    var _child1015 = (_cursor1003)._left7;
                    if (!((_child1015) == null)) {
                        var _val1016 = (_child1015)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1016)) ? (_augval1014) : (_val1016);
                    }
                    var _child1017 = (_cursor1003)._right8;
                    if (!((_child1017) == null)) {
                        var _val1018 = (_child1017)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1018)) ? (_augval1014) : (_val1018);
                    }
                    (_cursor1003)._min_ay13 = _augval1014;
                    /* _max_ay24 is max of ay2 */
                    var _augval1019 = (_cursor1003).ay2;
                    var _child1020 = (_cursor1003)._left7;
                    if (!((_child1020) == null)) {
                        var _val1021 = (_child1020)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1021)) ? (_val1021) : (_augval1019);
                    }
                    var _child1022 = (_cursor1003)._right8;
                    if (!((_child1022) == null)) {
                        var _val1023 = (_child1022)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1023)) ? (_val1023) : (_augval1019);
                    }
                    (_cursor1003)._max_ay24 = _augval1019;
                    (_cursor1003)._height10 = 1 + ((((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) > ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10))) ? ((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) : ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10)));
                    _changed1004 = false;
                    _changed1004 = (_changed1004) || (!((_old__min_ax121005) == ((_cursor1003)._min_ax12)));
                    _changed1004 = (_changed1004) || (!((_old__min_ay131006) == ((_cursor1003)._min_ay13)));
                    _changed1004 = (_changed1004) || (!((_old__max_ay241007) == ((_cursor1003)._max_ay24)));
                    _changed1004 = (_changed1004) || (!((_old_height1008) == ((_cursor1003)._height10)));
                    _cursor1003 = (_cursor1003)._parent9;
                }
            }
            var _cursor1024 = _parent978;
            var _changed1025 = true;
            while ((_changed1025) && (!((_cursor1024) == (null)))) {
                var _old__min_ax121026 = (_cursor1024)._min_ax12;
                var _old__min_ay131027 = (_cursor1024)._min_ay13;
                var _old__max_ay241028 = (_cursor1024)._max_ay24;
                var _old_height1029 = (_cursor1024)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval1030 = (_cursor1024).ax1;
                var _child1031 = (_cursor1024)._left7;
                if (!((_child1031) == null)) {
                    var _val1032 = (_child1031)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1032)) ? (_augval1030) : (_val1032);
                }
                var _child1033 = (_cursor1024)._right8;
                if (!((_child1033) == null)) {
                    var _val1034 = (_child1033)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1034)) ? (_augval1030) : (_val1034);
                }
                (_cursor1024)._min_ax12 = _augval1030;
                /* _min_ay13 is min of ay1 */
                var _augval1035 = (_cursor1024).ay1;
                var _child1036 = (_cursor1024)._left7;
                if (!((_child1036) == null)) {
                    var _val1037 = (_child1036)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1037)) ? (_augval1035) : (_val1037);
                }
                var _child1038 = (_cursor1024)._right8;
                if (!((_child1038) == null)) {
                    var _val1039 = (_child1038)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1039)) ? (_augval1035) : (_val1039);
                }
                (_cursor1024)._min_ay13 = _augval1035;
                /* _max_ay24 is max of ay2 */
                var _augval1040 = (_cursor1024).ay2;
                var _child1041 = (_cursor1024)._left7;
                if (!((_child1041) == null)) {
                    var _val1042 = (_child1041)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1042)) ? (_val1042) : (_augval1040);
                }
                var _child1043 = (_cursor1024)._right8;
                if (!((_child1043) == null)) {
                    var _val1044 = (_child1043)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1044)) ? (_val1044) : (_augval1040);
                }
                (_cursor1024)._max_ay24 = _augval1040;
                (_cursor1024)._height10 = 1 + ((((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) > ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10))) ? ((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) : ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10)));
                _changed1025 = false;
                _changed1025 = (_changed1025) || (!((_old__min_ax121026) == ((_cursor1024)._min_ax12)));
                _changed1025 = (_changed1025) || (!((_old__min_ay131027) == ((_cursor1024)._min_ay13)));
                _changed1025 = (_changed1025) || (!((_old__max_ay241028) == ((_cursor1024)._max_ay24)));
                _changed1025 = (_changed1025) || (!((_old_height1029) == ((_cursor1024)._height10)));
                _cursor1024 = (_cursor1024)._parent9;
            }
            if (((this)._root1) == (_to_remove977)) {
                (this)._root1 = _new_x981;
            }
            _prev_cursor5 = null;
        }
    };
}
; 
 
 buildViz = function (d3) {
    return function (widthInPixels = 800,
                     heightInPixels = 600,
                     max_snippets = null,
                     color = null,
                     sortByDist = true,
                     useFullDoc = false,
                     greyZeroScores = false,
                     asianMode = false,
                     nonTextFeaturesMode = false,
                     showCharacteristic = true,
                     wordVecMaxPValue = false,
                     saveSvgButton = false,
                     reverseSortScoresForNotCategory = false,
                     minPVal = 0.05,
                     pValueColors = false,
                     xLabelText = null,
                     yLabelText = null) {
        var divName = 'd3-div-1';

        // Set the dimensions of the canvas / graph
        var margin = {top: 30, right: 20, bottom: 30, left: 50},
            width = widthInPixels - margin.left - margin.right,
            height = heightInPixels - margin.top - margin.bottom;

        // Set the ranges
        var x = d3.scaleLinear().range([0, width]);
        var y = d3.scaleLinear().range([height, 0]);

        console.log('X Label');
        console.log(xLabelText);
        console.log('Y Label');
        console.log(yLabelText);
        console.log(yLabelText == null);
        console.log(yLabelText != null);
        console.log(yLabelText === undefined);
        console.log(yLabelText !== undefined);

        function axisLabelerFactory(axis) {
            if ((axis == "x" && xLabelText == null)
                || (axis == "y" && yLabelText == null))
                return function (d, i) {
                    return ["Infrequent", "Average", "Frequent"][i];
                };

            return function (d, i) {
                return ["Low", "Medium", "High"][i];
            }
        }

        var xAxis = d3.axisBottom(x).ticks(3).tickFormat(axisLabelerFactory('x'));
        var yAxis = d3.axisLeft(y).ticks(3).tickFormat(axisLabelerFactory('y'));

        // var label = d3.select("body").append("div")
        var label = d3.select('#' + divName).append("div")
            .attr("class", "label");

        var interpolateLightGreys = d3.interpolate(d3.rgb(230, 230, 230), d3.rgb(130, 130, 130));
        // setup fill color
        //var color = d3.interpolateRdYlBu;
        if (color == null) {
            color = d3.interpolateRdYlBu;
        }
        ;

        // Adds the svg canvas
        // var svg = d3.select("body")
        svg = d3.select('#' + divName)
            .append("svg")
            .attr("width", width + margin.left + margin.right + 200)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform",
                "translate(" + margin.left + "," + margin.top + ")");

        var lastCircleSelected = null;

        function deselectLastCircle() {
            if (lastCircleSelected) {
                lastCircleSelected.style["stroke"] = null;
                lastCircleSelected = null;
            }
        }

        function getSentenceBoundaries(text) {
            // !!! need to use spacy's sentence splitter
            if (asianMode) {
                var sentenceRe = /\n/gmi;
            } else {
                var sentenceRe = /\(?[^\.\?\!\n\b]+[\n\.!\?]\)?/g;
            }
            var offsets = [];
            var match;
            while ((match = sentenceRe.exec(text)) != null) {
                offsets.push(match.index);
            }
            offsets.push(text.length);
            return offsets;
        }

        function getMatchingSnippet(text, boundaries, start, end) {
            var sentenceStart = null;
            var sentenceEnd = null;
            for (var i in boundaries) {
                var position = boundaries[i];
                if (position <= start && (sentenceStart == null || position > sentenceStart)) {
                    sentenceStart = position;
                }
                if (position >= end) {
                    sentenceEnd = position;
                    break;
                }
            }
            var snippet = (text.slice(sentenceStart, start) + "<b>" + text.slice(start, end)
            + "</b>" + text.slice(end, sentenceEnd)).trim();
            if (sentenceStart == null) {
                sentenceStart = 0;
            }
            return {'snippet': snippet, 'sentenceStart': sentenceStart};
        }

        function gatherTermContexts(d) {
            var category_name = fullData['info']['category_name'];
            var not_category_name = fullData['info']['not_category_name'];
            var matches = [[], []];
            if (fullData.docs === undefined) return matches;
            if (!nonTextFeaturesMode) {
                return searchInText(d);
            } else {
                return searchInExtraFeatures(d);
            }
        }

        function searchInExtraFeatures(d) {
            var matches = [[], []];
            var term = d.term;
            for (var i in fullData.docs.extra) {

                if (term in fullData.docs.extra[i]) {
                    var strength = fullData.docs.extra[i][term] /
                        Object.values(fullData.docs.extra[i]).reduce(
                            function (a, b) {
                                return a + b
                            });
                    var text = fullData.docs.texts[i];
                    if (!useFullDoc)
                        text = text.slice(0, 300);
                    var curMatch = {'id': i, 'snippets': [text], 'strength': strength};

                    curMatch['meta'] = fullData.docs.meta[i];
                    matches[fullData.docs.labels[i]].push(curMatch);
                }
            }
            for (var i in [0, 1]) {
                matches[i] = matches[i].sort(function (a, b) {
                    return a.strength < b.strength ? 1 : -1
                })
            }
            return {'contexts': matches, 'info': d};
        }

        // from https://medium.com/reactnative/emojis-in-javascript-f693d0eb79fb
        var emojiRE = /(?:[\u2700-\u27bf]|(?:\ud83c[\udde6-\uddff]){2}|[\ud800-\udbff][\udc00-\udfff])[\ufe0e\ufe0f]?(?:[\u0300-\u036f\ufe20-\ufe23\u20d0-\u20f0]|\ud83c[\udffb-\udfff])?(?:\u200d(?:[^\ud800-\udfff]|(?:\ud83c[\udde6-\uddff]){2}|[\ud800-\udbff][\udc00-\udfff])[\ufe0e\ufe0f]?(?:[\u0300-\u036f\ufe20-\ufe23\u20d0-\u20f0]|\ud83c[\udffb-\udfff])?)*/g;

        function isEmoji(str) {
            console.log(str);
            if (str.match(emojiRE)) return true;
            return false;
        }

        function searchInText(d) {
            function stripNonWordChars(term) {
                //d.term.replace(" ", "[^\\w]+")
            }

            function buildMatcher(term) {

                var boundary = '\\b';
                var wordSep = "[^\\w]+";
                if (asianMode) {
                    boundary = '( |$|^)';
                    wordSep = ' ';
                }
                if (isEmoji(term)) {
                    boundary = '';
                    wordSep = '';
                }
                var regexp = new RegExp(boundary + '('
                    + term.replace('$', '\\$').replace(' ', wordSep, 'gim')
                    + ')' + boundary, 'gim');
                try {
                    regexp.exec('X');
                } catch (err) {
                    console.log("Can't search " + term);
                    console.log(err);
                    return null;
                }
                return regexp;
            }

            var matches = [[], []];
            var pattern = buildMatcher(d.term);
            if (pattern !== null) {
                for (var i in fullData.docs.texts) {
                    if (fullData.docs.labels[i] > 1) continue;
                    var text = fullData.docs.texts[i];
                    //var pattern = new RegExp("\\b(" + stripNonWordChars(d.term) + ")\\b", "gim");
                    var match;
                    var sentenceOffsets = null;
                    var lastSentenceStart = null;
                    var matchFound = false;
                    var curMatch = {'id': i, 'snippets': []};
                    if (fullData.docs.meta) {
                        curMatch['meta'] = fullData.docs.meta[i];
                    }
                    while ((match = pattern.exec(text)) != null) {
                        if (sentenceOffsets == null) {
                            sentenceOffsets = getSentenceBoundaries(text);
                        }
                        var foundSnippet = getMatchingSnippet(text, sentenceOffsets,
                            match.index, pattern.lastIndex);
                        if (foundSnippet.sentenceStart == lastSentenceStart) continue; // ensure we don't duplicate sentences
                        lastSentenceStart = foundSnippet.sentenceStart;
                        curMatch.snippets.push(foundSnippet.snippet);
                        matchFound = true;
                    }
                    if (matchFound) {
                        if (useFullDoc) {
                            curMatch.snippets = [
                                text
                                    .replace(/\n$/g, '\n\n')
                                    .replace(
                                        //new RegExp("\\b(" + d.term.replace(" ", "[^\\w]+") + ")\\b",
                                        //    'gim'),
                                        pattern,
                                        '<b>$&</b>')
                            ];
                        }
                        matches[fullData.docs.labels[i]].push(curMatch);

                    }
                }
            }
            return {'contexts': matches, 'info': d};
        }

        function displayTermContexts(termInfo, jump=true) {
            var contexts = termInfo.contexts;
            var info = termInfo.info;
            if (contexts[0].length == 0 && contexts[1].length == 0) {
                return null;
            }
            //var categoryNames = [fullData.info.category_name,
            //    fullData.info.not_category_name];
            var catInternalName = fullData.info.category_internal_name;
            fullData.docs.categories
                .map(
                    function (catName, catIndex) {
                        if (max_snippets != null) {
                            var contextsToDisplay = contexts[catIndex].slice(0, max_snippets);
                        }
                        var divId = catName == catInternalName ? '#cat' : '#notcat';
                        var temp = d3.select(divId)
                            .selectAll("div").remove();
                        contexts[catIndex].forEach(function (context) {
                            var meta = context.meta ? context.meta : '&nbsp;';
                            d3.select(divId)
                                .append("div")
                                .attr('class', 'snippet_meta')
                                .html(meta);
                            context.snippets.forEach(function (snippet) {
                                d3.select(divId)
                                    .append("div")
                                    .attr('class', 'snippet')
                                    .html(snippet);
                            })

                        });
                    });
            d3.select('#termstats')
                .selectAll("div")
                .remove();
            d3.select('#termstats')
                .append('div')
                .attr("class", "snippet_header")
                .html('Term: <b>' + info.term + '</b>');
            var message = '';
            var cat_name = fullData.info.category_name;
            var ncat_name = fullData.info.not_category_name;

            function getFrequencyDescription(name, count25k, count) {
                var desc = name + ' frequency: <div class=text_subhead>' + count25k
                    + ' per 25,000 terms</div>';
                if (count == 0) {
                    desc += '<u>Not found in any ' + name + ' documents.</u>';
                } else {
                    desc += '<u>Some of the ' + count + ' mentions:</u>';
                }
                return desc;
            }

            d3.select('#cathead')
                .style('fill', color(1))
                .html(getFrequencyDescription(cat_name, info.cat25k, info.cat));
            d3.select('#notcathead')
                .style('fill', color(0))
                .html(getFrequencyDescription(ncat_name, info.ncat25k, info.ncat));
            console.log(info);
            if (jump) {
                if (window.location.hash == '#snippets') {
                    window.location.hash = '#snippetsalt';
                } else {
                    window.location.hash = '#snippets';
                }
            }
        }

        function showTooltip(d, pageX, pageY) {
            deselectLastCircle();
            var message = d.term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            if (!sortByDist) {
                message += '<br/>score: ' + d.os.toFixed(5);
            }
            /*
             if (d.p) {
             message += ';  (p:' + d.p.toFixed(5) +')';
             }*/

            tooltip.transition()
                .duration(0)
                .style("opacity", 1)
                .style("z-index", 10000000);
            tooltip.html(message)
                .style("left", (pageX) + "px")
                .style("top", (pageY - 28) + "px");

            tooltip.on('click', function () {
                tooltip.transition()
                    .style('opacity', 0)
            });
        }

        handleSearch = function (event) {
            deselectLastCircle();
            var searchTerm = document
                .getElementById("searchTerm")
                .value
                .toLowerCase()
                .replace("'", " '")
                .trim();
            showToolTipForTerm(searchTerm);
            var termInfo = termDict[searchTerm];
            if (termInfo != null) {
                displayTermContexts(gatherTermContexts(termInfo), false);
            }
            return false;
        };

        function showToolTipForTerm(searchTerm) {
            var searchTermInfo = termDict[searchTerm];
            if (searchTermInfo === undefined) {
                d3.select("#alertMessage")
                    .text(searchTerm + " didn't make it into the visualization.");
            } else {
                d3.select("#alertMessage").text("");
                var circle = mysvg._groups[0][searchTermInfo.i];
                var mySVGMatrix = circle.getScreenCTM()
                    .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                var pageX = mySVGMatrix.e;
                var pageY = mySVGMatrix.f;
                circle.style["stroke"] = "black";
                showTooltip(searchTermInfo, pageX, pageY);
                lastCircleSelected = circle;

            }
        };

        function makeWordInteractive(domObj, term) {
            return domObj
                .on("mouseover", function (d) {
                    showToolTipForTerm(term);
                    d3.select(this).style("stroke", "black");
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                })
                .on("click", function (d) {
                    displayTermContexts(gatherTermContexts(termDict[term]));
                });
        }

        function processData(fullData) {
            var modelInfo = fullData['info'];
            /*
             categoryTermList.data(modelInfo['category_terms'])
             .enter()
             .append("li")
             .text(function(d) {return d;});
             */
            data = fullData['data'];
            termDict = Object();
            data.forEach(function (x, i) {
                termDict[x.term] = x;
                termDict[x.term].i = i;
            });

            console.log(data);
            // Scale the range of the data.  Add some space on either end.
            x.domain([-0.1, d3.max(data, function (d) {
                return d.x;
            }) + 0.1]);
            y.domain([-0.1, d3.max(data, function (d) {
                return d.y;
            }) + 0.1]);

            /*
             data.sort(function (a, b) {
             return Math.abs(b.os) - Math.abs(a.os)
             });
             */


            //var rangeTree = null; // keep boxes of all points and labels here
            var rectHolder = new RectangleHolder();
            // Add the scatterplot
            mysvg = svg
                .selectAll("dot")
                .data(data)
                .enter()
                .append("circle")
                .attr("r", function (d) {
                    if (pValueColors && d.p) {
                        return (d.p >= 1 - minPVal || d.p <= minPVal) ? 2 : 1.75;
                    }
                    return 2;
                })
                .attr("cx", function (d) {
                    return x(d.x);
                })
                .attr("cy", function (d) {
                    return y(d.y);
                })
                .style("fill", function (d) {
                    //.attr("fill", function (d) {
                    if (greyZeroScores && d.os == 0) {
                        return d3.rgb(230, 230, 230);
                    } else if (pValueColors && d.p) {
                        if (d.p >= 1 - minPVal) {
                            return d3.interpolateYlGnBu(d.s);
                        } else if (d.p <= minPVal) {
                            return d3.interpolateYlOrBr(d.s);
                        } else {
                            return interpolateLightGreys(d.s);
                        }
                    } else {
                        return color(d.s);
                    }
                })
                .on("mouseover", function (d) {
                    showTooltip(d, d3.event.pageX, d3.event.pageY);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    displayTermContexts(gatherTermContexts(d));
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                });

            coords = Object();

            function censorPoints(datum) {
                var term = datum.term;
                var curLabel = svg.append("text")
                    .attr("x", x(datum.x))
                    .attr("y", y(datum.y) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, '~~' + term);
                rectHolder.add(new Rectangle(x1, y1, x2, y2));

                curLabel.remove();
            }

            function labelPointsIfPossible(i, useOffset) {
                var term = data[i].term;

                var configs = [
                    {'anchor': 'end', 'xoff': -5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'end', 'xoff': -5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 3, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 3, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                ];
                if(useOffset) {

                }
                var matchedElement = null;
                for (var configI in configs) {
                    var config = configs[configI];
                    var curLabel = makeWordInteractive(
                        svg.append("text")
                            .attr("x", x(data[i].x) + config['xoff'])
                            .attr("y", y(data[i].y) + config['yoff'])
                            .attr('class', 'label')
                            .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                            .attr('font-size', '10px')
                            .attr("text-anchor", config['anchor'])
                            .attr("alignment-baseline", config['alignment'])
                            .text(term),
                        term
                    );
                    var bbox = curLabel.node().getBBox();
                    var borderToRemove = .5;
                    var x1 = bbox.x + borderToRemove,
                        y1 = bbox.y + borderToRemove,
                        x2 = bbox.x + bbox.width - borderToRemove,
                        y2 = bbox.y + bbox.height - borderToRemove;
                    //matchedElement = searchRangeTree(rangeTree, x1, y1, x2, y2);
                    var matchedElement = false;
                    rectHolder.findMatchingRectangles(x1, y1, x2, y2, function (elem) {
                        matchedElement = true;
                        return false;
                    });
                    if (matchedElement) {
                        curLabel.remove();
                    } else {
                        break;
                    }
                }

                if (!matchedElement) {
                    coords[term] = [x1, y1, x2, y2];
                    //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, term);
                    rectHolder.add(new Rectangle(x1, y1, x2, y2));
                    return true;

                } else {
                    //curLabel.remove();
                    return false;
                }

            }

            var radius = 2;

            function euclideanDistanceSort(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (Math.min(aCatDist, aNotCatDist) > Math.min(bCatDist, bNotCatDist)) * 2 - 1;
            }

            function euclideanDistanceSortForCategory(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                return (aCatDist > bCatDist) * 2 - 1;
            }

            function euclideanDistanceSortForNotCategory(a, b) {
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (aNotCatDist > bNotCatDist) * 2 - 1;
            }

            function scoreSort(a, b) {
                return a.s - b.s;
            }

            function scoreSortReverse(a, b) {
                return b.s - a.s;
            }

            function backgroundScoreSort(a, b) {
                return b.bg - a.bg;
            }

            function arePointsPredictiveOfDifferentCategories(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                var aGood = aCatDist < aNotCatDist;
                var bGood = bCatDist < bNotCatDist;
                return {aGood: aGood, bGood: bGood};
            }

            function scoreSortForCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                var aGood = __ret.aGood;
                var bGood = __ret.bGood;
                if (aGood && !bGood) return -1;
                if (!aGood && bGood) return 1;
                return b.s - a.s;
            }

            function scoreSortForNotCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                var aGood = __ret.aGood;
                var bGood = __ret.bGood;
                if (aGood && !bGood) return 1;
                if (!aGood && bGood) return -1;
                if (reverseSortScoresForNotCategory)
                    return a.s - b.s;
                else
                    return b.s - a.s;
            }

            data = data.sort(sortByDist ? euclideanDistanceSort : scoreSort);
            console.log("Sorted Data:");
            console.log(data);
            data.forEach(censorPoints);

            var myXAxis = svg.append("g")
                .attr("class", "x axis")
                .attr("transform", "translate(0," + height + ")")
                .call(xAxis);

            function registerFigureBBox(curLabel) {
                var bbox = curLabel.node().getBBox();
                var borderToRemove = 1.5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                rectHolder.add(new Rectangle(x1, y1, x2, y2));
                //return insertRangeTree(rangeTree, x1, y1, x2, y2, '~~_other_');
            }

            //rangeTree = registerFigureBBox(myXAxis);
            var xLabel = svg.append("text")
                .attr("class", "x label")
                .attr("text-anchor", "end")
                .attr("x", width)
                .attr("y", height - 6)
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '10px')
                .text(getLabelText('x'));

            //console.log('xLabel');
            //console.log(xLabel);

            //rangeTree = registerFigureBBox(xLabel);
            // Add the Y Axis
            var myYAxis = svg.append("g")
                .attr("class", "y axis")
                .call(yAxis)
                .selectAll("text")
                .style("text-anchor", "end")
                .attr("dx", "30px")
                .attr("dy", "-13px")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '10px')
                .attr("transform", "rotate(-90)");
            registerFigureBBox(myYAxis);

            function getLabelText(axis) {
                if (axis == 'y') {
                    if (yLabelText == null)
                        return modelInfo['category_name'] + " Frequency";
                    else
                        return yLabelText;
                } else {
                    if (xLabelText == null)
                        return modelInfo['not_category_name'] + " Frequency";
                    else
                        return xLabelText;
                }
            }

            var yLabel = svg.append("text")
                .attr("class", "y label")
                .attr("text-anchor", "end")
                .attr("y", 6)
                .attr("dy", ".75em")
                .attr("transform", "rotate(-90)")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '10px')
                .text(getLabelText('y'));
            registerFigureBBox(yLabel);

            var catHeader = svg.append("text")
                .attr("text-anchor", "start")
                .attr("x", width)
                .attr("dy", "6px")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '12px')
                .attr('font-weight', 'bolder')
                .attr('font-decoration', 'underline')
                .text("Top " + fullData['info']['category_name']);
            registerFigureBBox(catHeader);
            console.log(catHeader);

            function showWordList(word, termDataList) {
                var maxWidth = word.node().getBBox().width;
                for (var i in termDataList) {
                    var curTerm = termDataList[i].term;
                    word = (function (word, curTerm) {
                        return makeWordInteractive(
                            svg.append("text")
                                .attr("text-anchor", "start")
                                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                                .attr('font-size', '12px')
                                .attr("x", word.node().getBBox().x)
                                .attr("y", word.node().getBBox().y
                                    + 2 * word.node().getBBox().height)
                                .text(curTerm)
                            ,
                            curTerm);
                    })(word, curTerm);
                    if (word.node().getBBox().width > maxWidth)
                        maxWidth = word.node().getBBox().width;
                    registerFigureBBox(word);
                }
                return {
                    'word': word,
                    'maxWidth': maxWidth
                };
            }

            function pickEuclideanDistanceSortAlgo(category) {
                if (category == true) return euclideanDistanceSortForCategory;
                return euclideanDistanceSortForNotCategory;
            }

            function pickScoreSortAlgo(category) {
                console.log("PICK SCORE ALGO")
                console.log(category)
                if (category == true) {
                    return scoreSortForCategory;
                } else {
                    return scoreSortForNotCategory;
                }
            }

            function pickTermSortingAlgorithm(category) {
                if (sortByDist) return pickEuclideanDistanceSortAlgo(category);
                return pickScoreSortAlgo(category);
            }

            function showAssociatedWordList(header, isAssociatedToCategory, length=14) {
                var sortedData = null;
                var sortingAlgo = pickTermSortingAlgorithm(isAssociatedToCategory);
                sortedData = data.sort(sortingAlgo);
                if (wordVecMaxPValue) {
                    function signifTest(x) {
                        if (isAssociatedToCategory)
                            return x.p >= 1 - minPVal;
                        return x.p <= minPVal;
                    }

                    sortedData = sortedData.filter(signifTest)
                }
                return showWordList(header, sortedData.slice(0, length));

            }

            var wordListData = showAssociatedWordList(catHeader, true);
            var word = wordListData.word;
            var maxWidth = wordListData.maxWidth;

            catHeader = svg.append("text")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '12px')
                .attr('font-weight', 'bolder')
                .attr('font-decoration', 'underline')
                .attr("text-anchor", "start")
                .attr("x", width)
                .attr("y", word.node().getBBox().y + 4 * word.node().getBBox().height)
                .text("Top " + fullData['info']['not_category_name']);


            wordListData = showAssociatedWordList(catHeader, false);
            word = wordListData.word;
            if (wordListData.maxWidth > maxWidth) {
                maxWidth = wordListData.maxWidth;
            }


            if (!nonTextFeaturesMode && !asianMode && showCharacteristic) {
                var title = 'Characteristic';
                if (wordVecMaxPValue) {
                    title = 'Most similar';
                }
                word = svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr("text-anchor", "start")
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("x", catHeader.node().getBBox().x + maxWidth + 10)
                    .attr("dy", "6px")
                    .text(title);
                var sortMethod = backgroundScoreSort;
                if (wordVecMaxPValue) {
                    sortMethod = scoreSortReverse;
                }
                var wordListData = showWordList(word, data.sort(sortMethod).slice(0, 30));
                ;

                word = wordListData.word;
                maxWidth = wordListData.maxWidth;
                console.log(maxWidth);
                console.log(word.node().getBBox().x + maxWidth);

                svg.attr('width', word.node().getBBox().x + 3 * maxWidth + 10);
            }

            var numPointsLabeled = 0;
            for (var i = 0; i < data.length; i++) {
                if (labelPointsIfPossible(i)) numPointsLabeled++;
            }
            console.log('numPointsLabeled');
            console.log(numPointsLabeled);


            function populateCorpusStats() {
                var wordCounts = {};
                var docCounts = {}
                fullData.docs.labels.forEach(function (x, i) {
                    var cnt = (
                        fullData.docs.texts[i]
                            .trim()
                            .replace(/['";:,.?\-!]+/g, '')
                            .match(/\S+/g) || []
                    ).length;
                    wordCounts[x] = wordCounts[x] ? wordCounts[x] + cnt : cnt
                });
                fullData.docs.labels.forEach(function (x) {
                    docCounts[x] = docCounts[x] ? docCounts[x] + 1 : 1
                });
                var messages = [];
                fullData.docs.categories.forEach(function (x, i) {
                    var name = fullData.info.not_category_name;
                    if (x == fullData.info.category_internal_name) {
                        name = fullData.info.category_name;
                    }

                    messages.push('<b>' + name + '</b> document count: '
                        + Number(docCounts[i]).toLocaleString('en')
                        + '; word count: '
                        + Number(wordCounts[i]).toLocaleString('en'));
                });

                d3.select('#corpus-stats')
                    .style('width', width + margin.left + margin.right + 200)
                    .append('div')
                    .html(messages.join('<br />'));
            };

            if (fullData.docs) {
                populateCorpusStats();
            }

            if (saveSvgButton) {
                // from https://stackoverflow.com/questions/23218174/how-do-i-save-export-an-svg-file-after-creating-an-svg-with-d3-js-ie-safari-an
                var svgElement = document.getElementById("d3-div-1");

                var serializer = new XMLSerializer();
                var source = serializer.serializeToString(svgElement);

                if (!source.match(/^<svg[^>]+xmlns="http\:\/\/www\.w3\.org\/2000\/svg"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns="https://www.w3.org/2000/svg"');
                }
                if (!source.match(/^<svg[^>]+"http\:\/\/www\.w3\.org\/1999\/xlink"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns:xlink="https://www.w3.org/1999/xlink"');
                }

                source = '<?xml version="1.0" standalone="no"?>\r\n' + source;

                var url = "data:image/svg+xml;charset=utf-8," + encodeURIComponent(source);

                var downloadLink = document.createElement("a");
                downloadLink.href = url;
                downloadLink.download = fullData['info']['category_name'] + ".svg";
                downloadLink.innerText = 'Download SVG';
                document.body.appendChild(downloadLink);

            }

        };

        fullData = getDataAndInfo();
        processData(fullData);

        // The tool tip is down here in order to make sure it has the highest z-index
        var tooltip = d3.select('#' + divName)
            .append("div")
            .attr("class", sortByDist ? "tooltip" : "tooltipscore")
            .style("opacity", 0);
    };
}(d3);

function getDataAndInfo() { return{"info": {"not_category_terms": ["model", "semantic", "neural", "features", "semeval-2017", "sentiment", "information", "embeddings", "performance", "analysis"], "not_category_name": "All Others", "category_name": "Bucc", "category_internal_name": "BUCC", "category_terms": ["model", "semantic", "neural", "features", "semeval-2017", "sentiment", "information", "embeddings", "performance", "analysis"]}, "data": [{"ncat": 0, "cat": 8, "cat25k": 178, "bg": 0.00016406386185822833, "x": 0.0, "s": 1.0, "ncat25k": 0, "os": 1.0, "y": 0.988556338028169, "term": "bucc"}, {"ncat": 2, "cat": 3, "cat25k": 67, "bg": 6.197891229488079e-06, "x": 0.00044014084507042255, "s": 0.9995598591549296, "ncat25k": 1, "os": 0.9999998736226889, "y": 0.9713908450704225, "term": "plagiarism"}, {"ncat": 3, "cat": 3, "cat25k": 67, "bg": 4.890789286579344e-07, "x": 0.0008802816901408451, "s": 0.9991197183098592, "ncat25k": 1, "os": 0.9999993748072593, "y": 0.9700704225352113, "term": "collections"}, {"ncat": 3, "cat": 2, "cat25k": 44, "bg": 4.30203744493392e-06, "x": 0.0013204225352112676, "s": 0.996919014084507, "ncat25k": 1, "os": 0.9996843562940559, "y": 0.9581866197183099, "term": "bridging"}, {"ncat": 3, "cat": 2, "cat25k": 44, "bg": 1.2975941307222282e-05, "x": 0.0017605633802816902, "s": 0.996919014084507, "ncat25k": 1, "os": 0.9996843562940559, "y": 0.9599471830985915, "term": "theoretic"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 8.735018395249939e-08, "x": 0.0022007042253521128, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.8904049295774648, "term": "try"}, {"ncat": 4, "cat": 2, "cat25k": 44, "bg": 5.452723501342802e-07, "x": 0.002640845070422535, "s": 0.9955985915492959, "ncat25k": 1, "os": 0.9991890045486203, "y": 0.9533450704225352, "term": "pilot"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 2.0819009421122235e-08, "x": 0.0030809859154929575, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9009683098591549, "term": "had"}, {"ncat": 4, "cat": 2, "cat25k": 44, "bg": 9.21444902394029e-07, "x": 0.0035211267605633804, "s": 0.9955985915492959, "ncat25k": 1, "os": 0.9991890045486203, "y": 0.9590669014084507, "term": "dimension"}, {"ncat": 4, "cat": 2, "cat25k": 44, "bg": 0.00012305041990955793, "x": 0.003961267605633803, "s": 0.9955985915492959, "ncat25k": 1, "os": 0.9991890045486203, "y": 0.9595070422535211, "term": "autoencoders"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 5.555487655150881e-07, "x": 0.0044014084507042256, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9212147887323944, "term": "organized"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 2.634078688043374e-07, "x": 0.0048415492957746475, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9220950704225352, "term": "labor"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 8.11116185093469e-06, "x": 0.00528169014084507, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9238556338028169, "term": "smt"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 1.0139330831338537e-07, "x": 0.005721830985915493, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9251760563380281, "term": "material"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 1.4673344212150731e-08, "x": 0.006161971830985915, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9264964788732394, "term": "see"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 2.3195742375095566e-06, "x": 0.006602112676056338, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9352992957746479, "term": "feasible"}, {"ncat": 4, "cat": 1, "cat25k": 22, "bg": 1.9485273127996224e-07, "x": 0.007042253521126761, "s": 0.9876760563380282, "ncat25k": 1, "os": 0.9405501183485407, "y": 0.9361795774647887, "term": "purposes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.386886735717141e-07, "x": 0.007482394366197183, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.0013204225352112676, "term": "classified"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.3105567032152123e-06, "x": 0.007922535211267605, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.02112676056338028, "term": "annotate"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.3375322930453002e-06, "x": 0.008362676056338027, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.022887323943661973, "term": "notion"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.7033424689267747e-06, "x": 0.008802816901408451, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.025528169014084508, "term": "suggesting"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2957233730110176e-07, "x": 0.009242957746478873, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.032570422535211266, "term": "exchange"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.113755962350709e-07, "x": 0.009683098591549295, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.03301056338028169, "term": "led"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.4860085573883764e-07, "x": 0.010123239436619719, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.04929577464788732, "term": "websites"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.6251541588965383e-07, "x": 0.01056338028169014, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.05457746478873239, "term": "reporting"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.110785595137898e-07, "x": 0.011003521126760563, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.06426056338028169, "term": "goes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.855471182727598e-06, "x": 0.011443661971830986, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.07042253521126761, "term": "emphasize"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.622940491568357e-07, "x": 0.011883802816901408, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.07086267605633803, "term": "consideration"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.4441794923390654e-08, "x": 0.01232394366197183, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.07878521126760564, "term": "down"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.012764084507042254, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.0818661971830986, "term": "softmax"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.802797149059612e-07, "x": 0.013204225352112676, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.0880281690140845, "term": "implement"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.901443939227395e-08, "x": 0.013644366197183098, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.08890845070422536, "term": "subject"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 3.1885446220212844e-07, "x": 0.014084507042253521, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.8529929577464789, "term": "expert"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.5415175960700005e-06, "x": 0.014524647887323943, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.09242957746478873, "term": "noun"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.5265989952810842e-07, "x": 0.014964788732394365, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.09375, "term": "female"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.544061657917692e-07, "x": 0.015404929577464789, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.10387323943661972, "term": "platforms"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.633212519702161e-07, "x": 0.01584507042253521, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.12544014084507044, "term": "thousand"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.962332320058696e-07, "x": 0.016285211267605633, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.13776408450704225, "term": "outcomes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.0938354448442392e-07, "x": 0.016725352112676055, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.14524647887323944, "term": "funding"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.01716549295774648, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.15272887323943662, "term": "spelling correction"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.217583242980915e-07, "x": 0.017605633802816902, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.1544894366197183, "term": "replacement"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.018045774647887324, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.15713028169014084, "term": "noisy channel"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 8.501961955250604e-07, "x": 0.018485915492957746, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.1602112676056338, "term": "greatly"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.681383621342858e-07, "x": 0.018926056338028168, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.16285211267605634, "term": "extra"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.5817827514032447e-08, "x": 0.01936619718309859, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.16329225352112675, "term": "add"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.8991411703799307e-07, "x": 0.019806338028169015, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.16681338028169015, "term": "modified"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.699772719633598e-07, "x": 0.020246478873239437, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.16725352112676056, "term": "claims"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.284881090049776e-06, "x": 0.02068661971830986, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.17077464788732394, "term": "adapting"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.5345849643605687e-07, "x": 0.02112676056338028, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.17473591549295775, "term": "reason"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.132350226866368e-06, "x": 0.021566901408450703, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.176056338028169, "term": "suffer"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.96137422448672e-07, "x": 0.022007042253521125, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.18397887323943662, "term": "discussed"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.222094624610889e-07, "x": 0.02244718309859155, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.1914612676056338, "term": "described"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.0195807734837328e-06, "x": 0.022887323943661973, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.19234154929577466, "term": "modifying"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.461711219665657e-06, "x": 0.023327464788732395, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.20466549295774647, "term": "snippets"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.908094895578635e-07, "x": 0.023767605633802816, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.20598591549295775, "term": "concerned"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.8435184656027108e-05, "x": 0.02420774647887324, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2108274647887324, "term": "convolution"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.217213369863689e-08, "x": 0.02464788732394366, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2125880281690141, "term": "plan"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.086951204130143e-07, "x": 0.025088028169014086, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2130281690140845, "term": "pubmed"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 2.8433301623093696e-07, "x": 0.025528169014084508, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.878080985915493, "term": "factors"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.4457301804849556e-06, "x": 0.02596830985915493, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.22227112676056338, "term": "timeline"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.998912147947879e-06, "x": 0.02640845070422535, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.22535211267605634, "term": "timelines"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.204755410556549e-05, "x": 0.026848591549295774, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2266725352112676, "term": "ontologies"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.646737823092221e-06, "x": 0.027288732394366196, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2323943661971831, "term": "tackling"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.02772887323943662, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2363556338028169, "term": "5b"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.028169014084507043, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2381161971830986, "term": "blog posts"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2700638334082672e-05, "x": 0.028609154929577465, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2649647887323944, "term": "leveraged"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.6050431761940636e-07, "x": 0.029049295774647887, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2671654929577465, "term": "databases"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 9.882546918132685e-08, "x": 0.02948943661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.27684859154929575, "term": "costs"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0016654691755983e-06, "x": 0.02992957746478873, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.27860915492957744, "term": "macro"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.098483185262162e-06, "x": 0.030369718309859156, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.27992957746478875, "term": "enrich"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.0881954654790703e-06, "x": 0.030809859154929578, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.28389084507042256, "term": "interpreting"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.5268399375522465e-06, "x": 0.03125, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.2852112676056338, "term": "combinations"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.2839813540568363e-07, "x": 0.03169014084507042, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.289612676056338, "term": "biology"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.548455988080605e-06, "x": 0.032130281690140844, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.29005281690140844, "term": "complement"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.504922956397745e-08, "x": 0.032570422535211266, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.29401408450704225, "term": "product"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.03301056338028169, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.29797535211267606, "term": "2014"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.212650786619481e-06, "x": 0.03345070422535211, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3023767605633803, "term": "prevalence"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.6258382513617755e-06, "x": 0.03389084507042254, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.30985915492957744, "term": "stacking"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.3719306823791142e-06, "x": 0.03433098591549296, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.31117957746478875, "term": "strengths"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.138722380009548e-06, "x": 0.03477112676056338, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3142605633802817, "term": "naive"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.613803855202225e-06, "x": 0.035211267605633804, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.31778169014084506, "term": "labelled"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.418284274177515e-06, "x": 0.035651408450704226, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3199823943661972, "term": "facilitates"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.9350546804058596e-06, "x": 0.03609154929577465, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3204225352112676, "term": "labelling"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 1.345691098783808e-07, "x": 0.03653169014084507, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.8956866197183099, "term": "currently"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.212561550953566e-06, "x": 0.03697183098591549, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.32702464788732394, "term": "quantify"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0254790525393939e-05, "x": 0.037411971830985914, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.33098591549295775, "term": "batches"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.037852112676056336, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3415492957746479, "term": "doctor patient"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.963047938945911e-07, "x": 0.03829225352112676, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.34463028169014087, "term": "virtual"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.503553167833448e-07, "x": 0.03873239436619718, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3455105633802817, "term": "doctor"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.7548414271951885e-06, "x": 0.03917253521126761, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3472711267605634, "term": "retrieving"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.3940676765010116e-08, "x": 0.03961267605633803, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.34991197183098594, "term": "last"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.2842471464042522e-05, "x": 0.04005281690140845, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3534330985915493, "term": "dimensionality"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 5.031681141695161e-07, "x": 0.040492957746478875, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.8996478873239436, "term": "sizes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.572032034187185e-07, "x": 0.0409330985915493, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3600352112676056, "term": "observations"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.363223420949788e-06, "x": 0.04137323943661972, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3609154929577465, "term": "matrices"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.04181338028169014, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.37940140845070425, "term": "protein protein"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.994220971561317e-07, "x": 0.04225352112676056, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.3855633802816901, "term": "applies"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.5642242639589654e-07, "x": 0.042693661971830985, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.39172535211267606, "term": "medicine"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.1801938977244877e-06, "x": 0.043133802816901406, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.40096830985915494, "term": "utilized"}, {"ncat": 5, "cat": 2, "cat25k": 44, "bg": 4.647752945903509e-07, "x": 0.04357394366197183, "s": 0.994718309859155, "ncat25k": 1, "os": 0.9972466298640813, "y": 0.9559859154929577, "term": "toward"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 9.988940844033935e-08, "x": 0.04401408450704225, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4022887323943662, "term": "early"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 4.509565916700801e-05, "x": 0.04445422535211268, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9080105633802817, "term": "prosody"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 4.843253547592412e-07, "x": 0.0448943661971831, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9119718309859155, "term": "occur"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 1.4678847369932202e-07, "x": 0.04533450704225352, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9198943661971831, "term": "near"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 3.9429594401234173e-07, "x": 0.045774647887323945, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9286971830985915, "term": "estimates"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 1.1124493464730906e-06, "x": 0.04621478873239437, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9304577464788732, "term": "examined"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 6.734500295027234e-07, "x": 0.04665492957746479, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9308978873239436, "term": "measured"}, {"ncat": 5, "cat": 2, "cat25k": 44, "bg": 5.714898491360685e-07, "x": 0.04709507042253521, "s": 0.994718309859155, "ncat25k": 1, "os": 0.9972466298640813, "y": 0.9621478873239436, "term": "investigation"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 5.087087116112509e-07, "x": 0.04753521126760563, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9326584507042254, "term": "styles"}, {"ncat": 5, "cat": 1, "cat25k": 22, "bg": 7.049068447981929e-07, "x": 0.047975352112676055, "s": 0.9815140845070423, "ncat25k": 1, "os": 0.9174577440056528, "y": 0.9339788732394366, "term": "possibility"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.649042935236484e-08, "x": 0.04841549295774648, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.40625, "term": "history"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.318289565958236e-06, "x": 0.0488556338028169, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4119718309859155, "term": "attracted"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.0998551641133396e-05, "x": 0.04929577464788732, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.42077464788732394, "term": "imbalanced"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.548599669341483e-06, "x": 0.04973591549295775, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.42649647887323944, "term": "optimizing"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.05017605633802817, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4335387323943662, "term": "39"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.418080951888541e-07, "x": 0.050616197183098594, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.43617957746478875, "term": "kind"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2895355479816833e-05, "x": 0.051056338028169015, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4375, "term": "agnostic"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.01511280064632e-07, "x": 0.05149647887323944, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4392605633802817, "term": "appears"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.4671052834465895e-07, "x": 0.05193661971830986, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.44938380281690143, "term": "generally"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.929790674198032e-07, "x": 0.05237676056338028, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4498239436619718, "term": "exist"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.3490858351545542e-07, "x": 0.0528169014084507, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.45686619718309857, "term": "inside"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.053257042253521125, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4722711267605634, "term": "coreference resolvers"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.7244588955229e-06, "x": 0.05369718309859155, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4762323943661972, "term": "partitioning"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.88679703631726e-07, "x": 0.05413732394366197, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.48151408450704225, "term": "objectives"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.794765029362356e-05, "x": 0.05457746478873239, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4828345070422535, "term": "computationally"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.1785509879953993e-07, "x": 0.05501760563380282, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4845950704225352, "term": "elementary"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.05545774647887324, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4907570422535211, "term": "autoencoder"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.706048168965894e-06, "x": 0.055897887323943664, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.491637323943662, "term": "generalization"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.033069471556047e-08, "x": 0.056338028169014086, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.49383802816901406, "term": "offers"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.4410654661630623e-05, "x": 0.05677816901408451, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.4964788732394366, "term": "siamese"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 8.84467181933661e-07, "x": 0.05721830985915493, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.49691901408450706, "term": "nodes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.002119051288491e-07, "x": 0.05765845070422535, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.49955985915492956, "term": "essays"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.058098591549295774, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5035211267605634, "term": "loss functions"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.058538732394366196, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5066021126760564, "term": "listwise"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.2662852081048293e-06, "x": 0.05897887323943662, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5101232394366197, "term": "employs"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.05941901408450704, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5127640845070423, "term": "stocktwits"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.05985915492957746, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5132042253521126, "term": "stock market"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.06029929577464789, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5202464788732394, "term": "2010"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.11909613584537e-06, "x": 0.06073943661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5220070422535211, "term": "pid"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.301731707850401e-06, "x": 0.061179577464788734, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5233274647887324, "term": "sid"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.3195351323000724e-05, "x": 0.061619718309859156, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5330105633802817, "term": "phonemes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.698506559233183e-07, "x": 0.06205985915492958, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5404929577464789, "term": "quite"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.0625, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5418133802816901, "term": "n't"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.386491293615744e-06, "x": 0.06294014084507042, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5422535211267606, "term": "inferred"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.814215464488687e-06, "x": 0.06338028169014084, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5497359154929577, "term": "diagnoses"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.698103097202873e-06, "x": 0.06382042253521127, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5506161971830986, "term": "ccg"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.6482010538193565e-06, "x": 0.06426056338028169, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5523767605633803, "term": "proposing"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.06470070422535211, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.554137323943662, "term": "zero shot"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.171457851180806e-06, "x": 0.06514084507042253, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5602992957746479, "term": "aggregate"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.290258044626739e-07, "x": 0.06558098591549295, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5625, "term": "surrounding"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.785408248930821e-06, "x": 0.06602112676056338, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.565580985915493, "term": "encoded"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.8694069680275328e-05, "x": 0.0664612676056338, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5673415492957746, "term": "justifications"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 8.041500576173516e-06, "x": 0.06690140845070422, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5677816901408451, "term": "attentive"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.4567034945588483e-06, "x": 0.06734154929577464, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5713028169014085, "term": "adapted"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.887955904107436e-08, "x": 0.06778169014084508, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5726232394366197, "term": "v"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.8407769167823626e-06, "x": 0.0682218309859155, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5787852112676056, "term": "reliably"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.865690928533008e-06, "x": 0.06866197183098592, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.582306338028169, "term": "uncover"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2433859792489572e-07, "x": 0.06910211267605634, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5875880281690141, "term": "turn"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.867064050952832e-08, "x": 0.06954225352112677, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.59375, "term": "audio"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.574818165053775e-07, "x": 0.06998239436619719, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5959507042253521, "term": "factor"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2433542714192642e-05, "x": 0.07042253521126761, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5963908450704225, "term": "gpu"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.836940847005594e-07, "x": 0.07086267605633803, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5985915492957746, "term": "poems"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 9.240271784113986e-09, "x": 0.07130281690140845, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5994718309859155, "term": "page"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.343164756485266e-05, "x": 0.07174295774647887, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.5999119718309859, "term": "chatbot"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.763035030014837e-08, "x": 0.0721830985915493, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6012323943661971, "term": "advanced"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.07262323943661972, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.602112676056338, "term": "200"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.6981515455167013e-07, "x": 0.07306338028169014, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6078345070422535, "term": "indeed"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.855252210153762e-07, "x": 0.07350352112676056, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6091549295774648, "term": "stages"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 8.084512518770622e-08, "x": 0.07394366197183098, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6113556338028169, "term": "needs"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 9.719081514529526e-08, "x": 0.0743838028169014, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6131161971830986, "term": "technical"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.545641686129387e-07, "x": 0.07482394366197183, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6148767605633803, "term": "voice"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.8667021588970478e-06, "x": 0.07526408450704225, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6153169014084507, "term": "verbal"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.1661632552733215e-05, "x": 0.07570422535211267, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6161971830985915, "term": "annotating"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.0761443661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.616637323943662, "term": "robotreviewer"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.670270415383495e-06, "x": 0.07658450704225352, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6201584507042254, "term": "modalities"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0862999006796003e-06, "x": 0.07702464788732394, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6232394366197183, "term": "enabling"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.503148331953152e-07, "x": 0.07746478873239436, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6236795774647887, "term": "connected"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0929447780579443e-07, "x": 0.07790492957746478, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.625, "term": "thanks"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.145357031839322e-06, "x": 0.07834507042253522, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6267605633802817, "term": "anchor"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.335602186185505e-07, "x": 0.07878521126760564, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6272007042253521, "term": "dynamics"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.775988807213129e-06, "x": 0.07922535211267606, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6294014084507042, "term": "clauses"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.1713233415494215e-06, "x": 0.07966549295774648, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6298415492957746, "term": "storyline"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 9.79846382427733e-08, "x": 0.0801056338028169, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6307218309859155, "term": "environment"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.306562958744278e-07, "x": 0.08054577464788733, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6324823943661971, "term": "fail"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.359116882424934e-07, "x": 0.08098591549295775, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6338028169014085, "term": "capability"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.2741735437295598e-07, "x": 0.08142605633802817, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6368838028169014, "term": "directions"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.7138908142547865e-07, "x": 0.0818661971830986, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6404049295774648, "term": "lists"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.5343386524762842e-07, "x": 0.08230633802816902, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6426056338028169, "term": "numbers"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 8.92291605250979e-08, "x": 0.08274647887323944, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6434859154929577, "term": "discussion"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.4560080824857006e-07, "x": 0.08318661971830986, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.647887323943662, "term": "genre"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.1453504171992793e-06, "x": 0.08362676056338028, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6505281690140845, "term": "intra"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.0840669014084507, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6514084507042254, "term": "enjambment"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.7953309028862906e-06, "x": 0.08450704225352113, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.653169014084507, "term": "scholarly"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.08494718309859155, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6549295774647887, "term": "hashtag"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.3474812664539362e-07, "x": 0.08538732394366197, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6571302816901409, "term": "historical"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.063026924864317e-07, "x": 0.08582746478873239, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6580105633802817, "term": "script"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.065000334527765e-07, "x": 0.08626760563380281, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6610915492957746, "term": "restricted"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.9605366387132797e-06, "x": 0.08670774647887323, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6628521126760564, "term": "poses"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.7275764783592585e-07, "x": 0.08714788732394366, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6632922535211268, "term": "depending"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.3459721856024154e-06, "x": 0.08758802816901408, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6650528169014085, "term": "induce"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.5849782105120508e-07, "x": 0.0880281690140845, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6654929577464789, "term": "survey"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.3404254349479472e-06, "x": 0.08846830985915492, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6698943661971831, "term": "rewards"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.1821880669700082e-06, "x": 0.08890845070422536, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.676056338028169, "term": "rarely"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0900242290585633e-06, "x": 0.08934859154929578, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6764964788732394, "term": "stronger"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.0060836492747902e-06, "x": 0.0897887323943662, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6769366197183099, "term": "deployed"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.09022887323943662, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.679137323943662, "term": "mini batch"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.09066901408450705, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6795774647887324, "term": "cost weighting"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.1785253819011502e-05, "x": 0.09110915492957747, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6808978873239436, "term": "untranslated"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.46807584279103e-06, "x": 0.09154929577464789, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6822183098591549, "term": "mismatch"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.3534001918614385e-07, "x": 0.09198943661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6826584507042254, "term": "rare"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.09242957746478873, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.684419014084507, "term": "seq2seq"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0935350861825936e-06, "x": 0.09286971830985916, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6875, "term": "stanford"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.5788921801283975e-06, "x": 0.09330985915492958, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6945422535211268, "term": "scaling"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.52880027731884e-08, "x": 0.09375, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6963028169014085, "term": "age"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.725040446346221e-06, "x": 0.09419014084507042, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6967429577464789, "term": "mci"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.267575261890425e-06, "x": 0.09463028169014084, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6971830985915493, "term": "enriched"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.7345039155671384e-08, "x": 0.09507042253521127, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.6980633802816901, "term": "am"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.09551056338028169, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7002640845070423, "term": "bi directional"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.2406076657870585e-06, "x": 0.09595070422535211, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7007042253521126, "term": "clues"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.4473034387060617e-05, "x": 0.09639084507042253, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7037852112676056, "term": "reordering"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.9799143627640674e-07, "x": 0.09683098591549295, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.704225352112676, "term": "regarding"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.137584744279989e-07, "x": 0.09727112676056338, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7055457746478874, "term": "flow"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.4136269319162294e-07, "x": 0.0977112676056338, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.707306338028169, "term": "inventory"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.09815140845070422, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7099471830985915, "term": "cloze style"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.4967848313430337e-06, "x": 0.09859154929577464, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.710387323943662, "term": "considers"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.09903169014084508, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7125880281690141, "term": "pronoun resolution"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.6203384887102916e-05, "x": 0.0994718309859155, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7130281690140845, "term": "pronoun"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.09991197183098592, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7139084507042254, "term": "zero pronoun"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.1596468411509958e-06, "x": 0.10035211267605634, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.715669014084507, "term": "assuming"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.10079225352112677, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7161091549295775, "term": "15"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.813863763409958e-07, "x": 0.10123239436619719, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7174295774647887, "term": "flexibility"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.7099859692231255e-07, "x": 0.10167253521126761, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.71875, "term": "forward"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.9358273242026808e-06, "x": 0.10211267605633803, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.721830985915493, "term": "subjective"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.508335320419397e-08, "x": 0.10255281690140845, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7244718309859155, "term": "man"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.2014910258419825e-06, "x": 0.10299295774647887, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7253521126760564, "term": "variant"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.1034330985915493, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7284330985915493, "term": "2015"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.16147072629858e-07, "x": 0.10387323943661972, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7288732394366197, "term": "intelligence"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.97296516291314e-07, "x": 0.10431338028169014, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.730193661971831, "term": "powerful"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.0247466623432958e-06, "x": 0.10475352112676056, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7315140845070423, "term": "ignoring"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.106215509432207e-08, "x": 0.10519366197183098, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7328345070422535, "term": "prices"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2150950283293332e-06, "x": 0.1056338028169014, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.733274647887324, "term": "numerical"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.10607394366197183, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7337147887323944, "term": "stock prices"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.132435834543819e-07, "x": 0.10651408450704225, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7341549295774648, "term": "grid"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0954403740411829e-07, "x": 0.10695422535211267, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7350352112676056, "term": "deals"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 9.688282419633035e-07, "x": 0.1073943661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7359154929577465, "term": "depend"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.7324426037434273e-06, "x": 0.10783450704225352, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7363556338028169, "term": "selective"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 8.640677180239429e-07, "x": 0.10827464788732394, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7367957746478874, "term": "pointer"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.172423134320916e-08, "x": 0.10871478873239436, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7381161971830986, "term": "start"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.5628865702251042e-06, "x": 0.10915492957746478, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7403169014084507, "term": "consistency"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.07298130827418e-06, "x": 0.10959507042253522, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7411971830985915, "term": "suffers"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.027977323528369e-07, "x": 0.11003521126760564, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.741637323943662, "term": "modes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0033358911709654e-06, "x": 0.11047535211267606, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7420774647887324, "term": "adjacent"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.5518536665033476e-05, "x": 0.11091549295774648, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7425176056338029, "term": "argumentative"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.709210628418635e-06, "x": 0.1113556338028169, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7438380281690141, "term": "neighbor"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.8563261926797096e-07, "x": 0.11179577464788733, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7460387323943662, "term": "readers"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.536530027429782e-07, "x": 0.11223591549295775, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7464788732394366, "term": "alone"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.669016993870517e-08, "x": 0.11267605633802817, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7486795774647887, "term": "room"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.2561941893704104e-07, "x": 0.1131161971830986, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7504401408450704, "term": "seven"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 5.918693720621083e-06, "x": 0.11355633802816902, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7508802816901409, "term": "rst"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.0953510446372634e-05, "x": 0.11399647887323944, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7513204225352113, "term": "parses"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.149174673148307e-07, "x": 0.11443661971830986, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7526408450704225, "term": "simulation"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.4956332549379316e-06, "x": 0.11487676056338028, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7592429577464789, "term": "intrinsic"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2495554706413192e-06, "x": 0.1153169014084507, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7596830985915493, "term": "extends"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 8.045052292839903e-05, "x": 0.11575704225352113, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7610035211267606, "term": "multiword"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.784408723175635e-07, "x": 0.11619718309859155, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7623239436619719, "term": "partial"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.11663732394366197, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7632042253521126, "term": "grounded verb"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.6007843736801995e-08, "x": 0.11707746478873239, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7640845070422535, "term": "policy"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.299480661743842e-05, "x": 0.11751760563380281, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7649647887323944, "term": "attentional"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.195412232815172e-06, "x": 0.11795774647887323, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7658450704225352, "term": "synonyms"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 9.989176427665332e-08, "x": 0.11839788732394366, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.766725352112676, "term": "sound"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.11883802816901408, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7676056338028169, "term": "smatch"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.786629933555233e-05, "x": 0.1192781690140845, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.769806338028169, "term": "captioning"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.753326139966442e-05, "x": 0.11971830985915492, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7702464788732394, "term": "perceptron"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.2445242164798473e-06, "x": 0.12015845070422536, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7720070422535211, "term": "ideology"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.216016296564e-06, "x": 0.12059859154929578, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7724471830985915, "term": "defining"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.1210387323943662, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7733274647887324, "term": "binary codes"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.8103236677871426e-06, "x": 0.12147887323943662, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7737676056338029, "term": "estimating"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.451178944160492e-07, "x": 0.12191901408450705, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7759683098591549, "term": "divided"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.008514064429296e-06, "x": 0.12235915492957747, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7772887323943662, "term": "decode"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.8248649421064616e-07, "x": 0.12279929577464789, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7786091549295775, "term": "sql"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.12323943661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7799295774647887, "term": "triviaqa"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.058369966861659e-07, "x": 0.12367957746478873, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.78125, "term": "insight"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2756646831696687e-07, "x": 0.12411971830985916, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7816901408450704, "term": "players"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.784695735511381e-07, "x": 0.12455985915492958, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.784330985915493, "term": "missing"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.125, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7847711267605634, "term": "penn treebank"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 7.905104594810425e-07, "x": 0.12544014084507044, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7852112676056338, "term": "extensions"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.132342397626383e-06, "x": 0.12588028169014084, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7856514084507042, "term": "constituency"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.651167460795589e-05, "x": 0.12632042253521128, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7865316901408451, "term": "kbp"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0656568218447711e-06, "x": 0.1267605633802817, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7878521126760564, "term": "scored"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 6.723976462854871e-07, "x": 0.12720070422535212, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7882922535211268, "term": "targets"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.075722125729984e-06, "x": 0.12764084507042253, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7891725352112676, "term": "hyper"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.3937443585363313e-07, "x": 0.12808098591549297, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7922535211267606, "term": "addresses"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.023880217089199e-07, "x": 0.12852112676056338, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.792693661971831, "term": "covers"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.682042278641663e-06, "x": 0.1289612676056338, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7935739436619719, "term": "demonstrations"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.816799720304916e-06, "x": 0.12940140845070422, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7948943661971831, "term": "duluth"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0857733945974458e-07, "x": 0.12984154929577466, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.7953345070422535, "term": "included"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.1538323966137327e-05, "x": 0.13028169014084506, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.797975352112676, "term": "svr"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.887723685964439e-07, "x": 0.1307218309859155, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8023767605633803, "term": "workshop"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.1311619718309859, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8032570422535211, "term": "conceptnet"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.13160211267605634, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.804137323943662, "term": "random forest"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 2.2849522867688236e-07, "x": 0.13204225352112675, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8045774647887324, "term": "forest"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.1324823943661972, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8072183098591549, "term": "d."}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.1329225352112676, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8080985915492958, "term": "5th"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.0799524432142106e-06, "x": 0.13336267605633803, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8142605633802817, "term": "scales"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.4089065182383315e-07, "x": 0.13380281690140844, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.815580985915493, "term": "eat"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.3835593031565214e-06, "x": 0.13424295774647887, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8160211267605634, "term": "thesaurus"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.13468309859154928, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8169014084507042, "term": "c."}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.13512323943661972, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8177816901408451, "term": "0"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.13556338028169015, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8191021126760564, "term": "semeval-2016"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.13600352112676056, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8204225352112676, "term": "task1"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.569287913184268e-08, "x": 0.136443661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8217429577464789, "term": "business"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.1368838028169014, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8221830985915493, "term": "weight sharing"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 4.78413962033068e-05, "x": 0.13732394366197184, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8230633802816901, "term": "mwe"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 0.00010254306808859722, "x": 0.13776408450704225, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8235035211267606, "term": "disfluency"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 1.2604296456783534e-07, "x": 0.1382042253521127, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8248239436619719, "term": "income"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.1784799110534184e-06, "x": 0.1386443661971831, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8261443661971831, "term": "enron"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.960369375730936e-06, "x": 0.13908450704225353, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.8265845070422535, "term": "antibiotic"}, {"ncat": 5, "cat": 0, "cat25k": 0, "bg": 3.621246577921984e-05, "x": 0.13952464788732394, "s": 0.7288732394366197, "ncat25k": 1, "os": 0.4398981109481602, "y": 0.827024647887324, "term": "plf"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 2.6420442235930327e-07, "x": 0.13996478873239437, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.8415492957746479, "term": "authors"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00012305041990955793, "x": 0.14040492957746478, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.03697183098591549, "term": "crowdsourcing"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1299302990254133e-07, "x": 0.14084507042253522, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.05237676056338028, "term": "solutions"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0531907955442552e-07, "x": 0.14128521126760563, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.06558098591549295, "term": "central"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.2215557554955736e-05, "x": 0.14172535211267606, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.06734154929577464, "term": "typology"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.614771640686806e-07, "x": 0.14216549295774647, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.06998239436619719, "term": "hate"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.959723797584224e-05, "x": 0.1426056338028169, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.09463028169014084, "term": "taggers"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.3858749317167873e-05, "x": 0.1430457746478873, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.09507042253521127, "term": "adjectives"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.816895668860468e-08, "x": 0.14348591549295775, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.10431338028169014, "term": "next"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.778541299225121e-07, "x": 0.14392605633802816, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.10695422535211267, "term": "counts"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.027401773944707e-07, "x": 0.1443661971830986, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.11267605633802817, "term": "shift"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.3930299248274217e-06, "x": 0.144806338028169, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.12191901408450705, "term": "compares"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.744304533871621e-08, "x": 0.14524647887323944, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.136443661971831, "term": "experience"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.185647453645508e-08, "x": 0.14568661971830985, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.14744718309859156, "term": "house"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.225543006044956e-07, "x": 0.14612676056338028, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.16461267605633803, "term": "utility"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.180104626081268e-07, "x": 0.14656690140845072, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.1932218309859155, "term": "allowing"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.5630676296378983e-06, "x": 0.14700704225352113, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.19806338028169015, "term": "insights"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 4.2645170251703975e-07, "x": 0.14744718309859156, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.8741197183098591, "term": "estimated"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1152584267532652e-06, "x": 0.14788732394366197, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.21610915492957747, "term": "ir"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 1.4197091117437829e-06, "x": 0.1483274647887324, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.8802816901408451, "term": "studying"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.1487676056338028, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.22975352112676056, "term": "tf idf"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.6402048928872497e-07, "x": 0.14920774647887325, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.23107394366197184, "term": "fourth"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.1735346436969938e-06, "x": 0.14964788732394366, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.23283450704225353, "term": "tf"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.765191231095096e-06, "x": 0.1500880281690141, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.238556338028169, "term": "dementia"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.3665951975339334e-06, "x": 0.1505281690140845, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.24339788732394366, "term": "obtaining"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.077327920459932e-07, "x": 0.15096830985915494, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.2517605633802817, "term": "superior"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.2467900812231468e-07, "x": 0.15140845070422534, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.25880281690140844, "term": "rest"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.097039782983141e-07, "x": 0.15184859154929578, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.2654049295774648, "term": "assignment"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.1864711463698817e-07, "x": 0.1522887323943662, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.2676056338028169, "term": "mentioned"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.977908737312952e-06, "x": 0.15272887323943662, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.2689260563380282, "term": "viral"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.7114484062546873e-07, "x": 0.15316901408450703, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.2715669014084507, "term": "species"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.4082916928863195e-06, "x": 0.15360915492957747, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.27244718309859156, "term": "complicated"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.1467213916480243e-06, "x": 0.15404929577464788, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.2772887323943662, "term": "validate"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.905601034425077e-07, "x": 0.1544894366197183, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.27816901408450706, "term": "symptoms"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.7673688751787e-06, "x": 0.15492957746478872, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.2944542253521127, "term": "punctuation"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1506315768782502e-06, "x": 0.15536971830985916, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.30545774647887325, "term": "optimal"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 2.4005511665478396e-07, "x": 0.15580985915492956, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.891725352112676, "term": "technologies"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.491657877516535e-07, "x": 0.15625, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3261443661971831, "term": "procedures"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0779427139920646e-06, "x": 0.15669014084507044, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3265845070422535, "term": "passage"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.5090174924175961e-07, "x": 0.15713028169014084, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3331866197183099, "term": "drug"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.8491118958520252e-08, "x": 0.15757042253521128, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3380281690140845, "term": "go"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.962433398474665e-08, "x": 0.1580105633802817, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.34242957746478875, "term": "looking"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.7327624744344376e-06, "x": 0.15845070422535212, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.34419014084507044, "term": "simulated"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.188426633120675e-05, "x": 0.15889084507042253, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.34683098591549294, "term": "umls"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.15933098591549297, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3587147887323944, "term": "-"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.649565710587545e-06, "x": 0.15977112676056338, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3595950704225352, "term": "comparisons"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0150123970653724e-07, "x": 0.1602112676056338, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3613556338028169, "term": "reference"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 1.3604100275823134e-07, "x": 0.16065140845070422, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.9018485915492958, "term": "done"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 1.1355016597262777e-07, "x": 0.16109154929577466, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.9027288732394366, "term": "might"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.5250670773547985e-06, "x": 0.16153169014084506, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.37764084507042256, "term": "traditionally"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1848468748330724e-06, "x": 0.1619718309859155, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.38028169014084506, "term": "aimed"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.2736373735964198e-07, "x": 0.1624119718309859, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.3912852112676056, "term": "string"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.807174482971095e-06, "x": 0.16285211267605634, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.39436619718309857, "term": "organizers"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.467142787104255e-06, "x": 0.16329225352112675, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.40008802816901406, "term": "captured"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 2.7735678979327014e-05, "x": 0.1637323943661972, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.9185739436619719, "term": "preprocessing"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 8.295285279215778e-08, "x": 0.1641725352112676, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.9190140845070423, "term": "select"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 4.234083098719765e-06, "x": 0.16461267605633803, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.922975352112676, "term": "employing"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 1.3145202944563018e-07, "x": 0.16505281690140844, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.9242957746478874, "term": "lot"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 1.6565837905405989e-06, "x": 0.16549295774647887, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.926056338028169, "term": "argue"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 6.068354377158194e-07, "x": 0.16593309859154928, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.9269366197183099, "term": "adopted"}, {"ncat": 6, "cat": 1, "cat25k": 22, "bg": 1.9987491827614277e-07, "x": 0.16637323943661972, "s": 0.974911971830986, "ncat25k": 2, "os": 0.891111715133906, "y": 0.929137323943662, "term": "values"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.2348666548591345e-06, "x": 0.16681338028169015, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.40316901408450706, "term": "usefulness"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.6540823165092025e-06, "x": 0.16725352112676056, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4128521126760563, "term": "discusses"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.395259053821596e-07, "x": 0.167693661971831, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.41725352112676056, "term": "exposure"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.6063396488749484e-06, "x": 0.1681338028169014, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4203345070422535, "term": "predicts"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.292587700052648e-07, "x": 0.16857394366197184, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4238556338028169, "term": "connections"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.930786503399899e-08, "x": 0.16901408450704225, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4251760563380282, "term": "look"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.9275567211854354e-08, "x": 0.1694542253521127, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4273767605633803, "term": "digital"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.7972924540122203e-07, "x": 0.1698943661971831, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4317781690140845, "term": "appear"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.78290276479189e-06, "x": 0.17033450704225353, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.43309859154929575, "term": "roughly"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.132754301134598e-07, "x": 0.17077464788732394, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4410211267605634, "term": "informed"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.143375858986556e-07, "x": 0.17121478873239437, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.45202464788732394, "term": "regions"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.4196368044501965e-06, "x": 0.17165492957746478, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4551056338028169, "term": "tends"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.64435489470394e-07, "x": 0.17209507042253522, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.45774647887323944, "term": "running"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.359313893730344e-06, "x": 0.17253521126760563, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4621478873239437, "term": "contributes"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.17297535211267606, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4639084507042254, "term": "story cloze"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.540399530115896e-07, "x": 0.17341549295774647, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4647887323943662, "term": "operate"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.3473166835161624e-07, "x": 0.1738556338028169, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.46875, "term": "papers"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.925010057370709e-07, "x": 0.1742957746478873, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4753521126760563, "term": "weak"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.660258075690655e-05, "x": 0.17473591549295775, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4784330985915493, "term": "dialectal"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.351446344539224e-08, "x": 0.17517605633802816, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4867957746478873, "term": "student"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.550343483636423e-06, "x": 0.1756161971830986, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.48811619718309857, "term": "pivot"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.896218803230189e-06, "x": 0.176056338028169, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.49559859154929575, "term": "kernels"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0128434457170354e-06, "x": 0.17649647887323944, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.49735915492957744, "term": "essay"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.4857233485949923e-06, "x": 0.17693661971830985, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.4982394366197183, "term": "demonstrating"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.5937823244869215e-06, "x": 0.17737676056338028, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5026408450704225, "term": "posterior"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.955116288533325e-06, "x": 0.17781690140845072, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5039612676056338, "term": "hypotheses"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.514021398743223e-08, "x": 0.17825704225352113, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.508362676056338, "term": "tv"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.064753720441465e-07, "x": 0.17869718309859156, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5149647887323944, "term": "observed"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.286780994480513e-07, "x": 0.17913732394366197, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.516725352112676, "term": "consistent"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.501520097412289e-07, "x": 0.1795774647887324, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5189260563380281, "term": "applicable"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0449566473610928e-05, "x": 0.1800176056338028, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5246478873239436, "term": "robustness"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.323205048527668e-06, "x": 0.18045774647887325, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5277288732394366, "term": "utilizes"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.728438700747504e-07, "x": 0.18089788732394366, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5308098591549296, "term": "fashion"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.29555522783693e-07, "x": 0.1813380281690141, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5334507042253521, "term": "holds"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.2808152682943906e-07, "x": 0.1817781690140845, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5338908450704225, "term": "internal"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.148722300117294e-06, "x": 0.18221830985915494, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5360915492957746, "term": "theoretical"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.6812633012445553e-05, "x": 0.18265845070422534, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5378521126760564, "term": "grammars"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.078577864739317e-06, "x": 0.18309859154929578, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5435739436619719, "term": "learners"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.573769598719292e-06, "x": 0.1835387323943662, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.547975352112676, "term": "inherent"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 9.336728385631344e-08, "x": 0.18397887323943662, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5514964788732394, "term": "issue"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.023596920620568e-07, "x": 0.18441901408450703, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5536971830985915, "term": "reduced"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.48025171909347e-07, "x": 0.18485915492957747, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5545774647887324, "term": "specified"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.591442392084456e-07, "x": 0.18529929577464788, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5567781690140845, "term": "helpful"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.2612792634248593e-07, "x": 0.1857394366197183, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5660211267605634, "term": "serve"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.184091937046045e-07, "x": 0.18617957746478872, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5735035211267606, "term": "configuration"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.32836505504352e-06, "x": 0.18661971830985916, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.573943661971831, "term": "configurations"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.764454960635889e-07, "x": 0.18705985915492956, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5761443661971831, "term": "ending"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.320363420896702e-06, "x": 0.1875, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.577024647887324, "term": "greedy"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.060158007815804e-05, "x": 0.18794014084507044, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5779049295774648, "term": "gru"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.3942549851631015e-07, "x": 0.18838028169014084, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5827464788732394, "term": "aid"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.79690541865734e-06, "x": 0.18882042253521128, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5880281690140845, "term": "prevalent"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.0200175320688307e-07, "x": 0.1892605633802817, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.5977112676056338, "term": "interface"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.589806838169082e-07, "x": 0.18970070422535212, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6029929577464789, "term": "broad"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.923947941175292e-06, "x": 0.19014084507042253, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6043133802816901, "term": "analyzed"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.7926862618756367e-07, "x": 0.19058098591549297, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.610475352112676, "term": "sciences"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.3237369318621746e-07, "x": 0.19102112676056338, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6188380281690141, "term": "commercial"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.91510656635488e-07, "x": 0.1914612676056338, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6223591549295775, "term": "maintaining"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.7393396596981954e-05, "x": 0.19190140845070422, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6227992957746479, "term": "commonsense"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.7132171034492995e-06, "x": 0.19234154929577466, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.628080985915493, "term": "interpretations"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.19278169014084506, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6316021126760564, "term": "&"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.9863476875701966e-07, "x": 0.1932218309859155, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.633362676056338, "term": "claim"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.560932299555121e-06, "x": 0.1936619718309859, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6346830985915493, "term": "acquiring"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.9947896095398817e-06, "x": 0.19410211267605634, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6382042253521126, "term": "clusters"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.288689195057217e-07, "x": 0.19454225352112675, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.639524647887324, "term": "templates"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1056253665839106e-06, "x": 0.1949823943661972, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6430457746478874, "term": "translate"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 9.734961613829487e-07, "x": 0.1954225352112676, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6456866197183099, "term": "arc"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.931737462523127e-07, "x": 0.19586267605633803, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6461267605633803, "term": "oracle"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 9.72138037768535e-07, "x": 0.19630281690140844, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6527288732394366, "term": "besides"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.450486390917517e-06, "x": 0.19674295774647887, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6544894366197183, "term": "poorly"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.037548894491653e-05, "x": 0.19718309859154928, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6553697183098591, "term": "rhetorical"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.338842844238785e-05, "x": 0.19762323943661972, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6566901408450704, "term": "annotator"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 9.463934888191064e-08, "x": 0.19806338028169015, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6588908450704225, "term": "light"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.5786238751548465e-07, "x": 0.19850352112676056, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6597711267605634, "term": "believe"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.864676567809181e-07, "x": 0.198943661971831, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6602112676056338, "term": "comes"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1327671222045597e-07, "x": 0.1993838028169014, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6659330985915493, "term": "daily"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0989784903350793e-06, "x": 0.19982394366197184, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.667693661971831, "term": "preference"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.809061680410259e-06, "x": 0.20026408450704225, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6690140845070423, "term": "modelling"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.9628514015167935e-06, "x": 0.2007042253521127, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6734154929577465, "term": "framing"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.215596358224266e-07, "x": 0.2011443661971831, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6738556338028169, "term": "longer"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.37179571844876e-07, "x": 0.20158450704225353, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6751760563380281, "term": "addressed"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00010814708002883922, "x": 0.20202464788732394, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6800176056338029, "term": "rerank"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.967738097261648e-07, "x": 0.20246478873239437, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6866197183098591, "term": "reducing"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.3167740221553812e-06, "x": 0.20290492957746478, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6879401408450704, "term": "guided"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.113264965103157e-07, "x": 0.20334507042253522, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6888204225352113, "term": "rc"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.057737732738433e-05, "x": 0.20378521126760563, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6897007042253521, "term": "inflection"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0837462180643699e-06, "x": 0.20422535211267606, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6910211267605634, "term": "projection"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.333290297090076e-07, "x": 0.20466549295774647, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6936619718309859, "term": "keywords"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.0034995794988302e-07, "x": 0.2051056338028169, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6941021126760564, "term": "express"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.8208176613475574e-06, "x": 0.2055457746478873, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.6993838028169014, "term": "directional"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.7024091251066105e-06, "x": 0.20598591549295775, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7024647887323944, "term": "expressive"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.2303592791256265e-07, "x": 0.20642605633802816, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7051056338028169, "term": "manner"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.126784537969606e-07, "x": 0.2068661971830986, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7064260563380281, "term": "ask"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.9708640596514855e-05, "x": 0.207306338028169, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7086267605633803, "term": "principled"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.7829544988124355e-07, "x": 0.20774647887323944, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7121478873239436, "term": "handling"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.008821250036115e-08, "x": 0.20818661971830985, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7152288732394366, "term": "call"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.6325544416092418e-06, "x": 0.20862676056338028, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7165492957746479, "term": "synthetic"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.5171029388619065e-06, "x": 0.20906690140845072, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7209507042253521, "term": "relied"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.8340463188397824e-06, "x": 0.20950704225352113, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7213908450704225, "term": "tackle"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.544057294600468e-06, "x": 0.20994718309859156, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7227112676056338, "term": "bayesian"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.1832257584294e-07, "x": 0.21038732394366197, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.727112676056338, "term": "integrated"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.356238140640814e-08, "x": 0.2108274647887324, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7279929577464789, "term": "did"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.469460928258468e-06, "x": 0.2112676056338028, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7306338028169014, "term": "translated"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.9580730863833315e-06, "x": 0.21170774647887325, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7323943661971831, "term": "utilize"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1018973110675393e-06, "x": 0.21214788732394366, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7372359154929577, "term": "fewer"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.955140749178296e-08, "x": 0.2125880281690141, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7376760563380281, "term": "play"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 7.876708507304662e-06, "x": 0.2130281690140845, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.738556338028169, "term": "implicitly"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.21346830985915494, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7389964788732394, "term": "class ties"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.1178318682128392e-06, "x": 0.21390845070422534, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7394366197183099, "term": "ties"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.0938279187524973e-06, "x": 0.21434859154929578, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7433978873239436, "term": "vertex"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.372808129924875e-06, "x": 0.2147887323943662, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7447183098591549, "term": "aggregation"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.541999659795712e-06, "x": 0.21522887323943662, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7477992957746479, "term": "lemma"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.534392498198205e-07, "x": 0.21566901408450703, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7482394366197183, "term": "families"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.778395198933096e-05, "x": 0.21610915492957747, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7544014084507042, "term": "dnn"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 3.775541538508007e-06, "x": 0.21654929577464788, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7548415492957746, "term": "spontaneous"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0953964321842808e-06, "x": 0.2169894366197183, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7557218309859155, "term": "acoustic"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.423674452047601e-05, "x": 0.21742957746478872, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7570422535211268, "term": "referential"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.7111536862652954e-05, "x": 0.21786971830985916, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7579225352112676, "term": "antonyms"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.203894115073288e-05, "x": 0.21830985915492956, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7588028169014085, "term": "derivational"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00012305041990955793, "x": 0.21875, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7605633802816901, "term": "sgns"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 8.712953700743587e-07, "x": 0.21919014084507044, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7689260563380281, "term": "boost"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.146424807519355e-06, "x": 0.21963028169014084, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7711267605633803, "term": "likelihood"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 6.972325677332387e-05, "x": 0.22007042253521128, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.772887323943662, "term": "geolocation"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 9.122346632447731e-06, "x": 0.2205105633802817, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7742077464788732, "term": "embed"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 9.801279066918232e-05, "x": 0.22095070422535212, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7750880281690141, "term": "synsets"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 9.353915689416325e-07, "x": 0.22139084507042253, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7803697183098591, "term": "meta"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.5512509102119133e-06, "x": 0.22183098591549297, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7808098591549296, "term": "simplicity"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.271597236564209e-08, "x": 0.22227112676056338, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7821302816901409, "term": "details"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.8113035302456747e-06, "x": 0.2227112676056338, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7830105633802817, "term": "systematic"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.137659766686906e-07, "x": 0.22315140845070422, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7834507042253521, "term": "opinion"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.521091451079733e-07, "x": 0.22359154929577466, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7838908450704225, "term": "agree"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 2.98789528920959e-06, "x": 0.22403169014084506, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7869718309859155, "term": "analyzer"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 5.111448750698032e-06, "x": 0.2244718309859155, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7874119718309859, "term": "pagerank"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.2249119718309859, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7904929577464789, "term": "f2f"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.22535211267605634, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.7997359154929577, "term": "7th"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00012305041990955793, "x": 0.22579225352112675, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8067781690140845, "term": "takelab"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.2262323943661972, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.809419014084507, "term": "6 :"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00012305041990955793, "x": 0.2266725352112676, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8116197183098591, "term": "niletmrg"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.22711267605633803, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8120598591549296, "term": "determining rumour"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.22755281690140844, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8138204225352113, "term": "rumour veracity"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.0233576261377816e-06, "x": 0.22799295774647887, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8147007042253521, "term": "turkish"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00012305041990955793, "x": 0.22843309859154928, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8164612676056338, "term": "ecnu"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 4.892353345491104e-07, "x": 0.22887323943661972, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8186619718309859, "term": "participate"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00010399514689314498, "x": 0.22931338028169015, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8199823943661971, "term": "camr"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 1.2626038110222998e-06, "x": 0.22975352112676056, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8213028169014085, "term": "compound"}, {"ncat": 6, "cat": 0, "cat25k": 0, "bg": 0.00012305041990955793, "x": 0.230193661971831, "s": 0.6439260563380281, "ncat25k": 2, "os": 0.4256464383516332, "y": 0.8252640845070423, "term": "evinets"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.603819687043099e-07, "x": 0.2306338028169014, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.002640845070422535, "term": "newly"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.394977268455957e-07, "x": 0.23107394366197184, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.015404929577464789, "term": "discover"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.951883001680962e-06, "x": 0.23151408450704225, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.02596830985915493, "term": "constructive"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.626030797412516e-07, "x": 0.2319542253521127, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.03345070422535211, "term": "vary"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.0749076945655536e-07, "x": 0.2323943661971831, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.03389084507042254, "term": "individuals"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.23283450704225353, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.03433098591549296, "term": "determine whether"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.304831221479535e-06, "x": 0.23327464788732394, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.04357394366197183, "term": "neutral"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.7352196377186132e-07, "x": 0.23371478873239437, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.058098591549295774, "term": "availability"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.6070276144049275e-07, "x": 0.23415492957746478, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.06690140845070422, "term": "taken"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.6039591210395673e-06, "x": 0.23459507042253522, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.07658450704225352, "term": "validation"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 1.3752379806349304e-05, "x": 0.23503521126760563, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.8503521126760564, "term": "pooling"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.907548473505899e-07, "x": 0.23547535211267606, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.09595070422535211, "term": "poor"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.1802119602067714e-06, "x": 0.23591549295774647, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.09639084507042253, "term": "hypothesis"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3592681700172627e-05, "x": 0.2363556338028169, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.0994718309859155, "term": "nouns"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.7842847413589417e-07, "x": 0.2367957746478873, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.10915492957746478, "term": "seen"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 1.1914433517088649e-06, "x": 0.23723591549295775, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.8569542253521126, "term": "environments"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.23767605633802816, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.12323943661971831, "term": "logistic regression"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.1514167079021538e-05, "x": 0.2381161971830986, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.1311619718309859, "term": "granularity"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.1987139342147922e-06, "x": 0.238556338028169, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.14392605633802816, "term": "hmm"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.4237530202392715e-06, "x": 0.23899647887323944, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.15669014084507044, "term": "calculating"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.2297373324976972e-06, "x": 0.23943661971830985, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.15933098591549297, "term": "shelf"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.5721847281119882e-06, "x": 0.23987676056338028, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.16065140845070422, "term": "bias"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.4506572846636763e-07, "x": 0.24031690140845072, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.17033450704225353, "term": "modern"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.2549396441260085e-06, "x": 0.24075704225352113, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.1738556338028169, "term": "coding"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3617453458972757e-07, "x": 0.24119718309859156, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.18617957746478872, "term": "window"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.1616159994169815e-06, "x": 0.24163732394366197, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.18970070422535212, "term": "informative"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.3527187555798297e-07, "x": 0.2420774647887324, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.19278169014084506, "term": "involved"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.6281367185576603e-06, "x": 0.2425176056338028, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.20642605633802816, "term": "tuned"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.7648752510535045e-06, "x": 0.24295774647887325, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2306338028169014, "term": "batch"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.561231271389319e-07, "x": 0.24339788732394366, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.23327464788732394, "term": "fifth"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 9.64592908583022e-08, "x": 0.2438380281690141, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2367957746478873, "term": "blog"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.686598148848707e-07, "x": 0.2442781690140845, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.24163732394366197, "term": "promise"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.498332505757619e-07, "x": 0.24471830985915494, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2425176056338028, "term": "involving"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.6057526693193847e-07, "x": 0.24515845070422534, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.24515845070422534, "term": "growing"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.3720834104823884e-07, "x": 0.24559859154929578, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2460387323943662, "term": "experts"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 9.916859388432806e-08, "x": 0.2460387323943662, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.24691901408450703, "term": "act"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.9440008882417774e-07, "x": 0.24647887323943662, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2522007042253521, "term": "names"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.72985213800453e-06, "x": 0.24691901408450703, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.26188380281690143, "term": "taxonomy"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.483637638385552e-07, "x": 0.24735915492957747, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2711267605633803, "term": "proteins"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 2.410531612615517e-06, "x": 0.24779929577464788, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.8882042253521126, "term": "identifies"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.2151450097213225e-06, "x": 0.2482394366197183, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2764084507042254, "term": "enhancing"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.106356682412925e-07, "x": 0.24867957746478872, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2777288732394366, "term": "reflect"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.0508049165660897e-05, "x": 0.24911971830985916, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2834507042253521, "term": "divergence"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.4644127819805264e-06, "x": 0.24955985915492956, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2869718309859155, "term": "assessments"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.809998536793619e-06, "x": 0.25, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.2992957746478873, "term": "investigating"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3464621989441565e-07, "x": 0.25044014084507044, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.30809859154929575, "term": "planning"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 1.0738148372754417e-06, "x": 0.25088028169014087, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.8921654929577465, "term": "suggests"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.322626297076877e-07, "x": 0.25132042253521125, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3173415492957746, "term": "perfect"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.9703576579146662e-06, "x": 0.2517605633802817, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3217429577464789, "term": "assign"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.00012067509093730066, "x": 0.2522007042253521, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.32526408450704225, "term": "olelo"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.494793869684201e-08, "x": 0.25264084507042256, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3296654929577465, "term": "university"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.00012498772441992304, "x": 0.25308098591549294, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3305457746478873, "term": "summarisation"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.000929358733901e-06, "x": 0.2535211267605634, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.33758802816901406, "term": "pseudo"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.4941380163033937e-06, "x": 0.2539612676056338, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3389084507042254, "term": "performances"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.0985949906537573e-05, "x": 0.25440140845070425, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.34330985915492956, "term": "bayes"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 9.484520382031062e-07, "x": 0.2548415492957746, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.34375, "term": "whereas"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.767321532294941e-05, "x": 0.25528169014084506, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.355193661971831, "term": "wsd"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3625297505937953e-06, "x": 0.2557218309859155, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3569542253521127, "term": "comparative"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.25616197183098594, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.35827464788732394, "term": "wide range"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.417286630032517e-07, "x": 0.2566021126760563, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.366637323943662, "term": "indicates"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.4443830015797226e-08, "x": 0.25704225352112675, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3670774647887324, "term": "would"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.329890852335629e-07, "x": 0.2574823943661972, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.37235915492957744, "term": "encourage"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.231177116836711e-06, "x": 0.2579225352112676, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3824823943661972, "term": "richer"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.472411057759282e-07, "x": 0.258362676056338, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3873239436619718, "term": "exists"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.213082088128131e-05, "x": 0.25880281690140844, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.38776408450704225, "term": "distantly"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.5409810893437384e-07, "x": 0.2592429577464789, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.3899647887323944, "term": "speaker"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.0178634207687374e-06, "x": 0.2596830985915493, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.39524647887323944, "term": "consisted"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.696824700568226e-07, "x": 0.2601232394366197, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4027288732394366, "term": "millions"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 1.941529386382064e-06, "x": 0.2605633802816901, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.9097711267605634, "term": "informal"}, {"ncat": 7, "cat": 6, "cat25k": 133, "bg": 3.1904076907869626e-07, "x": 0.26100352112676056, "s": 0.9986795774647887, "ncat25k": 2, "os": 0.999996837452795, "y": 0.9863556338028169, "term": "french"}, {"ncat": 7, "cat": 2, "cat25k": 44, "bg": 2.1860347115593993e-06, "x": 0.261443661971831, "s": 0.9938380281690141, "ncat25k": 2, "os": 0.9858382555110698, "y": 0.9586267605633803, "term": "heavily"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 2.5678934991850147e-06, "x": 0.26188380281690143, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.9150528169014085, "term": "varying"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 2.570640188215848e-07, "x": 0.2623239436619718, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.9159330985915493, "term": "operations"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 7.743098322394836e-08, "x": 0.26276408450704225, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.9247359154929577, "term": "children"}, {"ncat": 7, "cat": 1, "cat25k": 22, "bg": 1.8197573558288704e-06, "x": 0.2632042253521127, "s": 0.9687500000000001, "ncat25k": 2, "os": 0.8642970741741762, "y": 0.9317781690140845, "term": "genres"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.2517703609390426e-05, "x": 0.2636443661971831, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4040492957746479, "term": "augment"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.0118896308850686e-06, "x": 0.2640845070422535, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.41153169014084506, "term": "indicators"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.834309314248984e-06, "x": 0.26452464788732394, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.414612676056338, "term": "illustrate"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.1168299897570735e-06, "x": 0.2649647887323944, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.41505281690140844, "term": "disorder"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.5963209575676775e-07, "x": 0.2654049295774648, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4194542253521127, "term": "behind"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.563463855516432e-08, "x": 0.2658450704225352, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4198943661971831, "term": "program"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.287939417067087e-08, "x": 0.2662852112676056, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.42869718309859156, "term": "why"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.104300276304898e-06, "x": 0.26672535211267606, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.43045774647887325, "term": "subset"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.154358138583189e-07, "x": 0.2671654929577465, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4308978873239437, "term": "crisis"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.146573743935583e-07, "x": 0.2676056338028169, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.43221830985915494, "term": "assume"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.7346786675041033e-06, "x": 0.2680457746478873, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.43794014084507044, "term": "substantially"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3143269144110313e-05, "x": 0.26848591549295775, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4564260563380282, "term": "lda"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.955768616367744e-06, "x": 0.2689260563380282, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4608274647887324, "term": "emotions"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.2227183514121476e-07, "x": 0.26936619718309857, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.46346830985915494, "term": "carry"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.1408050283228503e-06, "x": 0.269806338028169, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4709507042253521, "term": "arbitrary"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.711076962055995e-05, "x": 0.27024647887323944, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.47139084507042256, "term": "resolvers"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.00014355735116178915, "x": 0.2706866197183099, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4744718309859155, "term": "conll"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.5402692390629883e-05, "x": 0.2711267605633803, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4788732394366197, "term": "dialects"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.0398415281511098e-05, "x": 0.2715669014084507, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.47975352112676056, "term": "dialect"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.1763134169468115e-05, "x": 0.2720070422535211, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.480193661971831, "term": "heuristic"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.27244718309859156, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.48899647887323944, "term": "16"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.2106925234437886e-07, "x": 0.272887323943662, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.4977992957746479, "term": "giving"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.021653840501527e-07, "x": 0.2733274647887324, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5022007042253521, "term": "highest"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.483082272727603e-07, "x": 0.2737676056338028, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5057218309859155, "term": "entire"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.10581677623324e-07, "x": 0.27420774647887325, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5070422535211268, "term": "positions"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.135554505269511e-07, "x": 0.2746478873239437, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5176056338028169, "term": "idea"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.590654661667558e-07, "x": 0.27508802816901406, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5193661971830986, "term": "expressed"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.590656292480566e-07, "x": 0.2755281690140845, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5215669014084507, "term": "density"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.393605225657957e-07, "x": 0.27596830985915494, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5224471830985915, "term": "increases"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3113340191610253e-06, "x": 0.2764084507042254, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5237676056338029, "term": "naturally"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.4344025677445279e-06, "x": 0.27684859154929575, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5259683098591549, "term": "penn"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.00011522349242405538, "x": 0.2772887323943662, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5268485915492958, "term": "morphosyntactic"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 9.370204525930599e-08, "x": 0.2777288732394366, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.528169014084507, "term": "complete"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.591020481299648e-07, "x": 0.27816901408450706, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5286091549295775, "term": "absolute"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.095351483541075e-05, "x": 0.27860915492957744, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5316901408450704, "term": "inferring"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.213593051183117e-05, "x": 0.2790492957746479, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5321302816901409, "term": "phoneme"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.49179451368159e-07, "x": 0.2794894366197183, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5325704225352113, "term": "mixed"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.11099389884459e-07, "x": 0.27992957746478875, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5356514084507042, "term": "classical"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.8406386858470861e-07, "x": 0.28036971830985913, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5365316901408451, "term": "condition"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.8222013102928993e-06, "x": 0.28080985915492956, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5387323943661971, "term": "behavioral"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.804618586127742e-06, "x": 0.28125, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5488556338028169, "term": "indirect"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.764265352466541e-06, "x": 0.28169014084507044, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.551056338028169, "term": "exploits"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.451528468475343e-07, "x": 0.28213028169014087, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5532570422535211, "term": "slot"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.356353603743235e-07, "x": 0.28257042253521125, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5554577464788732, "term": "shot"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.322477709178221e-07, "x": 0.2830105633802817, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5572183098591549, "term": "empty"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.8313305335777725e-06, "x": 0.2834507042253521, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5576584507042254, "term": "integrate"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.773221950846973e-08, "x": 0.28389084507042256, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5598591549295775, "term": "required"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.0655332640077097e-06, "x": 0.28433098591549294, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5607394366197183, "term": "generator"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.249684896625203e-05, "x": 0.2847711267605634, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5611795774647887, "term": "nlg"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.645226744820328e-06, "x": 0.2852112676056338, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5664612676056338, "term": "overlap"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.0558405403248882e-06, "x": 0.28565140845070425, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5669014084507042, "term": "intermediate"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.566687289191748e-06, "x": 0.2860915492957746, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5682218309859155, "term": "heterogeneous"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.466622862479208e-06, "x": 0.28653169014084506, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5717429577464789, "term": "verbs"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.3401499521808476e-06, "x": 0.2869718309859155, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5752640845070423, "term": "variants"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.7410836061632073e-07, "x": 0.28741197183098594, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5783450704225352, "term": "mean"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.7434786596314224e-06, "x": 0.2878521126760563, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.579225352112676, "term": "simpler"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.0507829523100678e-06, "x": 0.28829225352112675, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5831866197183099, "term": "modular"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.138373943865359e-07, "x": 0.2887323943661972, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5840669014084507, "term": "discovery"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.60608293263257e-06, "x": 0.2891725352112676, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5867077464788732, "term": "robots"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.75690523859825e-07, "x": 0.289612676056338, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5871478873239436, "term": "recommendation"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.5544379146948902e-06, "x": 0.29005281690140844, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5972711267605634, "term": "poem"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.6942001399352157e-08, "x": 0.2904929577464789, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.5990316901408451, "term": "service"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.825686550425529e-07, "x": 0.2909330985915493, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6034330985915493, "term": "supports"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.637193072453434e-07, "x": 0.2913732394366197, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6060739436619719, "term": "modules"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.055866281826427e-06, "x": 0.2918133802816901, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6065140845070423, "term": "alleviate"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3045944835781372e-06, "x": 0.29225352112676056, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6069542253521126, "term": "schemes"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.00012397938399957492, "x": 0.292693661971831, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6087147887323944, "term": "ucca"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.00012448760014582835, "x": 0.29313380281690143, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6095950704225352, "term": "factuality"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.813973399894064e-06, "x": 0.2935739436619718, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6170774647887324, "term": "describing"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.5883586785527e-06, "x": 0.29401408450704225, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6183978873239436, "term": "dynamically"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.174255120221567e-05, "x": 0.2944542253521127, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6192781690140845, "term": "spatiotemporal"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.2142661001710556e-06, "x": 0.2948943661971831, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6241197183098591, "term": "measuring"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.3728638361286966e-06, "x": 0.2953345070422535, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6311619718309859, "term": "margin"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 9.533734335819118e-08, "x": 0.29577464788732394, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6351232394366197, "term": "person"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.2239567621341674e-05, "x": 0.2962147887323944, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6373239436619719, "term": "circumstantial"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 9.259249461111682e-07, "x": 0.2966549295774648, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6390845070422535, "term": "yield"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.3035143022981785e-07, "x": 0.2970950704225352, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.641725352112676, "term": "ground"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.4843850643295375e-06, "x": 0.2975352112676056, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6421654929577465, "term": "variations"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.0881955269722863e-06, "x": 0.29797535211267606, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6439260563380281, "term": "literary"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.1563251177196546e-06, "x": 0.2984154929577465, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6492077464788732, "term": "topical"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.5965484973533715e-06, "x": 0.2988556338028169, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6522887323943662, "term": "systematically"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.498449655582169e-05, "x": 0.2992957746478873, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6540492957746479, "term": "sparsity"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.5811145891114603e-07, "x": 0.29973591549295775, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6712147887323944, "term": "clear"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.431346227632016e-07, "x": 0.3001760563380282, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6747359154929577, "term": "apart"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.378085455724922e-07, "x": 0.30061619718309857, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6773767605633803, "term": "depth"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.2368845183122873e-07, "x": 0.301056338028169, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6813380281690141, "term": "until"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.8108019487769505e-06, "x": 0.30149647887323944, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6954225352112676, "term": "compose"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.0350094999839955e-06, "x": 0.3019366197183099, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6985035211267606, "term": "constrained"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.424952458626852e-07, "x": 0.3023767605633803, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.6998239436619719, "term": "faster"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.228872998307898e-08, "x": 0.3028169014084507, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7015845070422535, "term": "book"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.342037315170947e-08, "x": 0.3032570422535211, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7029049295774648, "term": "read"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.1781374124661025e-06, "x": 0.30369718309859156, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7033450704225352, "term": "prerequisite"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.381933510707982e-06, "x": 0.304137323943662, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7095070422535211, "term": "discrete"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.512614042202998e-05, "x": 0.3045774647887324, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7178697183098591, "term": "perplexity"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.832643818709188e-08, "x": 0.3050176056338028, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7191901408450704, "term": "students"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.987755325033633e-06, "x": 0.30545774647887325, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7235915492957746, "term": "caption"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.063788762375362e-05, "x": 0.3058978873239437, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7257922535211268, "term": "decoders"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.5995777542990515e-05, "x": 0.30633802816901406, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7266725352112676, "term": "variational"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.004427588555179e-06, "x": 0.3067781690140845, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7293133802816901, "term": "tend"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.5356265356265356e-05, "x": 0.30721830985915494, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7407570422535211, "term": "predicates"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 8.379710040484773e-07, "x": 0.3076584507042254, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7429577464788732, "term": "calculated"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.420439745090199e-06, "x": 0.30809859154929575, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7442781690140845, "term": "vertices"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.25045762279024e-07, "x": 0.3085387323943662, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7495598591549296, "term": "parameter"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 3.755584419904383e-06, "x": 0.3089788732394366, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7522007042253521, "term": "aligned"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.09696083922053e-06, "x": 0.30941901408450706, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.753080985915493, "term": "morphology"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 5.162571211216645e-06, "x": 0.30985915492957744, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7601232394366197, "term": "stochastic"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.3160617490322305e-08, "x": 0.3102992957746479, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.761443661971831, "term": "company"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.491760854195167e-08, "x": 0.3107394366197183, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7618838028169014, "term": "control"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.9641574086150277e-06, "x": 0.31117957746478875, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7715669014084507, "term": "symbolic"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 7.268328895412849e-07, "x": 0.31161971830985913, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7755281690140845, "term": "implemented"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.106947253303012e-06, "x": 0.31205985915492956, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7777288732394366, "term": "trafficking"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.569740933403328e-05, "x": 0.3125, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.778169014084507, "term": "hashing"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 4.725794228792577e-06, "x": 0.31294014084507044, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.7970950704225352, "term": "augmentation"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.31338028169014087, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8058978873239436, "term": "4a"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 6.73218642074336e-08, "x": 0.31382042253521125, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8076584507042254, "term": "rating"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.3142605633802817, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8085387323943662, "term": ":"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.3147007042253521, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8107394366197183, "term": "1st"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.845891132874047e-06, "x": 0.31514084507042256, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8151408450704225, "term": "humour"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.31558098591549294, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8173415492957746, "term": "semeval2017"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.1480429349594388e-07, "x": 0.3160211267605634, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8195422535211268, "term": "cancer"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 2.3834087511959602e-06, "x": 0.3164612676056338, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.8226232394366197, "term": "lid"}, {"ncat": 7, "cat": 0, "cat25k": 0, "bg": 1.0192425717237356e-05, "x": 0.31690140845070425, "s": 0.5620598591549296, "ncat25k": 2, "os": 0.4117059924935712, "y": 0.823943661971831, "term": "simplification"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.00016406386185822833, "x": 0.3173415492957746, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.0035211267605633804, "term": "hashtags"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.241983688769566e-05, "x": 0.31778169014084506, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.023767605633802816, "term": "argumentation"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.421779263728982e-07, "x": 0.3182218309859155, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.026848591549295774, "term": "define"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.132720139061786e-07, "x": 0.31866197183098594, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.028609154929577465, "term": "side"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.685849901949692e-06, "x": 0.3191021126760563, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.03169014084507042, "term": "conversations"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.858173540645858e-07, "x": 0.31954225352112675, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.0448943661971831, "term": "initial"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 9.707536203042827e-06, "x": 0.3199823943661972, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.08318661971830986, "term": "predefined"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.7413399056206665e-07, "x": 0.3204225352112676, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.10299295774647887, "term": "signal"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 8.807662666519872e-06, "x": 0.320862676056338, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.11487676056338028, "term": "unseen"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.1206231785495915e-05, "x": 0.32130281690140844, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.12411971830985916, "term": "logistic"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.157991181354864e-07, "x": 0.3217429577464789, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.12764084507042253, "term": "contribute"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.7842664439562795e-07, "x": 0.3221830985915493, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.13160211267605634, "term": "pain"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.5635342229204618e-07, "x": 0.3226232394366197, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.13204225352112675, "term": "record"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.0457203826149025e-07, "x": 0.3230633802816901, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.13380281690140844, "term": "processes"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.30418402834676e-07, "x": 0.32350352112676056, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.1368838028169014, "term": "unknown"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.0851430354163557e-05, "x": 0.323943661971831, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.1505281690140845, "term": "markov"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.1431724953814633e-06, "x": 0.32438380281690143, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.1580105633802817, "term": "spelling"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.18578578042525e-06, "x": 0.3248239436619718, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.16153169014084506, "term": "ranks"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.202628717651448e-06, "x": 0.32526408450704225, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.1619718309859155, "term": "optimized"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.259219879783076e-07, "x": 0.3257042253521127, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.1698943661971831, "term": "generic"}, {"ncat": 8, "cat": 1, "cat25k": 22, "bg": 2.8300743193238884e-06, "x": 0.3261443661971831, "s": 0.9661091549295776, "ncat25k": 2, "os": 0.838499060468824, "y": 0.8683978873239436, "term": "relying"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.357506427041654e-05, "x": 0.3265845070422535, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.1954225352112676, "term": "regularities"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.595944257285785e-07, "x": 0.32702464788732394, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.2112676056338028, "term": "variable"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.386132758808424e-07, "x": 0.3274647887323944, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.21346830985915494, "term": "computing"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.705549310768781e-08, "x": 0.3279049295774648, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.22007042253521128, "term": "links"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.6385878699893875e-07, "x": 0.3283450704225352, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.22139084507042253, "term": "disease"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.588170894335982e-06, "x": 0.3287852112676056, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.22887323943661972, "term": "rouge"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.0899197547839685e-06, "x": 0.32922535211267606, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.23503521126760563, "term": "constraint"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.122336840871128e-07, "x": 0.3296654929577465, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.23547535211267606, "term": "ideal"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.427333965046998e-06, "x": 0.3301056338028169, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.23723591549295775, "term": "markers"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.3692932564018736e-06, "x": 0.3305457746478873, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.24075704225352113, "term": "largely"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.271549460643462e-08, "x": 0.33098591549295775, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.2548415492957746, "term": "date"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.770711501396612e-05, "x": 0.3314260563380282, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.261443661971831, "term": "leverages"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.33186619718309857, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.26276408450704225, "term": "co occurrence"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.438805392338546e-06, "x": 0.332306338028169, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.28213028169014087, "term": "averaging"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.3688106233392476e-05, "x": 0.33274647887323944, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.28565140845070425, "term": "correlate"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.155152660089488e-05, "x": 0.3331866197183099, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.2909330985915493, "term": "extrinsic"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 5.448755037799717e-07, "x": 0.3336267605633803, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.292693661971831, "term": "speakers"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.490936002852095e-06, "x": 0.3340669014084507, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.3107394366197183, "term": "auxiliary"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.962849042601283e-07, "x": 0.3345070422535211, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.31954225352112675, "term": "ne"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.1790852096877769e-06, "x": 0.33494718309859156, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.3221830985915493, "term": "producing"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.4294731535624427e-06, "x": 0.335387323943662, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.32350352112676056, "term": "assessing"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.5130208207740785e-06, "x": 0.3358274647887324, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.3287852112676056, "term": "selecting"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.728515415465908e-07, "x": 0.3362676056338028, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.35255281690140844, "term": "sufficient"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.4610016340481465e-07, "x": 0.33670774647887325, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.363556338028169, "term": "association"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.379567266775282e-07, "x": 0.3371478873239437, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.3767605633802817, "term": "smaller"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.0069440263311055e-05, "x": 0.33758802816901406, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.3882042253521127, "term": "heuristics"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 5.37143164465482e-07, "x": 0.3380281690140845, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.3890845070422535, "term": "samples"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 9.538997538402066e-07, "x": 0.33846830985915494, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.39348591549295775, "term": "carefully"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.7470365728348613e-07, "x": 0.3389084507042254, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.39920774647887325, "term": "risk"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.111302270536397e-06, "x": 0.33934859154929575, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.3996478873239437, "term": "recognize"}, {"ncat": 8, "cat": 1, "cat25k": 22, "bg": 1.6918399925408655e-06, "x": 0.3397887323943662, "s": 0.9661091549295776, "ncat25k": 2, "os": 0.838499060468824, "y": 0.909330985915493, "term": "potentially"}, {"ncat": 8, "cat": 2, "cat25k": 44, "bg": 1.5226772320164205e-05, "x": 0.3402288732394366, "s": 0.9929577464788732, "ncat25k": 2, "os": 0.9763032822568858, "y": 0.9568661971830986, "term": "inducing"}, {"ncat": 8, "cat": 1, "cat25k": 22, "bg": 9.551022357351502e-06, "x": 0.34066901408450706, "s": 0.9661091549295776, "ncat25k": 2, "os": 0.838499060468824, "y": 0.9194542253521126, "term": "translating"}, {"ncat": 8, "cat": 1, "cat25k": 22, "bg": 1.3872184466760168e-06, "x": 0.34110915492957744, "s": 0.9661091549295776, "ncat25k": 2, "os": 0.838499060468824, "y": 0.9216549295774648, "term": "involves"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.0603148856877835e-06, "x": 0.3415492957746479, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.40360915492957744, "term": "hence"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.3419894366197183, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.40492957746478875, "term": "mental health"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.660344731150236e-06, "x": 0.34242957746478875, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.40889084507042256, "term": "anxiety"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.6058641258796547e-07, "x": 0.34286971830985913, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.42297535211267606, "term": "connection"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.5310679895166963e-07, "x": 0.34330985915492956, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4295774647887324, "term": "unique"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.2452984147506844e-06, "x": 0.34375, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.43133802816901406, "term": "orientation"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.34419014084507044, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.44058098591549294, "term": "predicate argument"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.07068791285219e-08, "x": 0.34463028169014087, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.44454225352112675, "term": "post"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.938238378519553e-07, "x": 0.34507042253521125, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.45598591549295775, "term": "criteria"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.268635930709368e-05, "x": 0.3455105633802817, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4586267605633803, "term": "regularization"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.3384011357614317e-06, "x": 0.3459507042253521, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4669894366197183, "term": "extending"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.748574743745223e-07, "x": 0.34639084507042256, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.46786971830985913, "term": "ways"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 8.893287657185594e-08, "x": 0.34683098591549294, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.46963028169014087, "term": "author"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.665472436188516e-05, "x": 0.3472711267605634, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4793133802816901, "term": "accuracies"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.8666658186113616e-06, "x": 0.3477112676056338, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4823943661971831, "term": "optimize"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.22942516825182e-05, "x": 0.34815140845070425, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4837147887323944, "term": "differentiable"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.4379712282268577e-07, "x": 0.3485915492957746, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4876760563380282, "term": "log"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.0720444340506945e-06, "x": 0.34903169014084506, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.4933978873239437, "term": "conceptual"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.5477025575142256e-05, "x": 0.3494718309859155, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5110035211267606, "term": "outperformed"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.8419675623506366e-07, "x": 0.34991197183098594, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5171654929577465, "term": "leading"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 8.146690614678398e-08, "x": 0.3503521126760563, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5211267605633803, "term": "description"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.3787621989153326e-06, "x": 0.35079225352112675, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5299295774647887, "term": "utilizing"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.177638699252852e-06, "x": 0.3512323943661972, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5352112676056338, "term": "segments"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.3516725352112676, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5413732394366197, "term": "a."}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 5.267446358466185e-06, "x": 0.352112676056338, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.545774647887324, "term": "constructing"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.8300730634312755e-06, "x": 0.35255281690140844, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5492957746478874, "term": "paradigm"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.106919543392685e-06, "x": 0.3529929577464789, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5585387323943662, "term": "approximation"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.6241906962296443e-05, "x": 0.3534330985915493, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5629401408450704, "term": "salient"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 5.574761382791687e-07, "x": 0.3538732394366197, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5686619718309859, "term": "presence"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.064485252521516e-06, "x": 0.3543133802816901, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5699823943661971, "term": "associations"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.0171864283697098e-05, "x": 0.35475352112676056, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5757042253521126, "term": "stylistic"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.7986713672059283e-06, "x": 0.355193661971831, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.5836267605633803, "term": "visualization"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.6561445404267916e-08, "x": 0.35563380281690143, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.585387323943662, "term": "view"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.151739667378846e-07, "x": 0.3560739436619718, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6073943661971831, "term": "formal"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.824218371587485e-07, "x": 0.35651408450704225, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6109154929577465, "term": "reliable"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 5.1519602886900954e-06, "x": 0.3569542253521127, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6139964788732394, "term": "factual"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.925369619961685e-07, "x": 0.3573943661971831, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6197183098591549, "term": "efficiency"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.074367366571181e-06, "x": 0.3578345070422535, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6399647887323944, "term": "overcome"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.138154075402036e-07, "x": 0.35827464788732394, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6408450704225352, "term": "reader"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.3259225313187046e-05, "x": 0.3587147887323944, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6412852112676056, "term": "sanskrit"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.3591549295774648, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6487676056338029, "term": "vs."}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 9.511252406049633e-06, "x": 0.3595950704225352, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6558098591549296, "term": "ambiguity"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.208456694074417e-05, "x": 0.3600352112676056, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6575704225352113, "term": "hittite"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.2594174910734848e-06, "x": 0.36047535211267606, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.659330985915493, "term": "attempts"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.4043504316710658e-06, "x": 0.3609154929577465, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6637323943661971, "term": "metadata"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 6.834725825422043e-08, "x": 0.3613556338028169, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.670774647887324, "term": "pages"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.127977076619994e-06, "x": 0.3617957746478873, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6720950704225352, "term": "behaviors"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.0118910777451724e-07, "x": 0.36223591549295775, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6786971830985915, "term": "mini"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.277871528545574e-08, "x": 0.3626760563380282, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6804577464788732, "term": "back"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.0903993185298223e-06, "x": 0.36311619718309857, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6870598591549296, "term": "observe"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.158322772691562e-05, "x": 0.363556338028169, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.690580985915493, "term": "monotonic"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.0846516370104831e-05, "x": 0.36399647887323944, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.6919014084507042, "term": "weakly"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.415502055002253e-07, "x": 0.3644366197183099, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7011443661971831, "term": "separate"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.715611435442731e-07, "x": 0.3648767605633803, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7046654929577465, "term": "attend"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.6025271853713304e-05, "x": 0.3653169014084507, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7068661971830986, "term": "fluency"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.255446119243258e-06, "x": 0.3657570422535211, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7117077464788732, "term": "characterize"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 9.212650074069706e-07, "x": 0.36619718309859156, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7134683098591549, "term": "gap"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.889168422869283e-06, "x": 0.366637323943662, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7147887323943662, "term": "pyramid"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.1268055562500278e-06, "x": 0.3670774647887324, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7183098591549296, "term": "segment"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.00011319980473033684, "x": 0.3675176056338028, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7200704225352113, "term": "metonymy"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.37037948720057e-06, "x": 0.36795774647887325, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7297535211267606, "term": "requiring"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.4521150364079727e-07, "x": 0.3683978873239437, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7310739436619719, "term": "trade"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.00016406386185822833, "x": 0.36883802816901406, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7319542253521126, "term": "sememes"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 4.624135792371679e-07, "x": 0.3692781690140845, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7345950704225352, "term": "codes"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 8.188399514878271e-07, "x": 0.36971830985915494, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7451584507042254, "term": "treat"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.216028965140266e-06, "x": 0.3701584507042254, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7455985915492958, "term": "gaze"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.899105734848282e-06, "x": 0.37059859154929575, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.75, "term": "perception"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.498303104350446e-06, "x": 0.3710387323943662, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7539612676056338, "term": "foil"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 2.111330176278915e-06, "x": 0.3714788732394366, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.758362676056338, "term": "tuning"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.5384499024051285e-06, "x": 0.37191901408450706, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7662852112676056, "term": "spectral"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.00016406386185822833, "x": 0.37235915492957744, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7790492957746479, "term": "microblog"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 1.5707670821411048e-06, "x": 0.3727992957746479, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7794894366197183, "term": "fake"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 8.829486068726512e-05, "x": 0.3732394366197183, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7860915492957746, "term": "paraphrases"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.37367957746478875, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7909330985915493, "term": "skip thought"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.1681289396797157e-07, "x": 0.37411971830985913, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.7931338028169014, "term": "instructions"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 5.506926336599859e-06, "x": 0.37455985915492956, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.8050176056338029, "term": "humorous"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.375, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.8054577464788732, "term": "\\#hashtagwars"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.00016406386185822833, "x": 0.37544014084507044, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.8098591549295775, "term": "scienceie"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 0.00016406386185822833, "x": 0.37588028169014087, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.8129401408450704, "term": "rumoureval"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 7.424992953217626e-07, "x": 0.37632042253521125, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.8182218309859155, "term": "stocks"}, {"ncat": 8, "cat": 0, "cat25k": 0, "bg": 3.770593331140105e-07, "x": 0.3767605633802817, "s": 0.5044014084507042, "ncat25k": 2, "os": 0.3980767981724979, "y": 0.820862676056338, "term": "tests"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.5070594183541955e-06, "x": 0.3772007042253521, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.014084507042253521, "term": "interpret"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.3711018052231055e-06, "x": 0.37764084507042256, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.02420774647887324, "term": "examine"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.640858441603361e-06, "x": 0.37808098591549294, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.045774647887323945, "term": "distinguish"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.8882458270325116e-08, "x": 0.3785211267605634, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.04973591549295775, "term": "just"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.864881301898832e-08, "x": 0.3789612676056338, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.05193661971830986, "term": "times"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.84935643834334e-07, "x": 0.37940140845070425, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.06382042253521127, "term": "requirements"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.394867827422271e-06, "x": 0.3798415492957746, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.06470070422535211, "term": "harassment"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 6.156700758142973e-07, "x": 0.38028169014084506, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.0664612676056338, "term": "practical"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.3807218309859155, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.0743838028169014, "term": "n grams"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.734775127037901e-06, "x": 0.38116197183098594, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.07834507042253522, "term": "introduces"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.4354031501198042e-06, "x": 0.3816021126760563, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.12852112676056338, "term": "increasingly"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 3.2128849538187947e-06, "x": 0.38204225352112675, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.863556338028169, "term": "weighted"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.475364280795972e-06, "x": 0.3824823943661972, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.16505281690140844, "term": "beneficial"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.419204501557486e-07, "x": 0.3829225352112676, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.17209507042253522, "term": "released"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.77114117455544e-06, "x": 0.383362676056338, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.17297535211267606, "term": "adapt"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 1.5657747689988997e-07, "x": 0.38380281690140844, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.8723591549295775, "term": "always"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.409553012430236e-07, "x": 0.3842429577464789, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.2055457746478873, "term": "phase"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 2.874921246922918e-07, "x": 0.3846830985915493, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.8767605633802817, "term": "length"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.5352560975851665e-07, "x": 0.3851232394366197, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.22403169014084506, "term": "established"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.552536153396795e-07, "x": 0.3855633802816901, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.22799295774647887, "term": "soft"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.5990863886432885e-07, "x": 0.38600352112676056, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.230193661971831, "term": "includes"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.7384719990110028e-05, "x": 0.386443661971831, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.23151408450704225, "term": "idf"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.665174331263676e-06, "x": 0.38688380281690143, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.23459507042253522, "term": "compression"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.517046942797247e-08, "x": 0.3873239436619718, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.25616197183098594, "term": "location"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.5173946892030053e-07, "x": 0.38776408450704225, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.2662852112676056, "term": "itself"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 3.125934248945837e-08, "x": 0.3882042253521127, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.8873239436619719, "term": "here"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.8899135594744213e-05, "x": 0.3886443661971831, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.2847711267605634, "term": "phonological"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 3.2389686799826067e-06, "x": 0.3890845070422535, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.8908450704225352, "term": "investigated"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 6.070819550183975e-07, "x": 0.38952464788732394, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.2988556338028169, "term": "contain"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.0755082687764472e-06, "x": 0.3899647887323944, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.30633802816901406, "term": "diagnosis"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.032865487346809e-07, "x": 0.3904049295774648, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.30941901408450706, "term": "enhance"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.2277173412610845e-07, "x": 0.3908450704225352, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.3226232394366197, "term": "likely"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 5.786748369381111e-07, "x": 0.3912852112676056, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.894806338028169, "term": "extension"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 6.426112958406367e-08, "x": 0.39172535211267606, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.32438380281690143, "term": "line"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.9956759649148125e-06, "x": 0.3921654929577465, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.3301056338028169, "term": "trivial"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.6972765738440455e-07, "x": 0.3926056338028169, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.3477112676056338, "term": "efforts"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 5.8833123440717945e-06, "x": 0.3930457746478873, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.3653169014084507, "term": "judgments"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.969351725155144e-06, "x": 0.39348591549295775, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.36619718309859156, "term": "occurrences"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 8.063665542376659e-07, "x": 0.3939260563380282, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.9022887323943662, "term": "conducted"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 2.3523556613091023e-07, "x": 0.39436619718309857, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.9036091549295775, "term": "published"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.531986652229998e-06, "x": 0.394806338028169, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.3772007042253521, "term": "complementary"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 5.636383733772303e-06, "x": 0.39524647887323944, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.3851232394366197, "term": "capturing"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.218080636775744e-07, "x": 0.3956866197183099, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.3908450704225352, "term": "working"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.66502307546191e-08, "x": 0.3961267605633803, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.4014084507042254, "term": "rate"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 1.5792966839112704e-06, "x": 0.3965669014084507, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.9115316901408451, "term": "induced"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 4.055731674576932e-07, "x": 0.3970070422535211, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.9132922535211268, "term": "fixed"}, {"ncat": 9, "cat": 3, "cat25k": 67, "bg": 1.038520903911286e-05, "x": 0.39744718309859156, "s": 0.9942781690140846, "ncat25k": 2, "os": 0.9931724106202222, "y": 0.9709507042253521, "term": "recursive"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 1.0984974312736067e-07, "x": 0.397887323943662, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.9168133802816901, "term": "article"}, {"ncat": 9, "cat": 1, "cat25k": 22, "bg": 2.5583435360073546e-07, "x": 0.3983274647887324, "s": 0.9590669014084507, "ncat25k": 2, "os": 0.814386488865425, "y": 0.920774647887324, "term": "engine"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.9639302390157655e-06, "x": 0.3987676056338028, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.417693661971831, "term": "advances"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.652325096798712e-06, "x": 0.39920774647887325, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.4269366197183099, "term": "exploration"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.529091784399723e-06, "x": 0.3996478873239437, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.43705985915492956, "term": "srl"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 0.00018456995201181247, "x": 0.40008802816901406, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.445862676056338, "term": "reranking"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.237430626133292e-07, "x": 0.4005281690140845, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.4511443661971831, "term": "alternative"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.5799525503726776e-07, "x": 0.40096830985915494, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.46610915492957744, "term": "appropriate"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.1767586081201585e-07, "x": 0.4014084507042254, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.47755281690140844, "term": "testing"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.1026855723333248e-06, "x": 0.40184859154929575, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.4903169014084507, "term": "structural"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.5750097059973132e-06, "x": 0.4022887323943662, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.49471830985915494, "term": "facilitate"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.3421802081287026e-06, "x": 0.4027288732394366, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5088028169014085, "term": "transcripts"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.1990543029867287e-07, "x": 0.40316901408450706, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5180457746478874, "term": "developing"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.475776895695148e-07, "x": 0.40360915492957744, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.519806338028169, "term": "lower"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.0474241352880945e-07, "x": 0.4040492957746479, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5374119718309859, "term": "left"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.4974244299804334e-05, "x": 0.4044894366197183, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5440140845070423, "term": "infer"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.1159991929897577e-07, "x": 0.40492957746478875, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5448943661971831, "term": "whose"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.313594320152361e-07, "x": 0.40536971830985913, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5453345070422535, "term": "perspective"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 9.078550341116441e-07, "x": 0.40580985915492956, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5470950704225352, "term": "flexible"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.166618760318522e-06, "x": 0.40625, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5475352112676056, "term": "exploring"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 9.206095007718798e-07, "x": 0.40669014084507044, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5519366197183099, "term": "massive"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.7304541200277293e-05, "x": 0.40713028169014087, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5558978873239436, "term": "generalize"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.591396235693997e-07, "x": 0.40757042253521125, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5563380281690141, "term": "surface"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 6.303820619601129e-07, "x": 0.4080105633802817, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5695422535211268, "term": "majority"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.205493503533166e-05, "x": 0.4084507042253521, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5862676056338029, "term": "visualizing"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.1591404030227487e-06, "x": 0.40889084507042256, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5919894366197183, "term": "assumption"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.2833875041692269e-06, "x": 0.40933098591549294, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5950704225352113, "term": "polish"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.110197960718759e-07, "x": 0.4097711267605634, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.5981514084507042, "term": "whole"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 7.124730640652216e-07, "x": 0.4102112676056338, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6025528169014085, "term": "engines"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 5.259712629171968e-07, "x": 0.41065140845070425, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6038732394366197, "term": "pattern"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.3767685825451082e-06, "x": 0.4110915492957746, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.605193661971831, "term": "toolkit"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.148490133968574e-06, "x": 0.41153169014084506, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.608274647887324, "term": "intuitive"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 5.036613381162506e-05, "x": 0.4119718309859155, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6157570422535211, "term": "lemmas"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.428938488272459e-07, "x": 0.41241197183098594, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6210387323943662, "term": "detailed"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.0633515172299423e-07, "x": 0.4128521126760563, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6342429577464789, "term": "position"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.531503328329161e-07, "x": 0.41329225352112675, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6443661971830986, "term": "decision"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 7.463203607513605e-07, "x": 0.4137323943661972, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6500880281690141, "term": "variables"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.1353559362936646e-06, "x": 0.4141725352112676, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6615316901408451, "term": "exhibit"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.267467837678733e-06, "x": 0.414612676056338, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6663732394366197, "term": "motivated"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 8.237438112813912e-07, "x": 0.41505281690140844, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6672535211267606, "term": "plays"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.098833151592507e-06, "x": 0.4154929577464789, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6817781690140845, "term": "adaptive"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.380447637888421e-06, "x": 0.4159330985915493, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.6857394366197183, "term": "independently"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.707066322899322e-07, "x": 0.4163732394366197, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.698943661971831, "term": "almost"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 9.298115241046923e-07, "x": 0.4168133802816901, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7112676056338029, "term": "dependent"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 7.363593721734539e-07, "x": 0.41725352112676056, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7143485915492958, "term": "directed"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.000086002465404e-06, "x": 0.417693661971831, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7169894366197183, "term": "demonstrates"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.3812855302129736e-06, "x": 0.41813380281690143, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7262323943661971, "term": "unlike"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.8940848781524676e-06, "x": 0.4185739436619718, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.735475352112676, "term": "intensity"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 1.158509898823469e-05, "x": 0.41901408450704225, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7398767605633803, "term": "augmented"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 6.843575631395891e-07, "x": 0.4194542253521127, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7473591549295775, "term": "instance"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.280183645990848e-06, "x": 0.4198943661971831, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7636443661971831, "term": "reward"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 6.14602462834989e-08, "x": 0.4203345070422535, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7654049295774648, "term": "must"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 5.944802312369571e-07, "x": 0.42077464788732394, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7693661971830986, "term": "sharing"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 0.00018456995201181247, "x": 0.4212147887323944, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7887323943661971, "term": "emoji"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.2042955572876147e-07, "x": 0.4216549295774648, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7913732394366197, "term": "thought"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 2.2432028150699862e-06, "x": 0.4220950704225352, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7918133802816901, "term": "geometry"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.4225352112676056, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.7962147887323944, "term": "3rd"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.42297535211267606, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.8001760563380281, "term": "pearson correlation"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 0.00018456995201181247, "x": 0.4234154929577465, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.8028169014084507, "term": "homographic"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 3.024027460588564e-07, "x": 0.4238556338028169, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.8243838028169014, "term": "ie"}, {"ncat": 9, "cat": 0, "cat25k": 0, "bg": 4.053623402216481e-08, "x": 0.4242957746478873, "s": 0.4625880281690141, "ncat25k": 2, "os": 0.38475844845933554, "y": 0.8257042253521126, "term": "email"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.917640471519883e-08, "x": 0.42473591549295775, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.01936619718309859, "term": "computer"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 9.619762995013164e-07, "x": 0.4251760563380282, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.04225352112676056, "term": "typical"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.6919957728869607e-07, "x": 0.42561619718309857, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.05501760563380282, "term": "companies"}, {"ncat": 10, "cat": 1, "cat25k": 22, "bg": 5.882634489725578e-06, "x": 0.426056338028169, "s": 0.9537852112676056, "ncat25k": 3, "os": 0.7921835489624496, "y": 0.8494718309859155, "term": "captures"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.920316153845125e-07, "x": 0.42649647887323944, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.0840669014084507, "term": "tested"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.246817576063197e-06, "x": 0.4269366197183099, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.09683098591549295, "term": "collect"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.5146514506801466e-06, "x": 0.4273767605633803, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.10211267605633803, "term": "preliminary"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.580875121221409e-06, "x": 0.4278169014084507, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.12588028169014084, "term": "explores"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.2197678249381884e-07, "x": 0.4282570422535211, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.12940140845070422, "term": "treatment"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.712499034232228e-07, "x": 0.42869718309859156, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.15184859154929578, "term": "fit"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.687650647607668e-07, "x": 0.429137323943662, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.17121478873239437, "term": "added"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.4553200681983437e-07, "x": 0.4295774647887324, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.18221830985915494, "term": "takes"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.292456991214388e-06, "x": 0.4300176056338028, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.18309859154929578, "term": "trigger"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.1617002842919884e-07, "x": 0.43045774647887325, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.19410211267605634, "term": "relationship"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.5270556049524735e-06, "x": 0.4308978873239437, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.19630281690140844, "term": "assumptions"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.471080155901758e-06, "x": 0.43133802816901406, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.19718309859154928, "term": "analogy"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.895441215785901e-07, "x": 0.4317781690140845, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.1993838028169014, "term": "core"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.801135608268648e-07, "x": 0.43221830985915494, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.20818661971830985, "term": "stage"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.847145225885132e-06, "x": 0.4326584507042254, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.21434859154929578, "term": "compute"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.995244534177242e-06, "x": 0.43309859154929575, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2169894366197183, "term": "produces"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 9.312312191615796e-08, "x": 0.4335387323943662, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.21919014084507044, "term": "personal"}, {"ncat": 10, "cat": 1, "cat25k": 22, "bg": 1.8510056555795076e-06, "x": 0.4339788732394366, "s": 0.9537852112676056, "ncat25k": 3, "os": 0.7921835489624496, "y": 0.8807218309859155, "term": "minimal"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.2592780809866065e-07, "x": 0.43441901408450706, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2420774647887324, "term": "larger"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.844642544706526e-07, "x": 0.43485915492957744, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2438380281690141, "term": "already"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 5.0622338372365266e-06, "x": 0.4352992957746479, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.24647887323943662, "term": "recognizing"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 7.203481021694492e-08, "x": 0.4357394366197183, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2482394366197183, "term": "before"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.963308628170098e-07, "x": 0.43617957746478875, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.24867957746478872, "term": "records"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.462967303368138e-07, "x": 0.43661971830985913, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2601232394366197, "term": "setup"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.5831991789719042e-07, "x": 0.43705985915492956, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2830105633802817, "term": "offer"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.7041485622183787e-06, "x": 0.4375, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.28433098591549294, "term": "commonly"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.608293884192482e-07, "x": 0.43794014084507044, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.28653169014084506, "term": "benefits"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.61663211282301e-08, "x": 0.43838028169014087, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.28829225352112675, "term": "own"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.27047626597878e-06, "x": 0.43882042253521125, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2891725352112676, "term": "fundamental"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.4142741273398281e-06, "x": 0.4392605633802817, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.2935739436619718, "term": "latter"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.927590190756713e-07, "x": 0.4397007042253521, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.29577464788732394, "term": "exact"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.4051258005616429e-06, "x": 0.44014084507042256, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.3089788732394366, "term": "intelligent"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.919112663715903e-06, "x": 0.44058098591549294, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.31161971830985913, "term": "distinct"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.316002161129377e-08, "x": 0.4410211267605634, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.3147007042253521, "term": "link"}, {"ncat": 10, "cat": 1, "cat25k": 22, "bg": 3.0052288660046174e-07, "x": 0.4414612676056338, "s": 0.9537852112676056, "ncat25k": 3, "os": 0.7921835489624496, "y": 0.8939260563380281, "term": "certain"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.12980097606223e-07, "x": 0.44190140845070425, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.3336267605633803, "term": "fact"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.6843798138725004e-06, "x": 0.4423415492957746, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.35079225352112675, "term": "summaries"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.1456689417742402e-05, "x": 0.44278169014084506, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.36223591549295775, "term": "contextual"}, {"ncat": 10, "cat": 1, "cat25k": 22, "bg": 3.517210430063708e-07, "x": 0.4432218309859155, "s": 0.9537852112676056, "ncat25k": 3, "os": 0.7921835489624496, "y": 0.9014084507042254, "term": "population"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.062853271410608e-06, "x": 0.44366197183098594, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.37411971830985913, "term": "builds"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 7.1338608347829935e-06, "x": 0.4441021126760563, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.37544014084507044, "term": "integrates"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.6409625055502144e-08, "x": 0.44454225352112675, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.3930457746478873, "term": "re"}, {"ncat": 10, "cat": 1, "cat25k": 22, "bg": 1.2399954909254875e-05, "x": 0.4449823943661972, "s": 0.9537852112676056, "ncat25k": 3, "os": 0.7921835489624496, "y": 0.9163732394366197, "term": "weighting"}, {"ncat": 10, "cat": 1, "cat25k": 22, "bg": 7.188408782327337e-07, "x": 0.4454225352112676, "s": 0.9537852112676056, "ncat25k": 3, "os": 0.7921835489624496, "y": 0.9278169014084507, "term": "leads"}, {"ncat": 10, "cat": 1, "cat25k": 22, "bg": 1.408265069284401e-06, "x": 0.445862676056338, "s": 0.9537852112676056, "ncat25k": 3, "os": 0.7921835489624496, "y": 0.9313380281690141, "term": "covering"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 7.876534688632904e-07, "x": 0.44630281690140844, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4044894366197183, "term": "tracking"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.2726610827813218e-06, "x": 0.4467429577464789, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4097711267605634, "term": "substantial"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.061935009517115e-06, "x": 0.4471830985915493, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.42561619718309857, "term": "schizophrenia"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.4765789618010115e-07, "x": 0.4476232394366197, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4282570422535211, "term": "nature"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 5.286818634153578e-07, "x": 0.4480633802816901, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4414612676056338, "term": "tags"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 5.130312502725478e-07, "x": 0.44850352112676056, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4423415492957746, "term": "competition"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 9.583087766709312e-05, "x": 0.448943661971831, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.44630281690140844, "term": "discriminative"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.729255596204856e-07, "x": 0.44938380281690143, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.45026408450704225, "term": "match"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.0446042897928685e-06, "x": 0.4498239436619718, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4524647887323944, "term": "diversity"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.638563975233023e-06, "x": 0.45026408450704225, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.457306338028169, "term": "dictionaries"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.9681914672404863e-06, "x": 0.4507042253521127, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4735915492957746, "term": "collaborative"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 6.234856507739031e-07, "x": 0.4511443661971831, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.48547535211267606, "term": "gain"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 0.00010174544307596824, "x": 0.4515845070422535, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.48723591549295775, "term": "interpretable"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.883312943331867e-06, "x": 0.45202464788732394, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.488556338028169, "term": "dimensional"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.804196432187438e-06, "x": 0.4524647887323944, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.4925176056338028, "term": "metaphor"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.0558356790262703e-07, "x": 0.4529049295774648, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.49295774647887325, "term": "every"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.578939598376187e-06, "x": 0.4533450704225352, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.49867957746478875, "term": "advantages"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.199357215860093e-06, "x": 0.4537852112676056, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.514524647887324, "term": "mixture"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.6734825748078553e-07, "x": 0.45422535211267606, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.522887323943662, "term": "expected"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 6.148240282944477e-07, "x": 0.4546654929577465, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5290492957746479, "term": "tag"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 7.306793323401526e-08, "x": 0.4551056338028169, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5369718309859155, "term": "right"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 8.31732248754481e-06, "x": 0.4555457746478873, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5444542253521126, "term": "modeled"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.545364324275541e-06, "x": 0.45598591549295775, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5501760563380281, "term": "limitations"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.5554709725006733e-06, "x": 0.4564260563380282, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5633802816901409, "term": "signals"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 5.553614875690661e-06, "x": 0.45686619718309857, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5765845070422535, "term": "incorporates"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.750958605913213e-07, "x": 0.457306338028169, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5801056338028169, "term": "necessary"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.8967671216663253e-06, "x": 0.45774647887323944, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5805457746478874, "term": "complexity"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.89888025314278e-07, "x": 0.4581866197183099, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5818661971830986, "term": "effort"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.594408460549922e-07, "x": 0.4586267605633803, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5889084507042254, "term": "release"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 7.964415945286373e-07, "x": 0.4590669014084507, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.5915492957746479, "term": "raw"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.3523066463505132e-06, "x": 0.4595070422535211, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.613556338028169, "term": "queries"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.748341689148482e-07, "x": 0.45994718309859156, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6263204225352113, "term": "cultural"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.2336468087444537e-07, "x": 0.460387323943662, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6302816901408451, "term": "action"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 7.995241232418465e-07, "x": 0.4608274647887324, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.644806338028169, "term": "extent"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.970288110706243e-07, "x": 0.4612676056338028, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6606514084507042, "term": "theory"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.69225425612522e-06, "x": 0.46170774647887325, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6619718309859155, "term": "demonstrated"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 5.794678265990045e-07, "x": 0.4621478873239437, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6725352112676056, "term": "statements"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.595261435336802e-07, "x": 0.46258802816901406, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6861795774647887, "term": "vision"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.876112048723639e-06, "x": 0.4630281690140845, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6892605633802817, "term": "quantitative"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 2.4068926666673888e-06, "x": 0.46346830985915494, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.6976232394366197, "term": "cognitive"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.5503534199311874e-06, "x": 0.4639084507042254, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.7059859154929577, "term": "evaluations"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 9.037056041631187e-07, "x": 0.46434859154929575, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.7491197183098591, "term": "estimate"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 5.892284907295208e-07, "x": 0.4647887323943662, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.7561619718309859, "term": "objects"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 1.3015011318829968e-06, "x": 0.4652288732394366, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.7574823943661971, "term": "inter"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 3.757338316565898e-06, "x": 0.46566901408450706, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.789612676056338, "term": "dense"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.46610915492957744, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.7988556338028169, "term": "2nd"}, {"ncat": 10, "cat": 0, "cat25k": 0, "bg": 4.2548800815235025e-06, "x": 0.4665492957746479, "s": 0.4234154929577465, "ncat25k": 3, "os": 0.3717501303550155, "y": 0.8006161971830986, "term": "pearson"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.301573839206988e-07, "x": 0.4669894366197183, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.02068661971830986, "term": "third"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.486392271832154e-08, "x": 0.46742957746478875, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.03609154929577465, "term": "who"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.7894984591899264e-07, "x": 0.46786971830985913, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.05897887323943662, "term": "providing"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.8276907341911774e-07, "x": 0.46830985915492956, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.06954225352112677, "term": "interest"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.664596356043365e-05, "x": 0.46875, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.09419014084507042, "term": "curated"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 5.773026938150781e-07, "x": 0.46919014084507044, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.13028169014084506, "term": "practices"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.9721769115353162e-06, "x": 0.46963028169014087, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.1324823943661972, "term": "consistently"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 4.968639864874676e-07, "x": 0.47007042253521125, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.13424295774647887, "term": "elements"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.0455891110448905e-05, "x": 0.4705105633802817, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.14568661971830985, "term": "entropy"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 7.742846851004775e-08, "x": 0.4709507042253521, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.15316901408450703, "term": "off"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.3980927432184885e-06, "x": 0.47139084507042256, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.16593309859154928, "term": "hierarchy"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.4154872911425647e-07, "x": 0.47183098591549294, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.16637323943661972, "term": "purpose"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.1498732122045252e-07, "x": 0.4722711267605634, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.16857394366197184, "term": "direct"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.8761967266266528e-05, "x": 0.4727112676056338, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.17737676056338028, "term": "negation"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.658082366712781e-05, "x": 0.47315140845070425, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.18882042253521128, "term": "analogies"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.488041004725242e-06, "x": 0.4735915492957746, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.2011443661971831, "term": "squad"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.6888580049643213e-06, "x": 0.47403169014084506, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.21654929577464788, "term": "probability"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.728711006037546e-07, "x": 0.4744718309859155, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.23415492957746478, "term": "summary"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.54938147071881e-07, "x": 0.47491197183098594, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.23767605633802816, "term": "above"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.361545537092205e-07, "x": 0.4753521126760563, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.23987676056338028, "term": "far"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.657995725837746e-06, "x": 0.47579225352112675, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.24295774647887325, "term": "roles"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.435070170910823e-07, "x": 0.4762323943661972, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.24955985915492956, "term": "maps"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 7.460773035789566e-07, "x": 0.4766725352112676, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.269806338028169, "term": "creation"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.2759220277389376e-06, "x": 0.477112676056338, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.29313380281690143, "term": "explicit"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.7480817015682205e-07, "x": 0.47755281690140844, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.2984154929577465, "term": "notes"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.70880051969601e-08, "x": 0.4779929577464789, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.29973591549295775, "term": "e"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.2407241768641033e-05, "x": 0.4784330985915493, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.3164612676056338, "term": "spans"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.6949795015342647e-06, "x": 0.4788732394366197, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.3503521126760563, "term": "studied"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.666765708728867e-07, "x": 0.4793133802816901, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.3538732394366197, "term": "effects"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.3920244591352237e-05, "x": 0.47975352112676056, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.3675176056338028, "term": "narratives"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 5.520692937302244e-06, "x": 0.480193661971831, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.37808098591549294, "term": "explored"}, {"ncat": 11, "cat": 1, "cat25k": 22, "bg": 1.4199453451203784e-06, "x": 0.48063380281690143, "s": 0.9515845070422536, "ncat25k": 3, "os": 0.7718894978098854, "y": 0.9084507042253521, "term": "derived"}, {"ncat": 11, "cat": 1, "cat25k": 22, "bg": 5.692548176924683e-07, "x": 0.4810739436619718, "s": 0.9515845070422536, "ncat25k": 3, "os": 0.7718894978098854, "y": 0.9110915492957746, "term": "settings"}, {"ncat": 11, "cat": 1, "cat25k": 22, "bg": 2.8007605465264096e-06, "x": 0.48151408450704225, "s": 0.9515845070422536, "ncat25k": 3, "os": 0.7718894978098854, "y": 0.9348591549295775, "term": "construct"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.3612506621633688e-07, "x": 0.4819542253521127, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.40669014084507044, "term": "series"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 6.756718573826241e-07, "x": 0.4823943661971831, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.4080105633802817, "term": "integration"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 7.704573858794142e-07, "x": 0.4828345070422535, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.41241197183098594, "term": "increasing"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.229685325928571e-07, "x": 0.48327464788732394, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.4185739436619718, "term": "around"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.80320518480831e-06, "x": 0.4837147887323944, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.41901408450704225, "term": "inputs"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.626577767356366e-06, "x": 0.4841549295774648, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.429137323943662, "term": "reveal"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.490116783081513e-06, "x": 0.4845950704225352, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.4326584507042254, "term": "additionally"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.0558901205228203e-06, "x": 0.4850352112676056, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.44190140845070425, "term": "artificial"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 5.439710464981753e-07, "x": 0.48547535211267606, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.4432218309859155, "term": "count"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 0.000119065659299028, "x": 0.4859154929577465, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.46258802816901406, "term": "cloze"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 8.462544755754668e-08, "x": 0.4863556338028169, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.46830985915492956, "term": "area"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.582679697849822e-06, "x": 0.4867957746478873, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.4766725352112676, "term": "outputs"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.690328265898225e-07, "x": 0.48723591549295775, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.48063380281690143, "term": "hard"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 5.622548648654935e-07, "x": 0.4876760563380282, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.4960387323943662, "term": "produced"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.2061492118417975e-06, "x": 0.48811619718309857, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.5048415492957746, "term": "enhanced"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.7882936670831222e-05, "x": 0.488556338028169, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.5092429577464789, "term": "dialogues"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.7333063315798755e-05, "x": 0.48899647887323944, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.5206866197183099, "term": "utterance"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 5.794894960943726e-06, "x": 0.4894366197183099, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.5589788732394366, "term": "parse"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.4202411425471e-06, "x": 0.4898767605633803, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.570862676056338, "term": "efficiently"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.627708232559727e-07, "x": 0.4903169014084507, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.5897887323943662, "term": "chat"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.919827815879088e-06, "x": 0.4907570422535211, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.5933098591549296, "term": "personality"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 6.9727448083160755e-06, "x": 0.49119718309859156, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.6258802816901409, "term": "relational"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 0.00019881434355118564, "x": 0.491637323943662, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.6285211267605634, "term": "framenet"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.1596810905150966e-06, "x": 0.4920774647887324, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.6355633802816901, "term": "narrative"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.34511858822326e-07, "x": 0.4925176056338028, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.6518485915492958, "term": "unit"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.958244835075231e-06, "x": 0.49295774647887325, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.6742957746478874, "term": "controversial"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 1.2300334815113666e-06, "x": 0.4933978873239437, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.695862676056338, "term": "seed"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 8.614636455293248e-07, "x": 0.49383802816901406, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.7249119718309859, "term": "usage"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 3.427454452324093e-07, "x": 0.4942781690140845, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.764524647887324, "term": "physical"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 2.400444300417787e-05, "x": 0.49471830985915494, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.7680457746478874, "term": "alignments"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 6.084217450329913e-08, "x": 0.4951584507042254, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.7900528169014085, "term": "d"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 4.6803532534911864e-07, "x": 0.49559859154929575, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.7940140845070423, "term": "ten"}, {"ncat": 11, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.4960387323943662, "s": 0.3952464788732395, "ncat25k": 3, "os": 0.3590506499811002, "y": 0.7975352112676056, "term": "9"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.575301122660939e-07, "x": 0.4964788732394366, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.011443661971830986, "term": "degree"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.154122047169887e-06, "x": 0.49691901408450706, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.013644366197183098, "term": "variation"}, {"ncat": 12, "cat": 1, "cat25k": 22, "bg": 1.1630259555633728e-06, "x": 0.49735915492957744, "s": 0.9476232394366197, "ncat25k": 3, "os": 0.7533981489284062, "y": 0.8345070422535211, "term": "dimensions"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.324232106768862e-06, "x": 0.4977992957746479, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.03477112676056338, "term": "successfully"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 4.240982522557609e-06, "x": 0.4982394366197183, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.04445422535211268, "term": "occurrence"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.1525582423737712e-07, "x": 0.49867957746478875, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.04753521126760563, "term": "others"}, {"ncat": 12, "cat": 1, "cat25k": 22, "bg": 2.3239330242502413e-06, "x": 0.49911971830985913, "s": 0.9476232394366197, "ncat25k": 3, "os": 0.7533981489284062, "y": 0.8472711267605634, "term": "methodology"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 5.191031135393794e-07, "x": 0.49955985915492956, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.0721830985915493, "term": "critical"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 4.495768638754312e-06, "x": 0.5, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.07966549295774648, "term": "grams"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 4.1681863903191507e-07, "x": 0.5004401408450704, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.0801056338028169, "term": "max"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.552132652964543e-07, "x": 0.5008802816901409, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.10167253521126761, "term": "fast"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.5978290722483113e-07, "x": 0.5013204225352113, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.1056338028169014, "term": "issues"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.3587084498764624e-05, "x": 0.5017605633802817, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.10783450704225352, "term": "categorization"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 6.257851811409517e-07, "x": 0.5022007042253521, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.12279929577464789, "term": "steps"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.0458423488206899e-07, "x": 0.5026408450704225, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.13952464788732394, "term": "run"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.0753912441376055e-06, "x": 0.503080985915493, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.1443661971830986, "term": "micro"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 4.2060251310001584e-05, "x": 0.5035211267605634, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.14920774647887325, "term": "crf"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.558180702206962e-07, "x": 0.5039612676056338, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.15492957746478872, "term": "channel"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.5044014084507042, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.18045774647887325, "term": "hand crafted"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.4379051878271138e-07, "x": 0.5048415492957746, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.1813380281690141, "term": "category"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.368213996108029e-06, "x": 0.5052816901408451, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.23371478873239437, "term": "metric"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.0197139557045011e-07, "x": 0.5057218309859155, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.24119718309859156, "term": "project"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 4.1756686398150875e-07, "x": 0.5061619718309859, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.2658450704225352, "term": "considered"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.927019291727709e-07, "x": 0.5066021126760564, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.2680457746478873, "term": "impact"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.2632074634471486e-07, "x": 0.5070422535211268, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.2966549295774648, "term": "means"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 5.319265530886747e-08, "x": 0.5074823943661971, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.3032570422535211, "term": "year"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 5.458321868962157e-07, "x": 0.5079225352112676, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.3045774647887324, "term": "documentation"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.852546962520635e-07, "x": 0.508362676056338, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.31866197183098594, "term": "practice"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 0.00017956545160711078, "x": 0.5088028169014085, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.3230633802816901, "term": "annotators"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 4.0840091574375995e-07, "x": 0.5092429577464789, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.32922535211267606, "term": "selected"}, {"ncat": 12, "cat": 1, "cat25k": 22, "bg": 8.621119624938323e-08, "x": 0.5096830985915493, "s": 0.9476232394366197, "ncat25k": 3, "os": 0.7533981489284062, "y": 0.8974471830985915, "term": "great"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 6.335165410376969e-07, "x": 0.5101232394366197, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.34507042253521125, "term": "machines"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.5535239489957373e-06, "x": 0.5105633802816901, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.3573943661971831, "term": "aspect"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.579617770673593e-06, "x": 0.5110035211267606, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.3626760563380282, "term": "extend"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.6488871302240484e-07, "x": 0.511443661971831, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.3644366197183099, "term": "weight"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.5740828598095483e-07, "x": 0.5118838028169014, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.3710387323943662, "term": "allow"}, {"ncat": 12, "cat": 1, "cat25k": 22, "bg": 5.221316127516911e-07, "x": 0.5123239436619719, "s": 0.9476232394366197, "ncat25k": 3, "os": 0.7533981489284062, "y": 0.9049295774647887, "term": "manual"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 8.85300797545356e-06, "x": 0.5127640845070423, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.397887323943662, "term": "tokens"}, {"ncat": 12, "cat": 1, "cat25k": 22, "bg": 8.432540004294109e-07, "x": 0.5132042253521126, "s": 0.9476232394366197, "ncat25k": 3, "os": 0.7533981489284062, "y": 0.9106514084507042, "term": "zero"}, {"ncat": 12, "cat": 1, "cat25k": 22, "bg": 2.9766743220624234e-05, "x": 0.5136443661971831, "s": 0.9476232394366197, "ncat25k": 3, "os": 0.7533981489284062, "y": 0.914612676056338, "term": "encoders"}, {"ncat": 12, "cat": 1, "cat25k": 22, "bg": 5.165266680732084e-06, "x": 0.5140845070422535, "s": 0.9476232394366197, "ncat25k": 3, "os": 0.7533981489284062, "y": 0.934419014084507, "term": "phenomena"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.810179590179866e-06, "x": 0.514524647887324, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.4084507042253521, "term": "emotional"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.5899343625375987e-06, "x": 0.5149647887323944, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.41065140845070425, "term": "accurately"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.058696353512299e-07, "x": 0.5154049295774648, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.426056338028169, "term": "power"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.373375638226668e-07, "x": 0.5158450704225352, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.4278169014084507, "term": "style"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.8876195154134086e-07, "x": 0.5162852112676056, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.5074823943661971, "term": "loss"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.032065057252949e-07, "x": 0.516725352112676, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.5242077464788732, "term": "version"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.077788001794517e-06, "x": 0.5171654929577465, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.5272887323943662, "term": "showed"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 6.275410509900637e-06, "x": 0.5176056338028169, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.5484154929577465, "term": "verb"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.0590760942723967e-06, "x": 0.5180457746478874, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.5620598591549296, "term": "sampling"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.8834621383880173e-07, "x": 0.5184859154929577, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.596830985915493, "term": "speed"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 5.105060116443443e-07, "x": 0.5189260563380281, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6047535211267606, "term": "implementation"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.7836479242105455e-05, "x": 0.5193661971830986, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6144366197183099, "term": "unstructured"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 9.764483108542727e-07, "x": 0.519806338028169, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6214788732394366, "term": "supporting"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 9.958576471286311e-07, "x": 0.5202464788732394, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6465669014084507, "term": "tracks"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.1508033302522175e-06, "x": 0.5206866197183099, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6483274647887324, "term": "findings"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.5338097988717427e-06, "x": 0.5211267605633803, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6584507042253521, "term": "translations"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.5230745798852617e-06, "x": 0.5215669014084507, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.664612676056338, "term": "spaces"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.5128876577648827e-07, "x": 0.5220070422535211, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6685739436619719, "term": "choice"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 4.268199652223536e-07, "x": 0.5224471830985915, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6703345070422535, "term": "positive"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 8.512922900727401e-07, "x": 0.522887323943662, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6923415492957746, "term": "distributed"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 6.608817484287537e-06, "x": 0.5233274647887324, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.6927816901408451, "term": "benchmarks"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 1.5509807368192487e-05, "x": 0.5237676056338029, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.7535211267605634, "term": "captions"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 3.4326519690807307e-07, "x": 0.5242077464788732, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.7566021126760564, "term": "object"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.192629379499681e-07, "x": 0.5246478873239436, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.7706866197183099, "term": "having"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 2.4257444255887738e-05, "x": 0.5250880281690141, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.7746478873239436, "term": "wordnet"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 0.0002460856993447968, "x": 0.5255281690140845, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.8014964788732394, "term": "tempeval"}, {"ncat": 12, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.5259683098591549, "s": 0.3683978873239437, "ncat25k": 3, "os": 0.34665845720905164, "y": 0.8089788732394366, "term": "b."}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 5.823802251629785e-07, "x": 0.5264084507042254, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.014524647887323943, "term": "difference"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.6062526873517627e-06, "x": 0.5268485915492958, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.017605633802816902, "term": "consisting"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 4.3334089513195334e-07, "x": 0.5272887323943662, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.03653169014084507, "term": "basis"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 6.074057194770975e-07, "x": 0.5277288732394366, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.04665492957746479, "term": "determine"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 7.505784352816277e-08, "x": 0.528169014084507, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.84375, "term": "after"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 4.6335073868800456e-05, "x": 0.5286091549295775, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.08274647887323944, "term": "sexism"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 4.0672172093596307e-07, "x": 0.5290492957746479, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.10123239436619719, "term": "deal"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 3.6455945184111705e-07, "x": 0.5294894366197183, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.8613556338028169, "term": "electronic"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 4.404468584094468e-07, "x": 0.5299295774647887, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.14084507042253522, "term": "maximum"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 8.693903507167487e-07, "x": 0.5303697183098591, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.15096830985915494, "term": "frequency"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.2895932404680708e-06, "x": 0.5308098591549296, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.15889084507042253, "term": "candidates"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.9836501451497847e-06, "x": 0.53125, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.16109154929577466, "term": "correction"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 1.0410739361785984e-06, "x": 0.5316901408450704, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.8648767605633803, "term": "adding"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 7.665878419769739e-08, "x": 0.5321302816901409, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.17165492957746478, "term": "review"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 7.093047386767205e-07, "x": 0.5325704225352113, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.17341549295774647, "term": "ones"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 6.989240172281545e-07, "x": 0.5330105633802817, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.17825704225352113, "term": "finding"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 5.491303781248423e-06, "x": 0.5334507042253521, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.1817781690140845, "term": "crafted"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.312636157602973e-06, "x": 0.5338908450704225, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.21214788732394366, "term": "mapping"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 5.595847691614899e-08, "x": 0.534330985915493, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.2557218309859155, "term": "name"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 2.2697458504462716e-07, "x": 0.5347711267605634, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.2566021126760563, "term": "co"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 5.313224603777515e-07, "x": 0.5352112676056338, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.2579225352112676, "term": "classes"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 8.752581717048491e-07, "x": 0.5356514084507042, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.28169014084507044, "term": "helps"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 2.4592533873188634e-07, "x": 0.5360915492957746, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.28741197183098594, "term": "past"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 6.227180974333477e-06, "x": 0.5365316901408451, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.2887323943661972, "term": "linguistics"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 1.1045611075749817e-06, "x": 0.5369718309859155, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.8890845070422535, "term": "attempt"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 6.003842459173871e-06, "x": 0.5374119718309859, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.301056338028169, "term": "qualitative"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 9.433337718712356e-07, "x": 0.5378521126760564, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.8943661971830986, "term": "amounts"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.265600349695112e-05, "x": 0.5382922535211268, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.34286971830985913, "term": "gaussian"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 7.060638720586194e-07, "x": 0.5387323943661971, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.3683978873239437, "term": "suggest"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 2.121828020126355e-06, "x": 0.5391725352112676, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.36971830985915494, "term": "embedded"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 6.026997611269039e-07, "x": 0.539612676056338, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.37191901408450706, "term": "successful"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 6.826834353323653e-07, "x": 0.5400528169014085, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.3727992957746479, "term": "essential"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.021072702509649e-07, "x": 0.5404929577464789, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.37588028169014087, "term": "engineering"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.0133112857583425e-06, "x": 0.5409330985915493, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.3816021126760563, "term": "remains"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.5239018810181636e-06, "x": 0.5413732394366197, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.383362676056338, "term": "incorporate"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.545849969552701e-06, "x": 0.5418133802816901, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.3904049295774648, "term": "represented"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 5.562996657922776e-05, "x": 0.5422535211267606, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.3983274647887324, "term": "semantically"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 9.215505113683788e-07, "x": 0.542693661971831, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.9088908450704225, "term": "mt"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 4.182755961136819e-06, "x": 0.5431338028169014, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.9154929577464789, "term": "phrases"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 1.729603648969527e-05, "x": 0.5435739436619719, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.9203345070422535, "term": "obtains"}, {"ncat": 13, "cat": 1, "cat25k": 22, "bg": 9.072284968236728e-09, "x": 0.5440140845070423, "s": 0.9432218309859156, "ncat25k": 4, "os": 0.7365604905227576, "y": 0.9273767605633803, "term": "i"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.43164080352492e-05, "x": 0.5444542253521126, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.41329225352112675, "term": "modal"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.8936690274407206e-05, "x": 0.5448943661971831, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.4242957746478873, "term": "affective"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.1381795999496926e-06, "x": 0.5453345070422535, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.4357394366197183, "term": "predicted"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 0.0001516892936529699, "x": 0.545774647887324, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.4590669014084507, "term": "diachronic"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 8.275326477543629e-07, "x": 0.5462147887323944, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.4810739436619718, "term": "decisions"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.7022045513021538e-05, "x": 0.5466549295774648, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.5008802816901409, "term": "probabilistic"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.7864930549892e-06, "x": 0.5470950704225352, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.5704225352112676, "term": "meaningful"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 4.361638330668149e-07, "x": 0.5475352112676056, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.5924295774647887, "term": "assessment"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 5.23677989518021e-07, "x": 0.547975352112676, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.5928697183098591, "term": "agents"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.292063756786689e-06, "x": 0.5484154929577465, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.5941901408450704, "term": "preferences"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 4.2059289459746326e-07, "x": 0.5488556338028169, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.6003521126760564, "term": "understand"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 8.478535509605132e-08, "x": 0.5492957746478874, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.6100352112676056, "term": "life"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.5659576674193128e-06, "x": 0.5497359154929577, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.65625, "term": "uncertainty"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.3997465096698598e-06, "x": 0.5501760563380281, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.672975352112676, "term": "simultaneously"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 3.461156505509895e-05, "x": 0.5506161971830986, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.6914612676056338, "term": "ner"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 2.1001722383563016e-06, "x": 0.551056338028169, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.6949823943661971, "term": "difficulty"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 7.113396250137601e-08, "x": 0.5514964788732394, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.7108274647887324, "term": "video"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 0.00017553216626946887, "x": 0.5519366197183099, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.7240316901408451, "term": "compositionality"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 2.9647975898476205e-05, "x": 0.5523767605633803, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.7552816901408451, "term": "asr"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.8233361216350332e-07, "x": 0.5528169014084507, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.7627640845070423, "term": "programs"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 1.6342560175192247e-05, "x": 0.5532570422535211, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.7671654929577465, "term": "grounding"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 0.0001875225387666787, "x": 0.5536971830985915, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.7992957746478874, "term": "cqa"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.554137323943662, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.801056338028169, "term": "12"}, {"ncat": 13, "cat": 0, "cat25k": 0, "bg": 5.1552319656106364e-05, "x": 0.5545774647887324, "s": 0.34375000000000006, "ncat25k": 4, "os": 0.33457166964763263, "y": 0.8125, "term": "veracity"}, {"ncat": 14, "cat": 1, "cat25k": 22, "bg": 3.6267371768849183e-06, "x": 0.5550176056338029, "s": 0.9397007042253521, "ncat25k": 4, "os": 0.7212165519646896, "y": 0.8314260563380281, "term": "publicly"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 8.533146091727663e-06, "x": 0.5554577464788732, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.009683098591549295, "term": "abusive"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.4333549699146074e-06, "x": 0.5558978873239436, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.028169014084507043, "term": "crowd"}, {"ncat": 14, "cat": 2, "cat25k": 44, "bg": 5.792540558147125e-07, "x": 0.5563380281690141, "s": 0.9810739436619719, "ncat25k": 4, "os": 0.8962263213087566, "y": 0.9423415492957746, "term": "contains"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 3.795968792310224e-07, "x": 0.5567781690140845, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.050616197183098594, "term": "rather"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 3.6541308748897625e-07, "x": 0.5572183098591549, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.05985915492957746, "term": "needed"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.067999937445718e-05, "x": 0.5576584507042254, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.07306338028169014, "term": "similarities"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.5733157556503529e-06, "x": 0.5580985915492958, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.1210387323943662, "term": "considering"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 3.778773436593895e-07, "x": 0.5585387323943662, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.14656690140845072, "term": "solution"}, {"ncat": 14, "cat": 1, "cat25k": 22, "bg": 5.597025367584641e-05, "x": 0.5589788732394366, "s": 0.9397007042253521, "ncat25k": 4, "os": 0.7212165519646896, "y": 0.8644366197183099, "term": "cosine"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.7356854039947843e-06, "x": 0.559419014084507, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.17693661971830985, "term": "gains"}, {"ncat": 14, "cat": 1, "cat25k": 22, "bg": 2.568770261175435e-06, "x": 0.5598591549295775, "s": 0.9397007042253521, "ncat25k": 4, "os": 0.7212165519646896, "y": 0.8798415492957746, "term": "constructed"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 7.72897082984276e-07, "x": 0.5602992957746479, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.25132042253521125, "term": "procedure"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 7.542484525650184e-07, "x": 0.5607394366197183, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.2632042253521127, "term": "examples"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.150845844979996e-06, "x": 0.5611795774647887, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.28125, "term": "indicate"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.765647983879856e-07, "x": 0.5616197183098591, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.2860915492957746, "term": "patients"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 9.637962668246558e-07, "x": 0.5620598591549296, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.2953345070422535, "term": "reduction"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 8.403459956567918e-07, "x": 0.5625, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.30061619718309857, "term": "identified"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.3546005366790546e-07, "x": 0.5629401408450704, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3028169014084507, "term": "million"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.2782867509814868e-07, "x": 0.5633802816901409, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3050176056338028, "term": "per"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 8.81088264842042e-07, "x": 0.5638204225352113, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3102992957746479, "term": "concept"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 3.5822534651585553e-06, "x": 0.5642605633802817, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.31338028169014087, "term": "unified"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.122108486730907e-06, "x": 0.5647007042253521, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3494718309859155, "term": "paragraph"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.537120561240486e-07, "x": 0.5651408450704225, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3560739436619718, "term": "value"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.9259833589534976e-06, "x": 0.565580985915493, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3578345070422535, "term": "applying"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.84465442560912e-06, "x": 0.5660211267605634, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3732394366197183, "term": "predictions"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.6918224136689053e-07, "x": 0.5664612676056338, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.37455985915492956, "term": "upon"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 6.659243829544667e-07, "x": 0.5669014084507042, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.375, "term": "easily"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.2561562873762574e-05, "x": 0.5673415492957746, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.3829225352112676, "term": "leveraging"}, {"ncat": 14, "cat": 1, "cat25k": 22, "bg": 5.299432229429804e-07, "x": 0.5677816901408451, "s": 0.9397007042253521, "ncat25k": 4, "os": 0.7212165519646896, "y": 0.9330985915492958, "term": "units"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 8.471617358828061e-06, "x": 0.5682218309859155, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.4300176056338028, "term": "coherent"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.0636495487466789e-05, "x": 0.5686619718309859, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.44366197183098594, "term": "predictive"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.5609710845716294e-06, "x": 0.5691021126760564, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.448943661971831, "term": "expensive"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 3.296531677815125e-07, "x": 0.5695422535211268, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.4951584507042254, "term": "basic"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.399340727368141e-06, "x": 0.5699823943661971, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5, "term": "induction"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.529794428001282e-06, "x": 0.5704225352112676, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5017605633802817, "term": "logical"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.092193136082609e-07, "x": 0.570862676056338, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.503080985915493, "term": "lead"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 3.242497018234483e-07, "x": 0.5713028169014085, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5096830985915493, "term": "cases"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.7232075793266762e-07, "x": 0.5717429577464789, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.511443661971831, "term": "market"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.5873635160754968e-07, "x": 0.5721830985915493, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5140845070422535, "term": "stock"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 7.448590823713856e-07, "x": 0.5726232394366197, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.53125, "term": "universal"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 6.643020676401856e-05, "x": 0.5730633802816901, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5400528169014085, "term": "linguistically"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 6.359364027258051e-06, "x": 0.5735035211267606, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5528169014084507, "term": "senses"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.347949120948813e-06, "x": 0.573943661971831, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5721830985915493, "term": "beam"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.6348598927367354e-06, "x": 0.5743838028169014, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5748239436619719, "term": "constraints"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 7.215276802498547e-06, "x": 0.5748239436619719, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5774647887323944, "term": "propagation"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.837187189469858e-06, "x": 0.5752640845070423, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.5946302816901409, "term": "computation"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 7.74783412064241e-07, "x": 0.5757042253521126, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.6122359154929577, "term": "facts"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 5.947226057652027e-07, "x": 0.5761443661971831, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.6496478873239436, "term": "interesting"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 2.7907319790974176e-06, "x": 0.5765845070422535, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.6536091549295775, "term": "solving"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 3.9180640453745394e-07, "x": 0.577024647887324, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.6641725352112676, "term": "six"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.9550479816762324e-06, "x": 0.5774647887323944, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.6681338028169014, "term": "demographic"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.5779049295774648, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.6852992957746479, "term": "$"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 6.404525327804764e-07, "x": 0.5783450704225352, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.6883802816901409, "term": "coverage"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.3745105404782604e-07, "x": 0.5787852112676056, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.7081866197183099, "term": "account"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.229741426842846e-05, "x": 0.579225352112676, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.7275528169014085, "term": "kbs"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 4.6198366377256615e-08, "x": 0.5796654929577465, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.7825704225352113, "term": "get"}, {"ncat": 14, "cat": 0, "cat25k": 0, "bg": 1.9764952362935346e-05, "x": 0.5801056338028169, "s": 0.3204225352112677, "ncat25k": 4, "os": 0.32278809592098867, "y": 0.8133802816901409, "term": "rumours"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 3.418452613495333e-07, "x": 0.5805457746478874, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.01584507042253521, "term": "wide"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 4.6528880382995963e-07, "x": 0.5809859154929577, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.020246478873239437, "term": "communication"}, {"ncat": 15, "cat": 1, "cat25k": 22, "bg": 7.3893036135542e-07, "x": 0.5814260563380281, "s": 0.9357394366197184, "ncat25k": 4, "os": 0.707210944306037, "y": 0.8371478873239436, "term": "discuss"}, {"ncat": 15, "cat": 2, "cat25k": 44, "bg": 3.6955146766658893e-07, "x": 0.5818661971830986, "s": 0.9731514084507042, "ncat25k": 4, "os": 0.8825107285126836, "y": 0.9441021126760564, "term": "though"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 4.712599451949817e-07, "x": 0.582306338028169, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.06338028169014084, "term": "forms"}, {"ncat": 15, "cat": 2, "cat25k": 44, "bg": 3.825916177799912e-07, "x": 0.5827464788732394, "s": 0.9731514084507042, "ncat25k": 4, "os": 0.8825107285126836, "y": 0.9445422535211268, "term": "construction"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 3.263446268880125e-06, "x": 0.5831866197183099, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.09154929577464789, "term": "focuses"}, {"ncat": 15, "cat": 2, "cat25k": 44, "bg": 1.1584694165777654e-06, "x": 0.5836267605633803, "s": 0.9731514084507042, "ncat25k": 4, "os": 0.8825107285126836, "y": 0.948943661971831, "term": "dynamic"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 3.425529558323637e-05, "x": 0.5840669014084507, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.12455985915492958, "term": "classifying"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.032745964803329e-06, "x": 0.5845070422535211, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.12984154929577466, "term": "aim"}, {"ncat": 15, "cat": 1, "cat25k": 22, "bg": 1.0708988947955285e-06, "x": 0.5849471830985915, "s": 0.9357394366197184, "ncat25k": 4, "os": 0.707210944306037, "y": 0.8626760563380281, "term": "introduced"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 4.523499807675867e-06, "x": 0.585387323943662, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.14788732394366197, "term": "namely"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 6.270533777306633e-07, "x": 0.5858274647887324, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.15404929577464788, "term": "showing"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.6422446188502146e-07, "x": 0.5862676056338029, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.15625, "term": "include"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.129963846430079e-06, "x": 0.5867077464788732, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.1800176056338028, "term": "importance"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 8.445232666159953e-05, "x": 0.5871478873239436, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.20378521126760563, "term": "extractive"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 8.391814489877542e-07, "x": 0.5875880281690141, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.20774647887323944, "term": "extended"}, {"ncat": 15, "cat": 1, "cat25k": 22, "bg": 4.4079976505372515e-06, "x": 0.5880281690140845, "s": 0.9357394366197184, "ncat25k": 4, "os": 0.707210944306037, "y": 0.875, "term": "robust"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 3.38177538698783e-06, "x": 0.5884683098591549, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.24559859154929578, "term": "achieving"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.4135889887243156e-07, "x": 0.5889084507042254, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.2706866197183099, "term": "making"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.6022213281945892e-07, "x": 0.5893485915492958, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.272887323943662, "term": "little"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 7.51347203102523e-07, "x": 0.5897887323943662, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.28257042253521125, "term": "reduce"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.7424216419919436e-06, "x": 0.5902288732394366, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.2970950704225352, "term": "commands"}, {"ncat": 15, "cat": 1, "cat25k": 22, "bg": 7.72784649792109e-06, "x": 0.590669014084507, "s": 0.9357394366197184, "ncat25k": 4, "os": 0.707210944306037, "y": 0.8926056338028169, "term": "leverage"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.2884831323536594e-05, "x": 0.5911091549295775, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.3362676056338028, "term": "normalization"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.8206153740651521e-06, "x": 0.5915492957746479, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.3371478873239437, "term": "capable"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 5.2757641944435654e-05, "x": 0.5919894366197183, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.3397887323943662, "term": "multimodal"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 6.279788175209104e-07, "x": 0.5924295774647887, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.3419894366197183, "term": "progress"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 4.91545821491858e-06, "x": 0.5928697183098591, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.3648767605633803, "term": "integrating"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 9.290169143012866e-05, "x": 0.5933098591549296, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.3657570422535211, "term": "relatedness"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 8.662105971626982e-06, "x": 0.59375, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.3701584507042254, "term": "relies"}, {"ncat": 15, "cat": 1, "cat25k": 22, "bg": 1.436607515998308e-07, "x": 0.5941901408450704, "s": 0.9357394366197184, "ncat25k": 4, "os": 0.707210944306037, "y": 0.9066901408450704, "term": "total"}, {"ncat": 15, "cat": 1, "cat25k": 22, "bg": 1.6604795641407193e-06, "x": 0.5946302816901409, "s": 0.9357394366197184, "ncat25k": 4, "os": 0.707210944306037, "y": 0.9102112676056338, "term": "acquisition"}, {"ncat": 15, "cat": 2, "cat25k": 44, "bg": 0.00011300223677956919, "x": 0.5950704225352113, "s": 0.9731514084507042, "ncat25k": 4, "os": 0.8825107285126836, "y": 0.957306338028169, "term": "lexicons"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.7208708577111184e-07, "x": 0.5955105633802817, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.4110915492957746, "term": "science"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 5.077505326176149e-07, "x": 0.5959507042253521, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.4154929577464789, "term": "success"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 0.0003075976622577669, "x": 0.5963908450704225, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.4441021126760563, "term": "abstractive"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.2158754624809074e-07, "x": 0.596830985915493, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.4537852112676056, "term": "field"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 4.450564002915594e-07, "x": 0.5972711267605634, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.4595070422535211, "term": "ideas"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.0220252470072344e-05, "x": 0.5977112676056338, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.4652288732394366, "term": "sentiments"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 5.207570279632637e-06, "x": 0.5981514084507042, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.47403169014084506, "term": "yields"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 8.097694910166872e-06, "x": 0.5985915492957746, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.48327464788732394, "term": "gradient"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.5990316901408451, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.5154049295774648, "term": "code switching"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.9959982451439614e-06, "x": 0.5994718309859155, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.5162852112676056, "term": "switching"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.9780825040035355e-06, "x": 0.5999119718309859, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.539612676056338, "term": "analyses"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 7.46035592970202e-08, "x": 0.6003521126760564, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.542693661971831, "term": "should"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 7.918226887289583e-06, "x": 0.6007922535211268, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.5431338028169014, "term": "downstream"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.4199193864433669e-05, "x": 0.6012323943661971, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.5691021126760564, "term": "causal"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 6.327408860899151e-07, "x": 0.6016725352112676, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.5809859154929577, "term": "beyond"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 6.406535965075667e-07, "x": 0.602112676056338, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.5845070422535211, "term": "presented"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 5.244646317485575e-07, "x": 0.6025528169014085, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.5911091549295775, "term": "agent"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.88936138294544e-07, "x": 0.6029929577464789, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.6117957746478874, "term": "database"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 1.8788918896880025e-06, "x": 0.6034330985915493, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.6245598591549296, "term": "pipeline"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 6.276012765409966e-07, "x": 0.6038732394366197, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.6320422535211268, "term": "benefit"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 2.083673956145331e-06, "x": 0.6043133802816901, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.6716549295774648, "term": "attributes"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 3.706142436936898e-05, "x": 0.6047535211267606, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.6782570422535211, "term": "empirically"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 3.1598974465816705e-06, "x": 0.605193661971831, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.6835387323943662, "term": "dialog"}, {"ncat": 15, "cat": 0, "cat25k": 0, "bg": 8.940574682259427e-06, "x": 0.6056338028169014, "s": 0.29929577464788737, "ncat25k": 4, "os": 0.31130525818213445, "y": 0.7684859154929577, "term": "stance"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.5187558636731513e-07, "x": 0.6060739436619719, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.02948943661971831, "term": "change"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 0.00015127448755767342, "x": 0.6065140845070423, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.11443661971830986, "term": "outperforming"}, {"ncat": 16, "cat": 1, "cat25k": 22, "bg": 3.4073230186316437e-06, "x": 0.6069542253521126, "s": 0.9326584507042254, "ncat25k": 4, "os": 0.6943998255699988, "y": 0.8600352112676056, "term": "crucial"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.6073943661971831, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.1430457746478873, "term": "random fields"}, {"ncat": 16, "cat": 1, "cat25k": 22, "bg": 1.097573960287637e-07, "x": 0.6078345070422535, "s": 0.9326584507042254, "ncat25k": 4, "os": 0.6943998255699988, "y": 0.8710387323943662, "term": "map"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 9.46916903662213e-08, "x": 0.608274647887324, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.1936619718309859, "term": "years"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.1506865058535636e-06, "x": 0.6087147887323944, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.22843309859154928, "term": "experiment"}, {"ncat": 16, "cat": 1, "cat25k": 22, "bg": 3.4639309328033875e-07, "x": 0.6091549295774648, "s": 0.9326584507042254, "ncat25k": 4, "os": 0.6943998255699988, "y": 0.8838028169014085, "term": "written"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.791992460922357e-07, "x": 0.6095950704225352, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.25264084507042256, "term": "reports"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 0.00032810080897355715, "x": 0.6100352112676056, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.25308098591549294, "term": "lstms"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.430213015765247e-06, "x": 0.610475352112676, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.2539612676056338, "term": "bi"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 3.383561564605814e-07, "x": 0.6109154929577465, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.25528169014084506, "term": "usually"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 5.997976170378061e-07, "x": 0.6113556338028169, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.2904929577464789, "term": "protein"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.5559461088340545e-06, "x": 0.6117957746478874, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.3274647887323944, "term": "contribution"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 3.1036057303424407e-06, "x": 0.6122359154929577, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.3279049295774648, "term": "ranging"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.9854229009721314e-06, "x": 0.6126760563380281, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.34815140845070425, "term": "participating"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 6.561886732730375e-07, "x": 0.6131161971830986, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.34903169014084506, "term": "requires"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.613556338028169, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.3516725352112676, "term": "skip gram"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 4.7733700843051735e-06, "x": 0.6139964788732394, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.36399647887323944, "term": "combines"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.4711552655888097e-06, "x": 0.6144366197183099, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.37632042253521125, "term": "extensive"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.2663712618253848e-06, "x": 0.6148767605633803, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.38688380281690143, "term": "noise"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.598091555609755e-06, "x": 0.6153169014084507, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.3965669014084507, "term": "representing"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.892880283952391e-07, "x": 0.6157570422535211, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.40184859154929575, "term": "conditions"}, {"ncat": 16, "cat": 1, "cat25k": 22, "bg": 6.702422777986681e-07, "x": 0.6161971830985915, "s": 0.9326584507042254, "ncat25k": 4, "os": 0.6943998255699988, "y": 0.9141725352112676, "term": "programming"}, {"ncat": 16, "cat": 4, "cat25k": 89, "bg": 3.717634057163809e-07, "x": 0.616637323943662, "s": 0.9933978873239437, "ncat25k": 4, "os": 0.9765651892005693, "y": 0.980193661971831, "term": "gold"}, {"ncat": 16, "cat": 1, "cat25k": 22, "bg": 0.0, "x": 0.6170774647887324, "s": 0.9326584507042254, "ncat25k": 4, "os": 0.6943998255699988, "y": 0.9295774647887324, "term": "+"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.0479518977058266e-06, "x": 0.6175176056338029, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.40536971830985913, "term": "depression"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.8318657498819887e-07, "x": 0.6179577464788732, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.40580985915492956, "term": "self"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 8.945593765323697e-07, "x": 0.6183978873239436, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.40757042253521125, "term": "mental"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.5277603849154096e-06, "x": 0.6188380281690141, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.4102112676056338, "term": "collected"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 4.2276393548833725e-07, "x": 0.6192781690140845, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.4168133802816901, "term": "effect"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.843852434645803e-06, "x": 0.6197183098591549, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.4225352112676056, "term": "descriptions"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 6.869904194892312e-07, "x": 0.6201584507042254, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.44014084507042256, "term": "spanish"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 6.175102757569325e-05, "x": 0.6205985915492958, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.4454225352112676, "term": "unlabeled"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.7725274529882789e-06, "x": 0.6210387323943662, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.44850352112676056, "term": "contrast"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 4.832639648183833e-05, "x": 0.6214788732394366, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.5052816901408451, "term": "pairwise"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.0647829945637505e-06, "x": 0.621919014084507, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.5184859154929577, "term": "conduct"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.85722792481477e-05, "x": 0.6223591549295775, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.5250880281690141, "term": "coherence"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 0.00020659956484966652, "x": 0.6227992957746479, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.5264084507042254, "term": "treebank"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.8098371168155975e-07, "x": 0.6232394366197183, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.5647007042253521, "term": "images"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.2049006623414248e-06, "x": 0.6236795774647887, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.5814260563380281, "term": "influence"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.34867884246244e-06, "x": 0.6241197183098591, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.5893485915492958, "term": "conversation"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 3.1135552519839186e-05, "x": 0.6245598591549296, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.590669014084507, "term": "conversational"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 4.905202666419118e-07, "x": 0.625, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.6007922535211268, "term": "primary"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.467321052115965e-07, "x": 0.6254401408450704, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.6056338028169014, "term": "access"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.2961962311474335e-05, "x": 0.6258802816901409, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.6254401408450704, "term": "chunk"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 3.4992908468393204e-06, "x": 0.6263204225352113, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.6360035211267606, "term": "span"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 1.090419492556933e-06, "x": 0.6267605633802817, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.6624119718309859, "term": "gender"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 4.40134851816911e-06, "x": 0.6272007042253521, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.702024647887324, "term": "distributions"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 2.9795047318259518e-05, "x": 0.6276408450704225, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.7966549295774648, "term": "quantification"}, {"ncat": 16, "cat": 0, "cat25k": 0, "bg": 3.08284569779249e-05, "x": 0.628080985915493, "s": 0.2794894366197183, "ncat25k": 4, "os": 0.3001204138178768, "y": 0.8111795774647887, "term": "rumour"}, {"ncat": 17, "cat": 1, "cat25k": 22, "bg": 1.1257226435795128e-06, "x": 0.6285211267605634, "s": 0.9278169014084507, "ncat25k": 5, "os": 0.6826534245192362, "y": 0.8424295774647887, "term": "lack"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.706014042191328e-07, "x": 0.6289612676056338, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.03961267605633803, "term": "easy"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 9.03844502591774e-06, "x": 0.6294014084507042, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.04841549295774648, "term": "meanings"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.514231362942046e-07, "x": 0.6298415492957746, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.05149647887323944, "term": "few"}, {"ncat": 17, "cat": 1, "cat25k": 22, "bg": 1.9390971909284702e-07, "x": 0.6302816901408451, "s": 0.9278169014084507, "ncat25k": 5, "os": 0.6826534245192362, "y": 0.8459507042253521, "term": "tools"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.771216876541245e-07, "x": 0.6307218309859155, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.05721830985915493, "term": "major"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.2860791208890808e-07, "x": 0.6311619718309859, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.06073943661971831, "term": "comment"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 5.519442349311026e-07, "x": 0.6316021126760564, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.0761443661971831, "term": "built"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 1.1404744903865214e-06, "x": 0.6320422535211268, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.08098591549295775, "term": "performed"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 9.199533194039774e-07, "x": 0.6324823943661971, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.10255281690140845, "term": "strategies"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 1.05505680503416e-06, "x": 0.6329225352112676, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.1113556338028169, "term": "behavior"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 6.979460924502678e-07, "x": 0.633362676056338, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.11883802816901408, "term": "fully"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 5.088408859039551e-06, "x": 0.6338028169014085, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.1289612676056338, "term": "schema"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.6342429577464789, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.1487676056338028, "term": "conditional random"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 1.0284020135990437e-06, "x": 0.6346830985915493, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.18265845070422534, "term": "advantage"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.803323983698176e-06, "x": 0.6351232394366197, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.1857394366197183, "term": "interactions"}, {"ncat": 17, "cat": 1, "cat25k": 22, "bg": 0.0, "x": 0.6355633802816901, "s": 0.9278169014084507, "ncat25k": 5, "os": 0.6826534245192362, "y": 0.8692781690140845, "term": "f1-score"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 9.724816301080313e-05, "x": 0.6360035211267606, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.20290492957746478, "term": "factoid"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 8.726599687105202e-07, "x": 0.636443661971831, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.2249119718309859, "term": "creating"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 4.14227026844214e-07, "x": 0.6368838028169014, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.22579225352112675, "term": "increase"}, {"ncat": 17, "cat": 1, "cat25k": 22, "bg": 8.095356098973823e-06, "x": 0.6373239436619719, "s": 0.9278169014084507, "ncat25k": 5, "os": 0.6826534245192362, "y": 0.883362676056338, "term": "analyzing"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 8.536163670795416e-07, "x": 0.6377640845070423, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.25044014084507044, "term": "enable"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 6.649624721583635e-07, "x": 0.6382042253521126, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.25088028169014087, "term": "defined"}, {"ncat": 17, "cat": 1, "cat25k": 22, "bg": 4.1621822525034336e-07, "x": 0.6386443661971831, "s": 0.9278169014084507, "ncat25k": 5, "os": 0.6826534245192362, "y": 0.8912852112676056, "term": "known"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 5.426513334467474e-07, "x": 0.6390845070422535, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.3160211267605634, "term": "sources"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 5.15674995591358e-07, "x": 0.639524647887324, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.33186619718309857, "term": "potential"}, {"ncat": 17, "cat": 1, "cat25k": 22, "bg": 5.583556921183285e-06, "x": 0.6399647887323944, "s": 0.9278169014084507, "ncat25k": 5, "os": 0.6826534245192362, "y": 0.9000880281690141, "term": "scenarios"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 1.8489306220324662e-06, "x": 0.6404049295774648, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.36883802816901406, "term": "mention"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 3.1495887933925334e-07, "x": 0.6408450704225352, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.3987676056338028, "term": "close"}, {"ncat": 17, "cat": 1, "cat25k": 22, "bg": 1.118218990746045e-07, "x": 0.6412852112676056, "s": 0.9278169014084507, "ncat25k": 5, "os": 0.6826534245192362, "y": 0.9234154929577465, "term": "group"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.941120523148526e-07, "x": 0.641725352112676, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.4137323943661972, "term": "stories"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.410798163510684e-06, "x": 0.6421654929577465, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.41813380281690143, "term": "enables"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 5.205457732586511e-07, "x": 0.6426056338028169, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.4234154929577465, "term": "associated"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 1.7896159115803415e-05, "x": 0.6430457746478874, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.43485915492957744, "term": "predicate"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.9412784993666303e-06, "x": 0.6434859154929577, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.4507042253521127, "term": "furthermore"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.6439260563380281, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.477112676056338, "term": "coreference resolution"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 7.129967330279988e-07, "x": 0.6443661971830986, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.4942781690140845, "term": "sub"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.1032547124349737e-06, "x": 0.644806338028169, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.5044014084507042, "term": "optimization"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 9.061971357240768e-06, "x": 0.6452464788732394, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.5902288732394366, "term": "implicit"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 1.3381174969488953e-06, "x": 0.6456866197183099, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.5955105633802817, "term": "poetry"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.9237842539599003e-06, "x": 0.6461267605633803, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.6901408450704225, "term": "inspired"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 2.2315348693731684e-05, "x": 0.6465669014084507, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.7077464788732394, "term": "gated"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 1.736949386086734e-07, "x": 0.6470070422535211, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.7222711267605634, "term": "k"}, {"ncat": 17, "cat": 0, "cat25k": 0, "bg": 9.643155594471045e-07, "x": 0.6474471830985915, "s": 0.26320422535211274, "ncat25k": 5, "os": 0.28923057631161475, "y": 0.7231514084507042, "term": "native"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 4.551766328096172e-07, "x": 0.647887323943662, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.047975352112676055, "term": "overall"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 2.806674615273128e-06, "x": 0.6483274647887324, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.0528169014084507, "term": "humans"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 8.303351124108829e-07, "x": 0.6487676056338029, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.07394366197183098, "term": "actions"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 1.8865706720116708e-06, "x": 0.6492077464788732, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.15977112676056338, "term": "sensitive"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 1.8943478028880596e-06, "x": 0.6496478873239436, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.19058098591549297, "term": "completion"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 4.368448540646958e-06, "x": 0.6500880281690141, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.20950704225352113, "term": "relevance"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 6.360718913894524e-07, "x": 0.6505281690140845, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.21390845070422534, "term": "instead"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 2.8745083093651243e-06, "x": 0.6509683098591549, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.26452464788732394, "term": "conventional"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.6514084507042254, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.27420774647887325, "term": "distant supervision"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 0.00036910584109993536, "x": 0.6518485915492958, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.2918133802816901, "term": "rnns"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 1.9535584881844984e-05, "x": 0.6522887323943662, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.3058978873239437, "term": "sparse"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 4.2764335594216436e-07, "x": 0.6527288732394366, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.31690140845070425, "term": "active"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 0.00036910584109993536, "x": 0.653169014084507, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.3314260563380282, "term": "cnns"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 3.474280720145048e-07, "x": 0.6536091549295775, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.33670774647887325, "term": "official"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 5.461745027536297e-06, "x": 0.6540492957746479, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.3402288732394366, "term": "weights"}, {"ncat": 18, "cat": 1, "cat25k": 22, "bg": 1.767921361852901e-07, "x": 0.6544894366197183, "s": 0.9251760563380281, "ncat25k": 5, "os": 0.6718563247160483, "y": 0.8987676056338029, "term": "size"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 6.025241141029372e-07, "x": 0.6549295774647887, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.37059859154929575, "term": "distance"}, {"ncat": 18, "cat": 1, "cat25k": 22, "bg": 2.1891804480376705e-06, "x": 0.6553697183098591, "s": 0.9251760563380281, "ncat25k": 5, "os": 0.6718563247160483, "y": 0.9044894366197183, "term": "corresponding"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 4.528070641926965e-06, "x": 0.6558098591549296, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.3921654929577465, "term": "comparing"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 2.6541016109560738e-08, "x": 0.65625, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.394806338028169, "term": "will"}, {"ncat": 18, "cat": 1, "cat25k": 22, "bg": 1.6477880028805069e-06, "x": 0.6566901408450704, "s": 0.9251760563380281, "ncat25k": 5, "os": 0.6718563247160483, "y": 0.9181338028169014, "term": "efficient"}, {"ncat": 18, "cat": 1, "cat25k": 22, "bg": 1.400036371471208e-07, "x": 0.6571302816901409, "s": 0.9251760563380281, "ncat25k": 5, "os": 0.6718563247160483, "y": 0.9282570422535211, "term": "because"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 4.323584251344364e-07, "x": 0.6575704225352113, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.4163732394366197, "term": "writing"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 1.1105559017130878e-06, "x": 0.6580105633802817, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.4216549295774648, "term": "handle"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 8.519150197729476e-07, "x": 0.6584507042253521, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.4220950704225352, "term": "particularly"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 3.2438604534484396e-06, "x": 0.6588908450704225, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.4449823943661972, "term": "underlying"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 2.473756195384961e-07, "x": 0.659330985915493, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.4480633802816901, "term": "less"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 1.3281748497917255e-06, "x": 0.6597711267605634, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.46170774647887325, "term": "frames"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 4.10916874255784e-06, "x": 0.6602112676056338, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.5849471830985915, "term": "bases"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 1.1611902654375149e-07, "x": 0.6606514084507042, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.6175176056338029, "term": "full"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 2.755078482084e-07, "x": 0.6610915492957746, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.6179577464788732, "term": "changes"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 5.907088391209399e-06, "x": 0.6615316901408451, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.6276408450704225, "term": "reasoning"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 1.1509966991972438e-06, "x": 0.6619718309859155, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.6694542253521126, "term": "expression"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 2.8559356576735895e-06, "x": 0.6624119718309859, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.7090669014084507, "term": "determining"}, {"ncat": 18, "cat": 0, "cat25k": 0, "bg": 0.00036910584109993536, "x": 0.6628521126760564, "s": 0.24955985915492956, "ncat25k": 5, "os": 0.27863253523995846, "y": 0.8036971830985915, "term": "microblogs"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 5.23051290175518e-07, "x": 0.6632922535211268, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.003961267605633803, "term": "according"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 4.003682713792232e-07, "x": 0.6637323943661971, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.011003521126760563, "term": "individual"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 2.807119149649738e-05, "x": 0.6641725352112676, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.018045774647887324, "term": "grammatical"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 2.700958221791459e-06, "x": 0.664612676056338, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.03125, "term": "arguments"}, {"ncat": 19, "cat": 1, "cat25k": 22, "bg": 1.8656676302575673e-07, "x": 0.6650528169014085, "s": 0.9225352112676057, "ncat25k": 5, "os": 0.6619066794337642, "y": 0.8419894366197183, "term": "since"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 4.350818892812767e-06, "x": 0.6654929577464789, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.035211267605633804, "term": "instances"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 4.043567481677133e-07, "x": 0.6659330985915493, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.08362676056338028, "term": "function"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 2.8686436160684083e-07, "x": 0.6663732394366197, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.1131161971830986, "term": "example"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 1.4272870848247228e-06, "x": 0.6668133802816901, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.1426056338028169, "term": "hidden"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 3.0031463490117593e-06, "x": 0.6672535211267606, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.144806338028169, "term": "aims"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 2.9424324702069033e-06, "x": 0.667693661971831, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.167693661971831, "term": "automated"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 4.3217604322633884e-07, "x": 0.6681338028169014, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.19674295774647887, "term": "makes"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 2.7332043692429638e-06, "x": 0.6685739436619719, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.33274647887323944, "term": "mechanisms"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 6.635504359185858e-07, "x": 0.6690140845070423, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.33934859154929575, "term": "strategy"}, {"ncat": 19, "cat": 1, "cat25k": 22, "bg": 1.160065117935265e-06, "x": 0.6694542253521126, "s": 0.9225352112676057, "ncat25k": 5, "os": 0.6619066794337642, "y": 0.9071302816901409, "term": "teams"}, {"ncat": 19, "cat": 1, "cat25k": 22, "bg": 2.4398210513249906e-06, "x": 0.6698943661971831, "s": 0.9225352112676057, "ncat25k": 5, "os": 0.6619066794337642, "y": 0.9256161971830986, "term": "mainly"}, {"ncat": 19, "cat": 1, "cat25k": 22, "bg": 3.3169539108673617e-07, "x": 0.6703345070422535, "s": 0.9225352112676057, "ncat25k": 5, "os": 0.6619066794337642, "y": 0.9335387323943662, "term": "become"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 7.233699904248656e-05, "x": 0.670774647887324, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.4397007042253521, "term": "parsers"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 7.907589412986341e-05, "x": 0.6712147887323944, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.4467429577464789, "term": "adversarial"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 2.8086015640215183e-06, "x": 0.6716549295774648, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.4555457746478873, "term": "diverse"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 1.63017995041679e-05, "x": 0.6720950704225352, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.47007042253521125, "term": "encode"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 1.7040037810947054e-05, "x": 0.6725352112676056, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.4859154929577465, "term": "reinforcement"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 9.902378186415917e-06, "x": 0.672975352112676, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.5651408450704225, "term": "grounded"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 1.7189299087738114e-07, "x": 0.6734154929577465, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.6509683098591549, "term": "place"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 1.9459208122887974e-05, "x": 0.6738556338028169, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.7196302816901409, "term": "latent"}, {"ncat": 19, "cat": 0, "cat25k": 0, "bg": 3.9881530863057316e-05, "x": 0.6742957746478874, "s": 0.23987676056338028, "ncat25k": 5, "os": 0.2683228753877459, "y": 0.746919014084507, "term": "sarcasm"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 3.9708129365908823e-07, "x": 0.6747359154929577, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.013204225352112676, "term": "parts"}, {"ncat": 20, "cat": 1, "cat25k": 22, "bg": 3.698787675664123e-07, "x": 0.6751760563380281, "s": 0.9198943661971832, "ncat25k": 6, "os": 0.6527149752255714, "y": 0.8446302816901409, "term": "original"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 3.7646171613988506e-07, "x": 0.6756161971830986, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.06029929577464789, "term": "limited"}, {"ncat": 20, "cat": 2, "cat25k": 44, "bg": 3.56974609677877e-07, "x": 0.676056338028169, "s": 0.9643485915492959, "ncat25k": 6, "os": 0.8206998845859386, "y": 0.9471830985915493, "term": "yet"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.95209814840714e-06, "x": 0.6764964788732394, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.13908450704225353, "term": "solve"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 1.0469989998542054e-05, "x": 0.6769366197183099, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.15757042253521128, "term": "noisy"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 1.0037669113708195e-06, "x": 0.6773767605633803, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.16901408450704225, "term": "component"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 5.022065701174536e-05, "x": 0.6778169014084507, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.17913732394366197, "term": "bidirectional"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 8.259818709782038e-07, "x": 0.6782570422535211, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.19190140845070422, "term": "respect"}, {"ncat": 20, "cat": 1, "cat25k": 22, "bg": 1.4869085666289923e-06, "x": 0.6786971830985915, "s": 0.9198943661971832, "ncat25k": 6, "os": 0.6527149752255714, "y": 0.8789612676056338, "term": "despite"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.369199599676343e-06, "x": 0.679137323943662, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.22755281690140844, "term": "oriented"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 6.740906517108421e-06, "x": 0.6795774647887324, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.24031690140845072, "term": "employ"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 9.771389785990686e-07, "x": 0.6800176056338029, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.26672535211267606, "term": "dictionary"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.021074552791781e-06, "x": 0.6804577464788732, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.3067781690140845, "term": "relatively"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.6471551215827094e-07, "x": 0.6808978873239436, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.33846830985915494, "term": "feedback"}, {"ncat": 20, "cat": 1, "cat25k": 22, "bg": 3.164478279247125e-06, "x": 0.6813380281690141, "s": 0.9198943661971832, "ncat25k": 6, "os": 0.6527149752255714, "y": 0.9040492957746479, "term": "hybrid"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 6.932543577968931e-06, "x": 0.6817781690140845, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.3970070422535211, "term": "token"}, {"ncat": 20, "cat": 2, "cat25k": 44, "bg": 1.2390502454586696e-05, "x": 0.6822183098591549, "s": 0.9643485915492959, "ncat25k": 6, "os": 0.8206998845859386, "y": 0.960387323943662, "term": "bilingual"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.887433186239787e-07, "x": 0.6826584507042254, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.460387323943662, "term": "story"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 6.493525468093901e-07, "x": 0.6830985915492958, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.46742957746478875, "term": "publications"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.5866411623848053e-05, "x": 0.6835387323943662, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.4705105633802817, "term": "exploiting"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.4547283652822363e-06, "x": 0.6839788732394366, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.6205985915492958, "term": "consists"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.684419014084507, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.636443661971831, "term": "8"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 8.282859657296682e-06, "x": 0.6848591549295775, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.6474471830985915, "term": "emotion"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 5.827038917336169e-05, "x": 0.6852992957746479, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.6668133802816901, "term": "paraphrase"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 1.5505160253002353e-06, "x": 0.6857394366197183, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.6839788732394366, "term": "aware"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 2.0204896032222123e-07, "x": 0.6861795774647887, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.7517605633802817, "term": "image"}, {"ncat": 20, "cat": 0, "cat25k": 0, "bg": 1.1512862356909153e-06, "x": 0.6866197183098591, "s": 0.22975352112676056, "ncat25k": 6, "os": 0.25829799497379424, "y": 0.7984154929577465, "term": "headlines"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 5.09459022567203e-07, "x": 0.6870598591549296, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.0048415492957746475, "term": "created"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 4.296403694264762e-07, "x": 0.6875, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.007482394366197183, "term": "rules"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 8.409733930635353e-07, "x": 0.6879401408450704, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.016725352112676055, "term": "variety"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 1.4813925456750356e-06, "x": 0.6883802816901409, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.040492957746478875, "term": "containing"}, {"ncat": 21, "cat": 1, "cat25k": 22, "bg": 1.530698020074812e-07, "x": 0.6888204225352113, "s": 0.9163732394366197, "ncat25k": 6, "os": 0.6442026644229215, "y": 0.8428697183098591, "term": "made"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 5.271286575339618e-07, "x": 0.6892605633802817, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.04401408450704225, "term": "although"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 7.881872500203145e-07, "x": 0.6897007042253521, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.061179577464788734, "term": "reported"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 6.872272290816499e-08, "x": 0.6901408450704225, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.06294014084507042, "term": "help"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.690580985915493, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.07746478873239436, "term": "word2vec"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 2.2387510754000703e-06, "x": 0.6910211267605634, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.08934859154929578, "term": "typically"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 4.401227187889331e-06, "x": 0.6914612676056338, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.0977112676056338, "term": "pos"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 8.486484061776754e-06, "x": 0.6919014084507042, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.1034330985915493, "term": "exploit"}, {"ncat": 21, "cat": 1, "cat25k": 22, "bg": 2.9859383611929757e-07, "x": 0.6923415492957746, "s": 0.9163732394366197, "ncat25k": 6, "os": 0.6442026644229215, "y": 0.858274647887324, "term": "against"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 1.02952609761638e-06, "x": 0.6927816901408451, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.14964788732394366, "term": "fields"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 0.00043061023622047243, "x": 0.6932218309859155, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.1500880281690141, "term": "bioasq"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 1.5144319960538227e-05, "x": 0.6936619718309859, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.22711267605633803, "term": "clustering"}, {"ncat": 21, "cat": 1, "cat25k": 22, "bg": 2.76756029112797e-07, "x": 0.6941021126760564, "s": 0.9163732394366197, "ncat25k": 6, "os": 0.6442026644229215, "y": 0.8860035211267606, "term": "cost"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 7.836867906923162e-07, "x": 0.6945422535211268, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.2623239436619718, "term": "require"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 3.3913321104082083e-06, "x": 0.6949823943661971, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.2636443661971831, "term": "binary"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 7.493387620666733e-07, "x": 0.6954225352112676, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.2746478873239437, "term": "abstract"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 7.070016732372933e-06, "x": 0.695862676056338, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.27508802816901406, "term": "distant"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 1.6089824063175453e-07, "x": 0.6963028169014085, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.304137323943662, "term": "states"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 4.989361967519253e-05, "x": 0.6967429577464789, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.3543133802816901, "term": "disambiguation"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 6.7487811219238005e-06, "x": 0.6971830985915493, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.35563380281690143, "term": "performs"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 1.0490929280271044e-06, "x": 0.6976232394366197, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.36311619718309857, "term": "measures"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 6.7545820551694e-07, "x": 0.6980633802816901, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.3807218309859155, "term": "therefore"}, {"ncat": 21, "cat": 1, "cat25k": 22, "bg": 0.0, "x": 0.6985035211267606, "s": 0.9163732394366197, "ncat25k": 6, "os": 0.6442026644229215, "y": 0.9053697183098591, "term": "6"}, {"ncat": 21, "cat": 1, "cat25k": 22, "bg": 4.6703526080129967e-07, "x": 0.698943661971831, "s": 0.9163732394366197, "ncat25k": 6, "os": 0.6442026644229215, "y": 0.90625, "term": "together"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 1.7140284700945087e-06, "x": 0.6993838028169014, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.4005281690140845, "term": "affect"}, {"ncat": 21, "cat": 1, "cat25k": 22, "bg": 1.5489343929828203e-06, "x": 0.6998239436619719, "s": 0.9163732394366197, "ncat25k": 6, "os": 0.6442026644229215, "y": 0.917693661971831, "term": "scheme"}, {"ncat": 21, "cat": 2, "cat25k": 44, "bg": 1.7388288383719545e-07, "x": 0.7002640845070423, "s": 0.9581866197183099, "ncat25k": 6, "os": 0.8098443002800464, "y": 0.9617077464788732, "term": "design"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 6.469973315981481e-06, "x": 0.7007042253521126, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.4546654929577465, "term": "explicitly"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 3.89530134839566e-06, "x": 0.7011443661971831, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.46919014084507044, "term": "encoding"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 1.5286261450683682e-06, "x": 0.7015845070422535, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.5409330985915493, "term": "trees"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 4.607289455921897e-06, "x": 0.702024647887324, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.5642605633802817, "term": "layers"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 3.416828255661063e-08, "x": 0.7024647887323944, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.6289612676056338, "term": "us"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 2.309845921727791e-06, "x": 0.7029049295774648, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.6470070422535211, "term": "humor"}, {"ncat": 21, "cat": 0, "cat25k": 0, "bg": 0.00019226103555455864, "x": 0.7033450704225352, "s": 0.21610915492957747, "ncat25k": 6, "os": 0.24855412298666074, "y": 0.6848591549295775, "term": "entailment"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.5296048855023823e-07, "x": 0.7037852112676056, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.0044014084507042256, "term": "comments"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 4.775486161856681e-07, "x": 0.704225352112676, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.055897887323943664, "term": "amount"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 5.227961084483494e-07, "x": 0.7046654929577465, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.0625, "term": "response"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 2.369675776113248e-06, "x": 0.7051056338028169, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.07174295774647887, "term": "researchers"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 5.326029700386741e-07, "x": 0.7055457746478874, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.08142605633802817, "term": "higher"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 3.3095524439578067e-06, "x": 0.7059859154929577, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.09022887323943662, "term": "widely"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 4.109242838666074e-07, "x": 0.7064260563380281, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.09330985915492958, "term": "called"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 6.668362047199273e-06, "x": 0.7068661971830986, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.11047535211267606, "term": "vocabulary"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.0344702412064858e-06, "x": 0.707306338028169, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.12235915492957747, "term": "difficult"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 7.44853611718985e-06, "x": 0.7077464788732394, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.20906690140845072, "term": "retrieval"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 9.420924060757425e-07, "x": 0.7081866197183099, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.21522887323943662, "term": "literature"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 6.978965208526088e-07, "x": 0.7086267605633803, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.2205105633802817, "term": "prior"}, {"ncat": 22, "cat": 1, "cat25k": 22, "bg": 4.706044982301281e-07, "x": 0.7090669014084507, "s": 0.9132922535211268, "ncat25k": 6, "os": 0.636300826434653, "y": 0.8855633802816901, "term": "final"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.4592923758606509e-05, "x": 0.7095070422535211, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.2592429577464789, "term": "hierarchical"}, {"ncat": 22, "cat": 1, "cat25k": 22, "bg": 2.691097148607065e-06, "x": 0.7099471830985915, "s": 0.9132922535211268, "ncat25k": 6, "os": 0.636300826434653, "y": 0.886443661971831, "term": "mining"}, {"ncat": 22, "cat": 1, "cat25k": 22, "bg": 4.208167854374778e-07, "x": 0.710387323943662, "s": 0.9132922535211268, "ncat25k": 6, "os": 0.636300826434653, "y": 0.8877640845070423, "term": "provides"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 9.685566876417892e-06, "x": 0.7108274647887324, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.2720070422535211, "term": "incorporating"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.4549833059183616e-07, "x": 0.7112676056338029, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.28036971830985913, "term": "could"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 7.600346161220617e-05, "x": 0.7117077464788732, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.29225352112676056, "term": "utterances"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 6.649370344675703e-07, "x": 0.7121478873239436, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.3001760563380282, "term": "distribution"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 3.477479879815134e-06, "x": 0.7125880281690141, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.31205985915492956, "term": "linking"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 6.522540268532982e-07, "x": 0.7130281690140845, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.3182218309859155, "term": "levels"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 2.2434199346114473e-06, "x": 0.7134683098591549, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.3340669014084507, "term": "interaction"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 7.584904516136617e-07, "x": 0.7139084507042254, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.3459507042253521, "term": "rule"}, {"ncat": 22, "cat": 1, "cat25k": 22, "bg": 2.414304966868257e-07, "x": 0.7143485915492958, "s": 0.9132922535211268, "ncat25k": 6, "os": 0.636300826434653, "y": 0.8983274647887324, "term": "still"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.7431255683480722e-06, "x": 0.7147887323943662, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.3485915492957746, "term": "accurate"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.5363721368186316e-07, "x": 0.7152288732394366, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.37367957746478875, "term": "development"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 2.2841327521966647e-07, "x": 0.715669014084507, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.3798415492957746, "term": "another"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.6452373873764898e-06, "x": 0.7161091549295775, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.3846830985915493, "term": "relative"}, {"ncat": 22, "cat": 4, "cat25k": 89, "bg": 1.8332007318137324e-06, "x": 0.7165492957746479, "s": 0.9872359154929577, "ncat25k": 6, "os": 0.9386466669526184, "y": 0.9797535211267606, "term": "matrix"}, {"ncat": 22, "cat": 1, "cat25k": 22, "bg": 1.6549041357205587e-06, "x": 0.7169894366197183, "s": 0.9132922535211268, "ncat25k": 6, "os": 0.636300826434653, "y": 0.9172535211267606, "term": "previously"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 4.369285235579447e-06, "x": 0.7174295774647887, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.4141725352112676, "term": "scenario"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.1282138357016552e-06, "x": 0.7178697183098591, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.43441901408450706, "term": "argument"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 3.3241111912171544e-06, "x": 0.7183098591549296, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.47315140845070425, "term": "combine"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 8.475561440934054e-07, "x": 0.71875, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.4841549295774648, "term": "functions"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 1.1317900587059503e-06, "x": 0.7191901408450704, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.5079225352112676, "term": "characters"}, {"ncat": 22, "cat": 0, "cat25k": 0, "bg": 4.084609719718723e-06, "x": 0.7196302816901409, "s": 0.20246478873239437, "ncat25k": 6, "os": 0.23908733563583423, "y": 0.8063380281690141, "term": "submissions"}, {"ncat": 23, "cat": 1, "cat25k": 22, "bg": 1.9762794587612857e-07, "x": 0.7200704225352113, "s": 0.9115316901408451, "ncat25k": 6, "os": 0.628948931570289, "y": 0.8358274647887324, "term": "being"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 1.673030486688933e-06, "x": 0.7205105633802817, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.03917253521126761, "term": "improved"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 2.1206007204528887e-07, "x": 0.7209507042253521, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.042693661971830985, "term": "posts"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 5.661586402110088e-08, "x": 0.7213908450704225, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.053257042253521125, "term": "what"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 6.017869199153478e-07, "x": 0.721830985915493, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.06514084507042253, "term": "apply"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 3.39158703564381e-06, "x": 0.7222711267605634, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.08054577464788733, "term": "precision"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 2.066319327371732e-06, "x": 0.7227112676056338, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.19586267605633803, "term": "challenges"}, {"ncat": 23, "cat": 1, "cat25k": 22, "bg": 1.5989362809901332e-06, "x": 0.7231514084507042, "s": 0.9115316901408451, "ncat25k": 6, "os": 0.628948931570289, "y": 0.8736795774647887, "term": "parameters"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 1.0783431033172014e-06, "x": 0.7235915492957746, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.22183098591549297, "term": "patient"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 1.0442347629616258e-07, "x": 0.7240316901408451, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.22315140845070422, "term": "health"}, {"ncat": 23, "cat": 1, "cat25k": 22, "bg": 8.076196355148649e-07, "x": 0.7244718309859155, "s": 0.9115316901408451, "ncat25k": 6, "os": 0.628948931570289, "y": 0.8868838028169014, "term": "especially"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 8.354177308405389e-07, "x": 0.7249119718309859, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.323943661971831, "term": "components"}, {"ncat": 23, "cat": 5, "cat25k": 111, "bg": 2.155582641477914e-06, "x": 0.7253521126760564, "s": 0.9920774647887325, "ncat25k": 6, "os": 0.9626224877695286, "y": 0.9828345070422535, "term": "runs"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 1.6154381382539016e-06, "x": 0.7257922535211268, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.4665492957746479, "term": "aspects"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 3.867070949318841e-06, "x": 0.7262323943661971, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.47579225352112675, "term": "moreover"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 4.5001449340156465e-06, "x": 0.7266725352112676, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.49911971830985913, "term": "scoring"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 2.868116878007392e-06, "x": 0.727112676056338, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.5796654929577465, "term": "composition"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 7.708798113460024e-08, "x": 0.7275528169014085, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.6452464788732394, "term": "c"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 3.6449530474580816e-05, "x": 0.7279929577464789, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.6778169014084507, "term": "bleu"}, {"ncat": 23, "cat": 0, "cat25k": 0, "bg": 1.0057787455118491e-06, "x": 0.7284330985915493, "s": 0.1954225352112676, "ncat25k": 6, "os": 0.2298935719291405, "y": 0.6932218309859155, "term": "kb"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 2.6373930688211238e-05, "x": 0.7288732394366197, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.00044014084507042255, "term": "classify"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 2.403433388715949e-07, "x": 0.7293133802816901, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.012764084507042254, "term": "main"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 3.8121921625046027e-07, "x": 0.7297535211267606, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.030809859154929578, "term": "groups"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 1.1646655044275488e-05, "x": 0.730193661971831, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.0488556338028169, "term": "proposes"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 1.5316185417230122e-07, "x": 0.7306338028169014, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.05369718309859155, "term": "under"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 3.499341587944786e-07, "x": 0.7310739436619719, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.05545774647887324, "term": "create"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 1.9124913295412614e-06, "x": 0.7315140845070423, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.07130281690140845, "term": "differences"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 9.212728582191011e-07, "x": 0.7319542253521126, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.10519366197183098, "term": "ability"}, {"ncat": 24, "cat": 1, "cat25k": 22, "bg": 4.624388741359583e-07, "x": 0.7323943661971831, "s": 0.9102112676056339, "ncat25k": 7, "os": 0.6220937354158309, "y": 0.8573943661971831, "term": "studies"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 0.000357771086133389, "x": 0.7328345070422535, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.1192781690140845, "term": "rnn"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 9.212540801479535e-06, "x": 0.733274647887324, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.14612676056338028, "term": "conditional"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 9.072041649743212e-06, "x": 0.7337147887323944, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.1522887323943662, "term": "generates"}, {"ncat": 24, "cat": 3, "cat25k": 67, "bg": 2.0662544489041102e-06, "x": 0.7341549295774648, "s": 0.9727112676056339, "ncat25k": 7, "os": 0.8694089554346378, "y": 0.96875, "term": "pair"}, {"ncat": 24, "cat": 1, "cat25k": 22, "bg": 6.4095760091108285e-06, "x": 0.7345950704225352, "s": 0.9102112676056339, "ncat25k": 7, "os": 0.6220937354158309, "y": 0.871919014084507, "term": "evaluating"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 4.2287565371532386e-08, "x": 0.7350352112676056, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.23899647887323944, "term": "if"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 2.6934145173359103e-06, "x": 0.735475352112676, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.24471830985915494, "term": "driven"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 2.5910984324826143e-06, "x": 0.7359154929577465, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.2596830985915493, "term": "labels"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 3.5046633269269547e-07, "x": 0.7363556338028169, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.26848591549295775, "term": "key"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 2.396776814539807e-06, "x": 0.7367957746478874, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.26936619718309857, "term": "focused"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 8.161187534398128e-06, "x": 0.7372359154929577, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.2737676056338028, "term": "empirical"}, {"ncat": 24, "cat": 13, "cat25k": 289, "bg": 9.403683219394157e-06, "x": 0.7376760563380281, "s": 0.997799295774648, "ncat25k": 7, "os": 0.9997122061617867, "y": 0.9938380281690141, "term": "comparable"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 8.980268945958705e-07, "x": 0.7381161971830986, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.31558098591549294, "term": "allows"}, {"ncat": 24, "cat": 1, "cat25k": 22, "bg": 0.00010778184953653804, "x": 0.738556338028169, "s": 0.9102112676056339, "ncat25k": 7, "os": 0.6220937354158309, "y": 0.897887323943662, "term": "svm"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 1.1773547585735956e-06, "x": 0.7389964788732394, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.352112676056338, "term": "bag"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 2.2247764922329113e-06, "x": 0.7394366197183099, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.3842429577464789, "term": "kernel"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 4.3717782442080607e-07, "x": 0.7398767605633803, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.42473591549295775, "term": "messages"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 5.913750899998965e-06, "x": 0.7403169014084507, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.44278169014084506, "term": "grammar"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 1.0539765547306967e-05, "x": 0.7407570422535211, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.4630281690140845, "term": "jointly"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 1.2058086818727516e-05, "x": 0.7411971830985915, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.4850352112676056, "term": "metrics"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 7.746069461193123e-08, "x": 0.741637323943662, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.5616197183098591, "term": "web"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 1.9985072816029164e-06, "x": 0.7420774647887324, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.6830985915492958, "term": "responses"}, {"ncat": 24, "cat": 0, "cat25k": 0, "bg": 0.00010766580534022396, "x": 0.7425176056338029, "s": 0.1835387323943662, "ncat25k": 7, "os": 0.2209686483918462, "y": 0.7944542253521126, "term": "puns"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 1.431091527596698e-07, "x": 0.7429577464788732, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.058538732394366196, "term": "public"}, {"ncat": 25, "cat": 2, "cat25k": 44, "bg": 1.8158385122084524e-07, "x": 0.7433978873239436, "s": 0.9511443661971831, "ncat25k": 7, "os": 0.7711629069368588, "y": 0.9463028169014085, "term": "f"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 4.093110067822834e-06, "x": 0.7438380281690141, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.08670774647887323, "term": "recall"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 5.654663587664645e-06, "x": 0.7442781690140845, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.09859154929577464, "term": "analyze"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 1.3040906820842874e-06, "x": 0.7447183098591549, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.10827464788732394, "term": "errors"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 5.417001888800219e-06, "x": 0.7451584507042254, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.11223591549295775, "term": "rely"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 2.3006403142122516e-06, "x": 0.7455985915492958, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.15536971830985916, "term": "candidate"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 8.397020817625306e-07, "x": 0.7460387323943662, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.18529929577464788, "term": "thus"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 8.217636003396119e-07, "x": 0.7464788732394366, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.19762323943661972, "term": "properties"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 2.641557639897156e-06, "x": 0.746919014084507, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.2975352112676056, "term": "technique"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 1.4326260057607612e-06, "x": 0.7473591549295775, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.3019366197183099, "term": "matching"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 2.9512381093140954e-06, "x": 0.7477992957746479, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.34066901408450706, "term": "respectively"}, {"ncat": 25, "cat": 1, "cat25k": 22, "bg": 2.3668095021029326e-06, "x": 0.7482394366197183, "s": 0.9084507042253522, "ncat25k": 7, "os": 0.6156883082000815, "y": 0.8992077464788732, "term": "continuous"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 3.893703028702884e-07, "x": 0.7486795774647887, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.36047535211267606, "term": "range"}, {"ncat": 25, "cat": 1, "cat25k": 22, "bg": 1.3326403603459536e-05, "x": 0.7491197183098591, "s": 0.9084507042253522, "ncat25k": 7, "os": 0.6156883082000815, "y": 0.9124119718309859, "term": "contexts"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 8.584895525341265e-07, "x": 0.7495598591549296, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.4819542253521127, "term": "directly"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 3.577797050177888e-05, "x": 0.75, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.5347711267605634, "term": "morphological"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 7.26718835389463e-05, "x": 0.7504401408450704, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.5382922535211268, "term": "compositional"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 1.4402535307095179e-05, "x": 0.7508802816901409, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.5391725352112676, "term": "architectures"}, {"ncat": 25, "cat": 0, "cat25k": 0, "bg": 2.1216958935933866e-07, "x": 0.7513204225352113, "s": 0.176056338028169, "ncat25k": 7, "os": 0.2123082729469109, "y": 0.5743838028169014, "term": "case"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 4.947080414849226e-07, "x": 0.7517605633802817, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.0008802816901408451, "term": "whether"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 5.269598805884588e-07, "x": 0.7522007042253521, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.037411971830985914, "term": "works"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 1.6276730817238097e-07, "x": 0.7526408450704225, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.06778169014084508, "term": "need"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 7.98681904534412e-07, "x": 0.753080985915493, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.09903169014084508, "term": "tool"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 1.210574442249942e-06, "x": 0.7535211267605634, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.11355633802816902, "term": "highly"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 3.1423368616539066e-06, "x": 0.7539612676056338, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.12015845070422536, "term": "semi"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 2.4756041116737396e-06, "x": 0.7544014084507042, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.1267605633802817, "term": "performing"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 5.127169293372847e-08, "x": 0.7548415492957746, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.15140845070422534, "term": "free"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 1.597044239661058e-05, "x": 0.7552816901408451, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.26100352112676056, "term": "dependencies"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 1.7986476245226235e-05, "x": 0.7557218309859155, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.31294014084507044, "term": "mentions"}, {"ncat": 26, "cat": 1, "cat25k": 22, "bg": 1.0488480181735064e-06, "x": 0.7561619718309859, "s": 0.9071302816901409, "ncat25k": 7, "os": 0.6096911920574052, "y": 0.8970070422535211, "term": "submitted"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.7566021126760564, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.3345070422535211, "term": "2016"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 8.091223566497759e-07, "x": 0.7570422535211268, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.45994718309859156, "term": "independent"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 9.364823633357069e-07, "x": 0.7574823943661971, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.47183098591549294, "term": "resolution"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 1.6899801047092172e-06, "x": 0.7579225352112676, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.4727112676056338, "term": "obtain"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.758362676056338, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.4920774647887324, "term": "et al"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 1.393433988448373e-07, "x": 0.7588028169014085, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.5105633802816901, "term": "message"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 2.7334296341619832e-05, "x": 0.7592429577464789, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.5580985915492958, "term": "decoding"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 6.332249528235232e-07, "x": 0.7596830985915493, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.6377640845070423, "term": "political"}, {"ncat": 26, "cat": 0, "cat25k": 0, "bg": 2.9608473489597743e-07, "x": 0.7601232394366197, "s": 0.16769366197183097, "ncat25k": 7, "os": 0.20390805797924472, "y": 0.6386443661971831, "term": "point"}, {"ncat": 27, "cat": 1, "cat25k": 22, "bg": 2.3077152897470477e-06, "x": 0.7605633802816901, "s": 0.9049295774647887, "ncat25k": 7, "os": 0.6040656740808438, "y": 0.8375880281690141, "term": "characteristics"}, {"ncat": 27, "cat": 1, "cat25k": 22, "bg": 1.8299439612339443e-06, "x": 0.7610035211267606, "s": 0.9049295774647887, "ncat25k": 7, "os": 0.6040656740808438, "y": 0.8516725352112676, "term": "negative"}, {"ncat": 27, "cat": 1, "cat25k": 22, "bg": 5.222934037663399e-07, "x": 0.761443661971831, "s": 0.9049295774647887, "ncat25k": 7, "os": 0.6040656740808438, "y": 0.8609154929577465, "term": "collection"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 6.463791551436615e-07, "x": 0.7618838028169014, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.18441901408450703, "term": "along"}, {"ncat": 27, "cat": 3, "cat25k": 67, "bg": 3.752734946319691e-07, "x": 0.7623239436619719, "s": 0.9678697183098591, "ncat25k": 7, "os": 0.8441484410994708, "y": 0.9696302816901409, "term": "provided"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 4.428337941524667e-07, "x": 0.7627640845070423, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.21038732394366197, "term": "future"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 9.69330073368593e-07, "x": 0.7632042253521126, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.21875, "term": "develop"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 1.1626693484424653e-06, "x": 0.7636443661971831, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.3076584507042254, "term": "setting"}, {"ncat": 27, "cat": 6, "cat25k": 133, "bg": 8.739425295392574e-06, "x": 0.7640845070422535, "s": 0.9925176056338029, "ncat25k": 7, "os": 0.9650956413093147, "y": 0.9859154929577465, "term": "alignment"}, {"ncat": 27, "cat": 1, "cat25k": 22, "bg": 0.0, "x": 0.764524647887324, "s": 0.9049295774647887, "ncat25k": 7, "os": 0.6040656740808438, "y": 0.9300176056338029, "term": "7"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 2.698617767979241e-06, "x": 0.7649647887323944, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.4476232394366197, "term": "wikipedia"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 5.174961420662608e-07, "x": 0.7654049295774648, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.4898767605633803, "term": "et"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 2.6825835942725455e-06, "x": 0.7658450704225352, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.5462147887323944, "term": "transition"}, {"ncat": 27, "cat": 0, "cat25k": 0, "bg": 1.4709988490796042e-05, "x": 0.7662852112676056, "s": 0.1641725352112676, "ncat25k": 7, "os": 0.19576353260962648, "y": 0.8102992957746479, "term": "sts"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 4.803021896702369e-07, "x": 0.766725352112676, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.010123239436619719, "term": "popular"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 2.3100038060200206e-07, "x": 0.7671654929577465, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.02772887323943662, "term": "much"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 1.8705308716924378e-06, "x": 0.7676056338028169, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.08714788732394366, "term": "combined"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 6.58402693901651e-06, "x": 0.7680457746478874, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.09551056338028169, "term": "expressions"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 1.2006361656483413e-05, "x": 0.7684859154929577, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.10871478873239436, "term": "gram"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 5.371016796397183e-07, "x": 0.7689260563380281, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.1681338028169014, "term": "five"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 4.537752741483319e-07, "x": 0.7693661971830986, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.1694542253521127, "term": "possible"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 2.1415859777302363e-07, "x": 0.769806338028169, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.18661971830985916, "term": "within"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 4.213852724793805e-06, "x": 0.7702464788732394, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.18838028169014084, "term": "phrase"}, {"ncat": 28, "cat": 1, "cat25k": 22, "bg": 4.5189320619780256e-07, "x": 0.7706866197183099, "s": 0.9036091549295775, "ncat25k": 8, "os": 0.5987791615035462, "y": 0.8930457746478874, "term": "building"}, {"ncat": 28, "cat": 1, "cat25k": 22, "bg": 1.1439328680235183e-06, "x": 0.7711267605633803, "s": 0.9036091549295775, "ncat25k": 8, "os": 0.5987791615035462, "y": 0.8965669014084507, "term": "external"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 1.2505887537367201e-06, "x": 0.7715669014084507, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.3529929577464789, "term": "skip"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 2.481151445673904e-06, "x": 0.7720070422535211, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.4471830985915493, "term": "objective"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 9.728488314522027e-05, "x": 0.7724471830985915, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.5004401408450704, "term": "generative"}, {"ncat": 28, "cat": 0, "cat25k": 0, "bg": 9.016287601541913e-07, "x": 0.772887323943662, "s": 0.1584507042253521, "ncat25k": 8, "os": 0.18787015420619885, "y": 0.5255281690140845, "term": "evidence"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 1.4314548369606752e-07, "x": 0.7733274647887324, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.0017605633802816902, "term": "make"}, {"ncat": 29, "cat": 1, "cat25k": 22, "bg": 2.0954482474102173e-07, "x": 0.7737676056338029, "s": 0.9014084507042254, "ncat25k": 8, "os": 0.5938026455519825, "y": 0.8301056338028169, "term": "report"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 1.936698928731683e-06, "x": 0.7742077464788732, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.06866197183098592, "term": "relationships"}, {"ncat": 29, "cat": 1, "cat25k": 22, "bg": 4.641640052340681e-07, "x": 0.7746478873239436, "s": 0.9014084507042254, "ncat25k": 8, "os": 0.5938026455519825, "y": 0.8512323943661971, "term": "four"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 3.3345492556389184e-07, "x": 0.7750880281690141, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.08582746478873239, "term": "categories"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 4.777822796401196e-07, "x": 0.7755281690140845, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.09110915492957747, "term": "hand"}, {"ncat": 29, "cat": 1, "cat25k": 22, "bg": 1.2572643687506104e-05, "x": 0.7759683098591549, "s": 0.9014084507042254, "ncat25k": 8, "os": 0.5938026455519825, "y": 0.8543133802816901, "term": "extracted"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 5.684417042940772e-06, "x": 0.7764084507042254, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.10079225352112677, "term": "algorithms"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 1.3141190101974048e-06, "x": 0.7768485915492958, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.14128521126760563, "term": "random"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 2.616877061326469e-06, "x": 0.7772887323943662, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.20422535211267606, "term": "submission"}, {"ncat": 29, "cat": 1, "cat25k": 22, "bg": 1.6910642137667508e-06, "x": 0.7777288732394366, "s": 0.9014084507042254, "ncat25k": 8, "os": 0.5938026455519825, "y": 0.8776408450704225, "term": "produce"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 4.38446476487635e-07, "x": 0.778169014084507, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.258362676056338, "term": "additional"}, {"ncat": 29, "cat": 2, "cat25k": 44, "bg": 2.8257557426762905e-06, "x": 0.7786091549295775, "s": 0.9471830985915494, "ncat25k": 8, "os": 0.7390968032925269, "y": 0.9529049295774648, "term": "resulting"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 1.8920051917927296e-06, "x": 0.7790492957746479, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.40713028169014087, "term": "participants"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 3.6383388323479186e-06, "x": 0.7794894366197183, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.43661971830985913, "term": "syntax"}, {"ncat": 29, "cat": 0, "cat25k": 0, "bg": 3.5796593633180823e-06, "x": 0.7799295774647887, "s": 0.15360915492957747, "ncat25k": 8, "os": 0.18022331916326573, "y": 0.4533450704225352, "term": "interpretation"}, {"ncat": 30, "cat": 1, "cat25k": 22, "bg": 1.262815618104352e-06, "x": 0.7803697183098591, "s": 0.8996478873239437, "ncat25k": 8, "os": 0.5891102415500526, "y": 0.8384683098591549, "term": "finally"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 2.585054330022228e-07, "x": 0.7808098591549296, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.02992957746478873, "term": "found"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 2.2688826682344543e-07, "x": 0.78125, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.030369718309859156, "term": "take"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 1.9632663122096228e-07, "x": 0.7816901408450704, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.035651408450704226, "term": "way"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 1.36967876376082e-07, "x": 0.7821302816901409, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.08494718309859155, "term": "n"}, {"ncat": 30, "cat": 2, "cat25k": 44, "bg": 3.491535688251951e-07, "x": 0.7825704225352113, "s": 0.9414612676056339, "ncat25k": 8, "os": 0.7319584946778213, "y": 0.9467429577464789, "term": "compare"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 1.3463331371651157e-06, "x": 0.7830105633802817, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.09727112676056338, "term": "correct"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 6.045915341304617e-07, "x": 0.7834507042253521, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.10035211267605634, "term": "either"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 6.704358570550301e-07, "x": 0.7838908450704225, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.10475352112676056, "term": "error"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 2.2153261580395958e-07, "x": 0.784330985915493, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.13512323943661972, "term": "local"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 1.771387660808786e-05, "x": 0.7847711267605634, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.14172535211267606, "term": "sequential"}, {"ncat": 30, "cat": 1, "cat25k": 22, "bg": 9.004648649865494e-07, "x": 0.7852112676056338, "s": 0.8996478873239437, "ncat25k": 8, "os": 0.5891102415500526, "y": 0.8679577464788732, "term": "shown"}, {"ncat": 30, "cat": 1, "cat25k": 22, "bg": 1.6150655355431889e-07, "x": 0.7856514084507042, "s": 0.8996478873239437, "ncat25k": 8, "os": 0.5891102415500526, "y": 0.8701584507042254, "term": "number"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 6.675732385420872e-07, "x": 0.7860915492957746, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.3886443661971831, "term": "points"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 5.77590360401945e-06, "x": 0.7865316901408451, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.38952464788732394, "term": "spoken"}, {"ncat": 30, "cat": 1, "cat25k": 22, "bg": 1.0145802380453975e-06, "x": 0.7869718309859155, "s": 0.8996478873239437, "ncat25k": 8, "os": 0.5891102415500526, "y": 0.9075704225352113, "term": "transfer"}, {"ncat": 30, "cat": 2, "cat25k": 44, "bg": 0.00021638581590976709, "x": 0.7874119718309859, "s": 0.9414612676056339, "ncat25k": 8, "os": 0.7319584946778213, "y": 0.9564260563380281, "term": "monolingual"}, {"ncat": 30, "cat": 2, "cat25k": 44, "bg": 3.302000444738185e-05, "x": 0.7878521126760564, "s": 0.9414612676056339, "ncat25k": 8, "os": 0.7319584946778213, "y": 0.9577464788732394, "term": "lexicon"}, {"ncat": 30, "cat": 2, "cat25k": 44, "bg": 1.1894068968613802e-06, "x": 0.7882922535211268, "s": 0.9414612676056339, "ncat25k": 8, "os": 0.7319584946778213, "y": 0.9608274647887324, "term": "german"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 2.906103223624062e-07, "x": 0.7887323943661971, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.4863556338028169, "term": "during"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 4.7662515833487756e-07, "x": 0.7891725352112676, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.4894366197183099, "term": "al"}, {"ncat": 30, "cat": 0, "cat25k": 0, "bg": 1.6435771020234874e-06, "x": 0.789612676056338, "s": 0.14744718309859153, "ncat25k": 8, "os": 0.1728183729784668, "y": 0.5061619718309859, "term": "rank"}, {"ncat": 31, "cat": 1, "cat25k": 22, "bg": 1.0988608916112635e-06, "x": 0.7900528169014085, "s": 0.8970070422535212, "ncat25k": 9, "os": 0.5846787942146678, "y": 0.8380281690140845, "term": "useful"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 7.269224401895251e-06, "x": 0.7904929577464789, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.04137323943661972, "term": "detect"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 3.1624009703062277e-06, "x": 0.7909330985915493, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.10959507042253522, "term": "effectively"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 1.1631414725220958e-05, "x": 0.7913732394366197, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.11399647887323944, "term": "promising"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 1.6803894860956982e-06, "x": 0.7918133802816901, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.12720070422535212, "term": "towards"}, {"ncat": 31, "cat": 1, "cat25k": 22, "bg": 2.36950883451006e-06, "x": 0.7922535211267606, "s": 0.8970070422535212, "ncat25k": 9, "os": 0.5846787942146678, "y": 0.8732394366197183, "term": "layer"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 2.8510085902728185e-05, "x": 0.792693661971831, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.24735915492957747, "term": "ontology"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 1.7327727869263524e-06, "x": 0.7931338028169014, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.3085387323943662, "term": "comparison"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 1.4828176117596045e-05, "x": 0.7935739436619719, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.3248239436619718, "term": "labeling"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 7.730068983384963e-06, "x": 0.7940140845070423, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.3617957746478873, "term": "correlation"}, {"ncat": 31, "cat": 1, "cat25k": 22, "bg": 6.841397234805583e-07, "x": 0.7944542253521126, "s": 0.8970070422535212, "ncat25k": 9, "os": 0.5846787942146678, "y": 0.9128521126760564, "term": "global"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 6.023209875794807e-07, "x": 0.7948943661971831, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.4515845070422535, "term": "average"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 7.422559716168978e-07, "x": 0.7953345070422535, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.46566901408450706, "term": "among"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 1.435079563993664e-07, "x": 0.795774647887324, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.6756161971830986, "term": "world"}, {"ncat": 31, "cat": 0, "cat25k": 0, "bg": 0.00041905199624204983, "x": 0.7962147887323944, "s": 0.14216549295774647, "ncat25k": 9, "os": 0.16565061966034006, "y": 0.7768485915492958, "term": "keyphrase"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 1.353958098060195e-07, "x": 0.7966549295774648, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.00528169014084507, "term": "list"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 1.3214640587854277e-07, "x": 0.7970950704225352, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.02464788732394366, "term": "top"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 1.5437531978606281e-06, "x": 0.7975352112676056, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.07350352112676056, "term": "label"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 6.828780004724384e-08, "x": 0.797975352112676, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.2262323943661972, "term": "no"}, {"ncat": 32, "cat": 1, "cat25k": 22, "bg": 6.595934859386812e-07, "x": 0.7984154929577465, "s": 0.8956866197183099, "ncat25k": 9, "os": 0.5804875385054281, "y": 0.8846830985915493, "term": "resource"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.7988556338028169, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.2755281690140845, "term": "/"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 4.284876248087372e-06, "x": 0.7992957746478874, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.2790492957746479, "term": "effectiveness"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 5.877777674712578e-06, "x": 0.7997359154929577, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.2878521126760563, "term": "sequences"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 1.7491407339311982e-07, "x": 0.8001760563380281, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.3283450704225352, "term": "good"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 1.0480334356727066e-06, "x": 0.8006161971830986, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.3961267605633803, "term": "strong"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 7.650283270266111e-07, "x": 0.801056338028169, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.4212147887323944, "term": "designed"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 7.0777207836939035e-06, "x": 0.8014964788732394, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.46434859154929575, "term": "challenging"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 2.156278290623862e-05, "x": 0.8019366197183099, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.5550176056338029, "term": "comprehension"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 1.2209429876856454e-06, "x": 0.8023767605633803, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.6016725352112676, "term": "japanese"}, {"ncat": 32, "cat": 0, "cat25k": 0, "bg": 4.583716204940816e-05, "x": 0.8028169014084507, "s": 0.13600352112676056, "ncat25k": 9, "os": 0.15871533049891606, "y": 0.8019366197183099, "term": "pun"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 1.3738524782687553e-07, "x": 0.8032570422535211, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.03873239436619718, "term": "people"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 3.4521479130936827e-07, "x": 0.8036971830985915, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.1073943661971831, "term": "class"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 1.4556789753219845e-06, "x": 0.804137323943662, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.22931338028169015, "term": "relevant"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 9.971187694166879e-08, "x": 0.8045774647887324, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.25, "term": "so"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 1.0041196289869254e-05, "x": 0.8050176056338029, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.2605633802816901, "term": "computational"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 2.101132244773222e-07, "x": 0.8054577464788732, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.28080985915492956, "term": "does"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 5.446211041271982e-07, "x": 0.8058978873239436, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.31514084507042256, "term": "problems"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 7.376117272217955e-06, "x": 0.8063380281690141, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.34639084507042256, "term": "generating"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 5.867564221446105e-07, "x": 0.8067781690140845, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.3939260563380282, "term": "track"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 0.00015904955839649127, "x": 0.8072183098591549, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.3956866197183099, "term": "baselines"}, {"ncat": 33, "cat": 1, "cat25k": 22, "bg": 1.1386374672241949e-06, "x": 0.8076584507042254, "s": 0.8939260563380282, "ncat25k": 9, "os": 0.5765178077416353, "y": 0.9137323943661971, "term": "tree"}, {"ncat": 33, "cat": 1, "cat25k": 22, "bg": 9.031232205650294e-07, "x": 0.8080985915492958, "s": 0.8939260563380282, "ncat25k": 9, "os": 0.5765178077416353, "y": 0.9225352112676056, "term": "selection"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 0.00048077623508501003, "x": 0.8085387323943662, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.47491197183098594, "term": "coreference"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 2.7290195253077278e-05, "x": 0.8089788732394366, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.5118838028169014, "term": "predicting"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 5.174903911443281e-06, "x": 0.809419014084507, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.5884683098591549, "term": "robot"}, {"ncat": 33, "cat": 0, "cat25k": 0, "bg": 1.4292662932946684e-06, "x": 0.8098591549295775, "s": 0.12984154929577466, "ncat25k": 9, "os": 0.15200775223221957, "y": 0.6329225352112676, "term": "frame"}, {"ncat": 34, "cat": 0, "cat25k": 0, "bg": 2.5957129548388126e-07, "x": 0.8102992957746479, "s": 0.12808098591549294, "ncat25k": 9, "os": 0.14552311464159867, "y": 0.1483274647887324, "term": "address"}, {"ncat": 34, "cat": 0, "cat25k": 0, "bg": 1.2555463761164945e-06, "x": 0.8107394366197183, "s": 0.12808098591549294, "ncat25k": 9, "os": 0.14552311464159867, "y": 0.18089788732394366, "term": "consider"}, {"ncat": 34, "cat": 0, "cat25k": 0, "bg": 1.21608280236267e-06, "x": 0.8111795774647887, "s": 0.12808098591549294, "ncat25k": 9, "os": 0.14552311464159867, "y": 0.207306338028169, "term": "output"}, {"ncat": 34, "cat": 0, "cat25k": 0, "bg": 1.0062522298475426e-05, "x": 0.8116197183098591, "s": 0.12808098591549294, "ncat25k": 9, "os": 0.14552311464159867, "y": 0.5466549295774648, "term": "graphs"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 1.3657531923993103e-06, "x": 0.8120598591549296, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.04005281690140845, "term": "sets"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 1.3443884650263592e-07, "x": 0.8125, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.056338028169014086, "term": "like"}, {"ncat": 35, "cat": 1, "cat25k": 22, "bg": 1.2149774317942046e-05, "x": 0.8129401408450704, "s": 0.8917253521126761, "ncat25k": 10, "os": 0.5691772702416992, "y": 0.8639964788732394, "term": "manually"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 9.41315690387822e-05, "x": 0.8133802816901409, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.1624119718309859, "term": "outperform"}, {"ncat": 35, "cat": 1, "cat25k": 22, "bg": 4.709756481648637e-07, "x": 0.8138204225352113, "s": 0.8917253521126761, "ncat25k": 10, "os": 0.5691772702416992, "y": 0.8772007042253521, "term": "application"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 3.66379163702921e-05, "x": 0.8142605633802817, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.2244718309859155, "term": "extracting"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 0.0001386083719456655, "x": 0.8147007042253521, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.2574823943661972, "term": "classifiers"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 2.7383712035074936e-06, "x": 0.8151408450704225, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.35651408450704225, "term": "structures"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 3.0378830086950286e-06, "x": 0.815580985915493, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.3692781690140845, "term": "linear"}, {"ncat": 35, "cat": 0, "cat25k": 0, "bg": 2.623163130018205e-05, "x": 0.8160211267605634, "s": 0.12455985915492958, "ncat25k": 10, "os": 0.13925663760850165, "y": 0.49119718309859156, "term": "learns"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 1.980693411052808e-06, "x": 0.8164612676056338, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.019806338028169015, "term": "interactive"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 0.0002703636390945822, "x": 0.8169014084507042, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.08230633802816902, "term": "tweet"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 9.978591624627115e-07, "x": 0.8173415492957746, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.09286971830985916, "term": "particular"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 1.1229206667466227e-05, "x": 0.8177816901408451, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.125, "term": "regression"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 4.748439042995374e-07, "x": 0.8182218309859155, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.14040492957746478, "term": "articles"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 5.646058209166318e-07, "x": 0.8186619718309859, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.1875, "term": "result"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 4.010357193046286e-06, "x": 0.8191021126760564, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.21170774647887325, "term": "improving"}, {"ncat": 36, "cat": 2, "cat25k": 44, "bg": 7.512974957178514e-06, "x": 0.8195422535211268, "s": 0.9348591549295775, "ncat25k": 10, "os": 0.6951160533934455, "y": 0.9524647887323944, "term": "investigate"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 6.186610070641205e-07, "x": 0.8199823943661971, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.30149647887323944, "term": "term"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 4.1912764448587287e-07, "x": 0.8204225352112676, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.33494718309859156, "term": "team"}, {"ncat": 36, "cat": 0, "cat25k": 0, "bg": 7.560651467314007e-07, "x": 0.820862676056338, "s": 0.12015845070422536, "ncat25k": 10, "os": 0.1332035376648914, "y": 0.36795774647887325, "term": "topics"}, {"ncat": 36, "cat": 2, "cat25k": 44, "bg": 9.83174418712355e-07, "x": 0.8213028169014085, "s": 0.9348591549295775, "ncat25k": 10, "os": 0.6951160533934455, "y": 0.9555457746478874, "term": "build"}, {"ncat": 37, "cat": 2, "cat25k": 44, "bg": 7.428156583712314e-07, "x": 0.8217429577464789, "s": 0.9317781690140846, "ncat25k": 10, "os": 0.6898314633447934, "y": 0.9375, "term": "due"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 2.4851224701096436e-07, "x": 0.8221830985915493, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.051056338028169015, "term": "real"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 2.299469552770263e-06, "x": 0.8226232394366197, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.12367957746478873, "term": "measure"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 4.129370038373566e-06, "x": 0.8230633802816901, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.13336267605633803, "term": "capture"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 1.0432867864824208e-06, "x": 0.8235035211267606, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.16549295774647887, "term": "pre"}, {"ncat": 37, "cat": 1, "cat25k": 22, "bg": 2.6700723673929635e-06, "x": 0.823943661971831, "s": 0.889524647887324, "ncat25k": 10, "os": 0.5625410669402449, "y": 0.8811619718309859, "term": "participation"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 4.878425033440285e-05, "x": 0.8243838028169014, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.2733274647887324, "term": "amr"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 2.8231759088480432e-06, "x": 0.8248239436619719, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.34110915492957744, "term": "specifically"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.8252640845070423, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.38600352112676056, "term": "10"}, {"ncat": 37, "cat": 0, "cat25k": 0, "bg": 2.8312857844392916e-06, "x": 0.8257042253521126, "s": 0.11663732394366197, "ncat25k": 10, "os": 0.1273590340687858, "y": 0.45422535211267606, "term": "represent"}, {"ncat": 38, "cat": 0, "cat25k": 0, "bg": 2.8136508595588605e-07, "x": 0.8261443661971831, "s": 0.11399647887323945, "ncat25k": 11, "os": 0.12171835443562301, "y": 0.0409330985915493, "term": "those"}, {"ncat": 38, "cat": 4, "cat25k": 89, "bg": 7.109087165279421e-06, "x": 0.8265845070422535, "s": 0.9656690140845071, "ncat25k": 11, "os": 0.8322725439733225, "y": 0.9779929577464789, "term": "identifying"}, {"ncat": 38, "cat": 1, "cat25k": 22, "bg": 3.7415557404118607e-07, "x": 0.827024647887324, "s": 0.8882042253521127, "ncat25k": 11, "os": 0.5594565653248083, "y": 0.8758802816901409, "term": "small"}, {"ncat": 38, "cat": 24, "cat25k": 533, "bg": 6.750162180811912e-06, "x": 0.8274647887323944, "s": 0.9982394366197184, "ncat25k": 11, "os": 0.9999246044576988, "y": 0.9977992957746479, "term": "parallel"}, {"ncat": 38, "cat": 0, "cat25k": 0, "bg": 9.269170654249653e-07, "x": 0.8279049295774648, "s": 0.11399647887323945, "ncat25k": 11, "os": 0.12171835443562301, "y": 0.332306338028169, "term": "base"}, {"ncat": 38, "cat": 1, "cat25k": 22, "bg": 0.00015628287199831694, "x": 0.8283450704225352, "s": 0.8882042253521127, "ncat25k": 11, "os": 0.5594565653248083, "y": 0.9005281690140845, "term": "distributional"}, {"ncat": 38, "cat": 0, "cat25k": 0, "bg": 4.167798872994281e-06, "x": 0.8287852112676056, "s": 0.11399647887323945, "ncat25k": 11, "os": 0.12171835443562301, "y": 0.386443661971831, "term": "generate"}, {"ncat": 38, "cat": 0, "cat25k": 0, "bg": 2.6751068634794413e-05, "x": 0.829225352112676, "s": 0.11399647887323945, "ncat25k": 11, "os": 0.12171835443562301, "y": 0.4352992957746479, "term": "inference"}, {"ncat": 38, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.8296654929577465, "s": 0.11399647887323945, "ncat25k": 11, "os": 0.12171835443562301, "y": 0.5013204225352113, "term": "f1"}, {"ncat": 39, "cat": 1, "cat25k": 22, "bg": 2.0760173185516747e-06, "x": 0.8301056338028169, "s": 0.886443661971831, "ncat25k": 11, "os": 0.5565136915498996, "y": 0.8507922535211268, "term": "generated"}, {"ncat": 39, "cat": 0, "cat25k": 0, "bg": 1.2302183258999757e-05, "x": 0.8305457746478874, "s": 0.11047535211267606, "ncat25k": 11, "os": 0.11627673995516824, "y": 0.10651408450704225, "term": "predict"}, {"ncat": 39, "cat": 0, "cat25k": 0, "bg": 1.834732689444089e-06, "x": 0.8309859154929577, "s": 0.11047535211267606, "ncat25k": 11, "os": 0.11627673995516824, "y": 0.13996478873239437, "term": "scientific"}, {"ncat": 39, "cat": 0, "cat25k": 0, "bg": 1.5324666016586475e-06, "x": 0.8314260563380281, "s": 0.11047535211267606, "ncat25k": 11, "os": 0.11627673995516824, "y": 0.1892605633802817, "term": "answers"}, {"ncat": 39, "cat": 0, "cat25k": 0, "bg": 2.559349174567235e-06, "x": 0.8318661971830986, "s": 0.11047535211267606, "ncat25k": 11, "os": 0.11627673995516824, "y": 0.21963028169014084, "term": "perform"}, {"ncat": 39, "cat": 0, "cat25k": 0, "bg": 0.00022770930280082438, "x": 0.832306338028169, "s": 0.11047535211267606, "ncat25k": 11, "os": 0.11627673995516824, "y": 0.2319542253521127, "term": "summarization"}, {"ncat": 39, "cat": 0, "cat25k": 0, "bg": 9.020467440622774e-06, "x": 0.8327464788732394, "s": 0.11047535211267606, "ncat25k": 11, "os": 0.11627673995516824, "y": 0.31382042253521125, "term": "structured"}, {"ncat": 39, "cat": 1, "cat25k": 22, "bg": 1.2959042282714563e-06, "x": 0.8331866197183099, "s": 0.886443661971831, "ncat25k": 11, "os": 0.5565136915498996, "y": 0.8952464788732394, "term": "recently"}, {"ncat": 39, "cat": 0, "cat25k": 0, "bg": 1.1433151389875446e-05, "x": 0.8336267605633803, "s": 0.11047535211267606, "ncat25k": 11, "os": 0.11627673995516824, "y": 0.5303697183098591, "term": "combining"}, {"ncat": 40, "cat": 0, "cat25k": 0, "bg": 3.67719047938613e-05, "x": 0.8340669014084507, "s": 0.10871478873239437, "ncat25k": 11, "os": 0.11102945022260513, "y": 0.037852112676056336, "term": "annotations"}, {"ncat": 40, "cat": 0, "cat25k": 0, "bg": 1.3644484430364094e-05, "x": 0.8345070422535211, "s": 0.10871478873239437, "ncat25k": 11, "os": 0.11102945022260513, "y": 0.05941901408450704, "term": "labeled"}, {"ncat": 40, "cat": 1, "cat25k": 22, "bg": 2.1984793600859314e-06, "x": 0.8349471830985915, "s": 0.8851232394366197, "ncat25k": 11, "os": 0.5537030218372229, "y": 0.860475352112676, "term": "rich"}, {"ncat": 40, "cat": 0, "cat25k": 0, "bg": 8.386886750316637e-07, "x": 0.835387323943662, "s": 0.10871478873239437, "ncat25k": 11, "os": 0.11102945022260513, "y": 0.4159330985915493, "term": "reading"}, {"ncat": 40, "cat": 0, "cat25k": 0, "bg": 1.5978682839224191e-06, "x": 0.8358274647887324, "s": 0.10871478873239437, "ncat25k": 11, "os": 0.11102945022260513, "y": 0.5638204225352113, "term": "visual"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 7.597036084930083e-07, "x": 0.8362676056338029, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.007042253521126761, "term": "common"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 3.165122267129217e-06, "x": 0.8367077464788732, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.009242957746478873, "term": "patterns"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 3.222018361889715e-06, "x": 0.8371478873239436, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.09991197183098592, "term": "competitive"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 5.278333849891253e-07, "x": 0.8375880281690141, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.13468309859154928, "term": "medical"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 2.2024482952432436e-06, "x": 0.8380281690140845, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.17253521126760563, "term": "improvement"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 7.489501408729548e-07, "x": 0.8384683098591549, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.20246478873239437, "term": "able"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 2.6129737524555977e-06, "x": 0.8389084507042254, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.20994718309859156, "term": "query"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 1.7102876576611469e-06, "x": 0.8393485915492958, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.21742957746478872, "term": "traditional"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.8397887323943662, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.2962147887323944, "term": "e.g."}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 5.833086002187734e-07, "x": 0.8402288732394366, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.6126760563380281, "term": "memory"}, {"ncat": 41, "cat": 0, "cat25k": 0, "bg": 0.0005227458180334557, "x": 0.840669014084507, "s": 0.10299295774647887, "ncat25k": 11, "os": 0.10597176771129307, "y": 0.7764084507042254, "term": "keyphrases"}, {"ncat": 42, "cat": 2, "cat25k": 44, "bg": 5.519284448855356e-07, "x": 0.8411091549295775, "s": 0.9247359154929577, "ncat25k": 12, "os": 0.666331434485902, "y": 0.9401408450704225, "term": "second"}, {"ncat": 42, "cat": 1, "cat25k": 22, "bg": 1.5666928046893054e-07, "x": 0.8415492957746479, "s": 0.8833626760563381, "ncat25k": 12, "os": 0.5484445536995297, "y": 0.8411091549295775, "term": "some"}, {"ncat": 42, "cat": 0, "cat25k": 0, "bg": 1.6664216630054984e-05, "x": 0.8419894366197183, "s": 0.0994718309859155, "ncat25k": 12, "os": 0.10109900191340976, "y": 0.11795774647887323, "term": "improves"}, {"ncat": 42, "cat": 1, "cat25k": 22, "bg": 1.0129302191429772e-06, "x": 0.8424295774647887, "s": 0.8833626760563381, "ncat25k": 12, "os": 0.5484445536995297, "y": 0.8595950704225352, "term": "shows"}, {"ncat": 42, "cat": 0, "cat25k": 0, "bg": 2.96348551901621e-06, "x": 0.8428697183098591, "s": 0.0994718309859155, "ncat25k": 12, "os": 0.10109900191340976, "y": 0.14348591549295775, "term": "combination"}, {"ncat": 42, "cat": 0, "cat25k": 0, "bg": 3.869457512283201e-07, "x": 0.8433098591549296, "s": 0.0994718309859155, "ncat25k": 12, "os": 0.10109900191340976, "y": 0.18705985915492956, "term": "current"}, {"ncat": 42, "cat": 0, "cat25k": 0, "bg": 1.2781386406113032e-05, "x": 0.84375, "s": 0.0994718309859155, "ncat25k": 12, "os": 0.10109900191340976, "y": 0.2007042253521127, "term": "participated"}, {"ncat": 42, "cat": 1, "cat25k": 22, "bg": 1.9598450537384955e-05, "x": 0.8441901408450704, "s": 0.8833626760563381, "ncat25k": 12, "os": 0.5484445536995297, "y": 0.8851232394366197, "term": "semantics"}, {"ncat": 42, "cat": 0, "cat25k": 0, "bg": 9.066475038801545e-07, "x": 0.8446302816901409, "s": 0.0994718309859155, "ncat25k": 12, "os": 0.10109900191340976, "y": 0.2535211267605634, "term": "often"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 1.1695581953118668e-06, "x": 0.8450704225352113, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.0030809859154929575, "term": "addition"}, {"ncat": 43, "cat": 1, "cat25k": 22, "bg": 1.237973976132638e-07, "x": 0.8455105633802817, "s": 0.8829225352112677, "ncat25k": 12, "os": 0.5459816214415713, "y": 0.8318661971830986, "term": "any"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 4.2681436428854135e-07, "x": 0.8459507042253521, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.01716549295774648, "term": "form"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 2.7011952286338755e-06, "x": 0.8463908450704225, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.02244718309859155, "term": "explore"}, {"ncat": 43, "cat": 4, "cat25k": 89, "bg": 4.236126943385201e-06, "x": 0.846830985915493, "s": 0.9573063380281691, "ncat25k": 12, "os": 0.8048052073135581, "y": 0.9788732394366197, "term": "presents"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 4.812758780220837e-06, "x": 0.8472711267605634, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.12059859154929578, "term": "mechanism"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 1.0387482194979827e-07, "x": 0.8477112676056338, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.1307218309859155, "term": "may"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 1.4279931150806368e-05, "x": 0.8481514084507042, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.1386443661971831, "term": "benchmark"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 8.048455445248042e-07, "x": 0.8485915492957746, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.1756161971830986, "term": "short"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 3.749252438300166e-06, "x": 0.8490316901408451, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.17649647887323944, "term": "concepts"}, {"ncat": 43, "cat": 2, "cat25k": 44, "bg": 4.978193025452007e-06, "x": 0.8494718309859155, "s": 0.9242957746478874, "ncat25k": 12, "os": 0.6621460272797216, "y": 0.9511443661971831, "term": "domains"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 1.3084964478125212e-06, "x": 0.8499119718309859, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.20202464788732394, "term": "focus"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 8.59793907400396e-06, "x": 0.8503521126760564, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.21786971830985916, "term": "supervision"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 1.1655239111296148e-06, "x": 0.8507922535211268, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.24911971830985916, "term": "role"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 2.514832982046256e-06, "x": 0.8512323943661971, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.27024647887323944, "term": "named"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 7.211838316455047e-07, "x": 0.8516725352112676, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.2948943661971831, "term": "similar"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 2.47583448793107e-06, "x": 0.852112676056338, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.3257042253521127, "term": "compared"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 4.650834893003433e-07, "x": 0.8525528169014085, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.3714788732394366, "term": "low"}, {"ncat": 43, "cat": 0, "cat25k": 0, "bg": 3.0254450482435363e-05, "x": 0.8529929577464789, "s": 0.09154929577464789, "ncat25k": 12, "os": 0.09640649317340433, "y": 0.43882042253521125, "term": "encoder"}, {"ncat": 44, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.8534330985915493, "s": 0.08582746478873239, "ncat25k": 12, "os": 0.09188961623782571, "y": 0.08846830985915492, "term": "'"}, {"ncat": 44, "cat": 0, "cat25k": 0, "bg": 9.465529928230846e-06, "x": 0.8538732394366197, "s": 0.08582746478873239, "ncat25k": 12, "os": 0.09188961623782571, "y": 0.11839788732394366, "term": "cnn"}, {"ncat": 44, "cat": 0, "cat25k": 0, "bg": 2.0959205020021123e-07, "x": 0.8543133802816901, "s": 0.08582746478873239, "ncat25k": 12, "os": 0.09188961623782571, "y": 0.19982394366197184, "term": "b"}, {"ncat": 44, "cat": 0, "cat25k": 0, "bg": 2.821821060168947e-07, "x": 0.8547535211267606, "s": 0.08582746478873239, "ncat25k": 12, "os": 0.09188961623782571, "y": 0.22095070422535212, "term": "general"}, {"ncat": 44, "cat": 0, "cat25k": 0, "bg": 1.0601554706191386e-07, "x": 0.855193661971831, "s": 0.08582746478873239, "ncat25k": 12, "os": 0.09188961623782571, "y": 0.24779929577464788, "term": "up"}, {"ncat": 44, "cat": 0, "cat25k": 0, "bg": 2.6267015851221734e-07, "x": 0.8556338028169014, "s": 0.08582746478873239, "ncat25k": 12, "os": 0.09188961623782571, "y": 0.25440140845070425, "term": "very"}, {"ncat": 44, "cat": 2, "cat25k": 44, "bg": 4.998762262996184e-06, "x": 0.8560739436619719, "s": 0.9220950704225352, "ncat25k": 12, "os": 0.6581117127100288, "y": 0.9551056338028169, "term": "statistical"}, {"ncat": 44, "cat": 0, "cat25k": 0, "bg": 6.307117940955342e-05, "x": 0.8565140845070423, "s": 0.08582746478873239, "ncat25k": 12, "os": 0.09188961623782571, "y": 0.5123239436619719, "term": "polarity"}, {"ncat": 45, "cat": 1, "cat25k": 22, "bg": 8.318515224600816e-06, "x": 0.8569542253521126, "s": 0.8802816901408451, "ncat25k": 12, "os": 0.5413550233736374, "y": 0.8283450704225352, "term": "extract"}, {"ncat": 45, "cat": 4, "cat25k": 89, "bg": 1.7168972341104412e-07, "x": 0.8573943661971831, "s": 0.9568661971830986, "ncat25k": 12, "os": 0.7946785266682339, "y": 0.9735915492957746, "term": "were"}, {"ncat": 45, "cat": 1, "cat25k": 22, "bg": 3.038039584229225e-07, "x": 0.8578345070422535, "s": 0.8802816901408451, "ncat25k": 12, "os": 0.5413550233736374, "y": 0.840669014084507, "term": "part"}, {"ncat": 45, "cat": 2, "cat25k": 44, "bg": 1.0899202907666163e-06, "x": 0.858274647887324, "s": 0.9212147887323944, "ncat25k": 12, "os": 0.6542208216189574, "y": 0.9414612676056338, "term": "step"}, {"ncat": 45, "cat": 2, "cat25k": 44, "bg": 2.7989668834575702e-06, "x": 0.8587147887323944, "s": 0.9212147887323944, "ncat25k": 12, "os": 0.6542208216189574, "y": 0.9546654929577465, "term": "obtained"}, {"ncat": 45, "cat": 1, "cat25k": 22, "bg": 9.25350906390768e-07, "x": 0.8591549295774648, "s": 0.8802816901408451, "ncat25k": 12, "os": 0.5413550233736374, "y": 0.903169014084507, "term": "applications"}, {"ncat": 45, "cat": 0, "cat25k": 0, "bg": 9.260263407163697e-06, "x": 0.8595950704225352, "s": 0.08494718309859156, "ncat25k": 12, "os": 0.087543783543728, "y": 0.5858274647887324, "term": "ranked"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 1.5297266967005226e-07, "x": 0.8600352112676056, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.061619718309859156, "term": "online"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 3.3769594899205465e-07, "x": 0.860475352112676, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.09815140845070422, "term": "type"}, {"ncat": 46, "cat": 4, "cat25k": 89, "bg": 1.5495494468046493e-06, "x": 0.8609154929577465, "s": 0.9533450704225352, "ncat25k": 13, "os": 0.7897911352455795, "y": 0.9784330985915493, "term": "documents"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 4.868290104539968e-07, "x": 0.8613556338028169, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.11003521126760564, "term": "content"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 3.721753609393059e-05, "x": 0.8617957746478874, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.14216549295774647, "term": "tagging"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 2.485419636900723e-06, "x": 0.8622359154929577, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.14700704225352113, "term": "challenge"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 8.858096945708916e-06, "x": 0.8626760563380281, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.1742957746478873, "term": "evaluated"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 9.675560748148558e-08, "x": 0.8631161971830986, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.1949823943661972, "term": "do"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 1.878173500768418e-05, "x": 0.863556338028169, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.198943661971831, "term": "qa"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 5.199892398400517e-07, "x": 0.8639964788732394, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.2442781690140845, "term": "process"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 3.512299258830793e-07, "x": 0.8644366197183099, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.3926056338028169, "term": "community"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 3.9622920722632496e-06, "x": 0.8648767605633803, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.4581866197183099, "term": "learned"}, {"ncat": 46, "cat": 0, "cat25k": 0, "bg": 6.198299027746941e-07, "x": 0.8653169014084507, "s": 0.0779049295774648, "ncat25k": 13, "os": 0.08336444826642969, "y": 0.5136443661971831, "term": "financial"}, {"ncat": 47, "cat": 1, "cat25k": 22, "bg": 2.6702505879975237e-06, "x": 0.8657570422535211, "s": 0.878080985915493, "ncat25k": 13, "os": 0.5370889386086656, "y": 0.8349471830985915, "term": "identify"}, {"ncat": 47, "cat": 1, "cat25k": 22, "bg": 5.099554187411273e-06, "x": 0.8661971830985915, "s": 0.878080985915493, "ncat25k": 13, "os": 0.5370889386086656, "y": 0.8763204225352113, "term": "improvements"}, {"ncat": 47, "cat": 1, "cat25k": 22, "bg": 2.2236432950020462e-06, "x": 0.866637323943662, "s": 0.878080985915493, "ncat25k": 13, "os": 0.5370889386086656, "y": 0.8829225352112676, "term": "goal"}, {"ncat": 47, "cat": 1, "cat25k": 22, "bg": 1.515439391232842e-06, "x": 0.8670774647887324, "s": 0.878080985915493, "ncat25k": 13, "os": 0.5370889386086656, "y": 0.8961267605633803, "term": "developed"}, {"ncat": 47, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.8675176056338029, "s": 0.07570422535211269, "ncat25k": 13, "os": 0.07934710714603749, "y": 0.5730633802816901, "term": "5"}, {"ncat": 48, "cat": 3, "cat25k": 67, "bg": 4.1497951219703506e-07, "x": 0.8679577464788732, "s": 0.9392605633802817, "ncat25k": 13, "os": 0.7187792304735754, "y": 0.9647887323943662, "term": "even"}, {"ncat": 48, "cat": 1, "cat25k": 22, "bg": 3.9146340545161134e-07, "x": 0.8683978873239436, "s": 0.8767605633802817, "ncat25k": 13, "os": 0.5350782742387398, "y": 0.8661971830985915, "term": "code"}, {"ncat": 48, "cat": 0, "cat25k": 0, "bg": 6.767078265743335e-07, "x": 0.8688380281690141, "s": 0.07350352112676056, "ncat25k": 13, "os": 0.07548730311071783, "y": 0.19014084507042253, "term": "recent"}, {"ncat": 49, "cat": 1, "cat25k": 22, "bg": 1.4259888202191297e-07, "x": 0.8692781690140845, "s": 0.8736795774647887, "ncat25k": 14, "os": 0.5331431173675663, "y": 0.835387323943662, "term": "there"}, {"ncat": 49, "cat": 2, "cat25k": 44, "bg": 4.692541937914376e-07, "x": 0.8697183098591549, "s": 0.9159330985915494, "ncat25k": 14, "os": 0.6399541799812294, "y": 0.9419014084507042, "term": "resources"}, {"ncat": 49, "cat": 1, "cat25k": 22, "bg": 4.2567655435261725e-07, "x": 0.8701584507042254, "s": 0.8736795774647887, "ncat25k": 14, "os": 0.5331431173675663, "y": 0.8534330985915493, "term": "same"}, {"ncat": 49, "cat": 0, "cat25k": 0, "bg": 5.035353835855481e-06, "x": 0.8705985915492958, "s": 0.07262323943661973, "ncat25k": 14, "os": 0.07178062771333249, "y": 0.32130281690140844, "term": "experimental"}, {"ncat": 49, "cat": 1, "cat25k": 22, "bg": 0.0, "x": 0.8710387323943662, "s": 0.8736795774647887, "ncat25k": 14, "os": 0.5331431173675663, "y": 0.9322183098591549, "term": "2"}, {"ncat": 50, "cat": 0, "cat25k": 0, "bg": 2.219372892663825e-06, "x": 0.8714788732394366, "s": 0.0682218309859155, "ncat25k": 14, "os": 0.0682227233966915, "y": 0.04533450704225352, "term": "applied"}, {"ncat": 50, "cat": 2, "cat25k": 44, "bg": 1.0692471713400223e-06, "x": 0.871919014084507, "s": 0.9154929577464789, "ncat25k": 14, "os": 0.6366810052992359, "y": 0.9476232394366197, "term": "via"}, {"ncat": 50, "cat": 0, "cat25k": 0, "bg": 7.415549790937854e-07, "x": 0.8723591549295775, "s": 0.0682218309859155, "ncat25k": 14, "os": 0.0682227233966915, "y": 0.19454225352112675, "term": "single"}, {"ncat": 50, "cat": 1, "cat25k": 22, "bg": 4.254114136055504e-07, "x": 0.8727992957746479, "s": 0.8723591549295775, "ncat25k": 14, "os": 0.5312793230312589, "y": 0.8727992957746479, "term": "open"}, {"ncat": 51, "cat": 0, "cat25k": 0, "bg": 3.363752206225712e-05, "x": 0.8732394366197183, "s": 0.06602112676056339, "ncat25k": 14, "os": 0.06480928560133598, "y": 0.008802816901408451, "term": "detecting"}, {"ncat": 51, "cat": 1, "cat25k": 22, "bg": 1.7797188273562704e-06, "x": 0.8736795774647887, "s": 0.8710387323943662, "ncat25k": 14, "os": 0.5294830419467692, "y": 0.8331866197183099, "term": "significant"}, {"ncat": 51, "cat": 0, "cat25k": 0, "bg": 2.0610669068388867e-06, "x": 0.8741197183098591, "s": 0.06602112676056339, "ncat25k": 14, "os": 0.06480928560133598, "y": 0.07262323943661972, "term": "understanding"}, {"ncat": 51, "cat": 0, "cat25k": 0, "bg": 3.5311834657527136e-05, "x": 0.8745598591549296, "s": 0.06602112676056339, "ncat25k": 14, "os": 0.06480928560133598, "y": 0.534330985915493, "term": "decoder"}, {"ncat": 52, "cat": 0, "cat25k": 0, "bg": 1.4021852698795225e-07, "x": 0.875, "s": 0.06470070422535212, "ncat25k": 14, "os": 0.06153606472848583, "y": 0.05413732394366197, "term": "out"}, {"ncat": 52, "cat": 0, "cat25k": 0, "bg": 3.7436536476467493e-07, "x": 0.8754401408450704, "s": 0.06470070422535212, "ncat25k": 14, "os": 0.06153606472848583, "y": 0.05677816901408451, "term": "terms"}, {"ncat": 52, "cat": 1, "cat25k": 22, "bg": 4.94649790864635e-07, "x": 0.8758802816901409, "s": 0.869718309859155, "ncat25k": 14, "os": 0.527750694817012, "y": 0.8824823943661971, "term": "including"}, {"ncat": 52, "cat": 1, "cat25k": 22, "bg": 5.851873771637868e-07, "x": 0.8763204225352113, "s": 0.869718309859155, "ncat25k": 14, "os": 0.527750694817012, "y": 0.9058098591549296, "term": "provide"}, {"ncat": 52, "cat": 2, "cat25k": 44, "bg": 0.0, "x": 0.8767605633802817, "s": 0.9128521126760564, "ncat25k": 14, "os": 0.6304488347470947, "y": 0.9612676056338029, "term": "3"}, {"ncat": 53, "cat": 0, "cat25k": 0, "bg": 2.0335421263119345e-06, "x": 0.8772007042253521, "s": 0.06073943661971831, "ncat25k": 15, "os": 0.058398867969513824, "y": 0.08538732394366197, "term": "character"}, {"ncat": 53, "cat": 0, "cat25k": 0, "bg": 2.837182118535504e-07, "x": 0.8776408450704225, "s": 0.06073943661971831, "ncat25k": 15, "os": 0.058398867969513824, "y": 0.09066901408450705, "term": "support"}, {"ncat": 53, "cat": 1, "cat25k": 22, "bg": 1.419961671290154e-06, "x": 0.878080985915493, "s": 0.8688380281690141, "ncat25k": 15, "os": 0.5260789492467121, "y": 0.8538732394366197, "term": "effective"}, {"ncat": 53, "cat": 0, "cat25k": 0, "bg": 3.321370238185173e-06, "x": 0.8785211267605634, "s": 0.06073943661971831, "ncat25k": 15, "os": 0.058398867969513824, "y": 0.11179577464788733, "term": "train"}, {"ncat": 53, "cat": 1, "cat25k": 22, "bg": 2.3913936400000444e-05, "x": 0.8789612676056338, "s": 0.8688380281690141, "ncat25k": 15, "os": 0.5260789492467121, "y": 0.8688380281690141, "term": "biomedical"}, {"ncat": 53, "cat": 9, "cat25k": 200, "bg": 3.988877850491519e-06, "x": 0.8794014084507042, "s": 0.9867957746478874, "ncat25k": 15, "os": 0.9277420521413791, "y": 0.9898767605633803, "term": "shared"}, {"ncat": 53, "cat": 0, "cat25k": 0, "bg": 2.683064748427243e-05, "x": 0.8798415492957746, "s": 0.06073943661971831, "ncat25k": 15, "os": 0.058398867969513824, "y": 0.3591549295774648, "term": "parser"}, {"ncat": 53, "cat": 0, "cat25k": 0, "bg": 4.1960728906313873e-07, "x": 0.8802816901408451, "s": 0.06073943661971831, "ncat25k": 15, "os": 0.058398867969513824, "y": 0.38380281690140844, "term": "long"}, {"ncat": 53, "cat": 0, "cat25k": 0, "bg": 9.826479209580261e-05, "x": 0.8807218309859155, "s": 0.06073943661971831, "ncat25k": 15, "os": 0.058398867969513824, "y": 0.5294894366197183, "term": "grained"}, {"ncat": 54, "cat": 0, "cat25k": 0, "bg": 4.670774390737377e-07, "x": 0.8811619718309859, "s": 0.058978873239436624, "ncat25k": 15, "os": 0.05539356101212073, "y": 0.018926056338028168, "term": "related"}, {"ncat": 54, "cat": 0, "cat25k": 0, "bg": 9.415124613097224e-06, "x": 0.8816021126760564, "s": 0.058978873239436624, "ncat25k": 15, "os": 0.05539356101212073, "y": 0.1835387323943662, "term": "entities"}, {"ncat": 55, "cat": 0, "cat25k": 0, "bg": 1.0943446457397463e-05, "x": 0.8820422535211268, "s": 0.05721830985915493, "ncat25k": 15, "os": 0.052516069632214446, "y": 0.007922535211267605, "term": "arabic"}, {"ncat": 55, "cat": 0, "cat25k": 0, "bg": 0.0011273957158962795, "x": 0.8824823943661971, "s": 0.05721830985915493, "ncat25k": 15, "os": 0.052516069632214446, "y": 0.43838028169014087, "term": "lstm"}, {"ncat": 55, "cat": 0, "cat25k": 0, "bg": 4.391143940899993e-05, "x": 0.8829225352112676, "s": 0.05721830985915493, "ncat25k": 15, "os": 0.052516069632214446, "y": 0.5158450704225352, "term": "multilingual"}, {"ncat": 56, "cat": 0, "cat25k": 0, "bg": 1.0216581307129845e-06, "x": 0.883362676056338, "s": 0.05325704225352113, "ncat25k": 15, "os": 0.04976238117942111, "y": 0.05765845070422535, "term": "further"}, {"ncat": 56, "cat": 0, "cat25k": 0, "bg": 3.872912854412158e-06, "x": 0.8838028169014085, "s": 0.05325704225352113, "ncat25k": 15, "os": 0.04976238117942111, "y": 0.0897887323943662, "term": "meaning"}, {"ncat": 56, "cat": 3, "cat25k": 67, "bg": 2.7410628285282853e-06, "x": 0.8842429577464789, "s": 0.9313380281690142, "ncat25k": 15, "os": 0.6882844570530623, "y": 0.9683098591549296, "term": "clinical"}, {"ncat": 56, "cat": 1, "cat25k": 22, "bg": 3.385514214424436e-07, "x": 0.8846830985915493, "s": 0.8657570422535212, "ncat25k": 15, "os": 0.5213972792928132, "y": 0.8816021126760564, "term": "order"}, {"ncat": 56, "cat": 0, "cat25k": 0, "bg": 4.578885613713762e-05, "x": 0.8851232394366197, "s": 0.05325704225352113, "ncat25k": 15, "os": 0.04976238117942111, "y": 0.2913732394366197, "term": "segmentation"}, {"ncat": 56, "cat": 0, "cat25k": 0, "bg": 5.0472829011529755e-06, "x": 0.8855633802816901, "s": 0.05325704225352113, "ncat25k": 15, "os": 0.04976238117942111, "y": 0.3191021126760563, "term": "recognition"}, {"ncat": 56, "cat": 0, "cat25k": 0, "bg": 2.1030458676181448e-05, "x": 0.8860035211267606, "s": 0.05325704225352113, "ncat25k": 15, "os": 0.04976238117942111, "y": 0.39744718309859156, "term": "temporal"}, {"ncat": 57, "cat": 2, "cat25k": 44, "bg": 7.897318370511767e-07, "x": 0.886443661971831, "s": 0.909330985915493, "ncat25k": 16, "os": 0.6164897073374636, "y": 0.9370598591549296, "term": "users"}, {"ncat": 57, "cat": 3, "cat25k": 67, "bg": 8.148034191351436e-07, "x": 0.8868838028169014, "s": 0.9308978873239436, "ncat25k": 16, "os": 0.684922640504455, "y": 0.9674295774647887, "term": "standard"}, {"ncat": 57, "cat": 0, "cat25k": 0, "bg": 4.754470985653134e-06, "x": 0.8873239436619719, "s": 0.05237676056338028, "ncat25k": 16, "os": 0.04712854596309063, "y": 0.12808098591549297, "term": "identification"}, {"ncat": 57, "cat": 0, "cat25k": 0, "bg": 2.4566266404609257e-06, "x": 0.8877640845070423, "s": 0.05237676056338028, "ncat25k": 16, "os": 0.04712854596309063, "y": 0.20158450704225353, "term": "scale"}, {"ncat": 58, "cat": 0, "cat25k": 0, "bg": 2.87300638124438e-06, "x": 0.8882042253521126, "s": 0.04973591549295776, "ncat25k": 16, "os": 0.04461067854470685, "y": 0.04709507042253521, "term": "techniques"}, {"ncat": 58, "cat": 1, "cat25k": 22, "bg": 2.349939383114465e-07, "x": 0.8886443661971831, "s": 0.8644366197183099, "ncat25k": 16, "os": 0.5185274403639591, "y": 0.8547535211267606, "term": "find"}, {"ncat": 58, "cat": 0, "cat25k": 0, "bg": 0.00020619106446137026, "x": 0.8890845070422535, "s": 0.04973591549295776, "ncat25k": 16, "os": 0.04461067854470685, "y": 0.15360915492957747, "term": "unsupervised"}, {"ncat": 58, "cat": 0, "cat25k": 0, "bg": 2.1434630935383273e-05, "x": 0.889524647887324, "s": 0.04973591549295776, "ncat25k": 16, "os": 0.04461067854470685, "y": 0.17781690140845072, "term": "adaptation"}, {"ncat": 58, "cat": 0, "cat25k": 0, "bg": 5.5880941607354574e-06, "x": 0.8899647887323944, "s": 0.04973591549295776, "ncat25k": 16, "os": 0.04461067854470685, "y": 0.2794894366197183, "term": "significantly"}, {"ncat": 58, "cat": 3, "cat25k": 67, "bg": 5.2778228651898674e-06, "x": 0.8904049295774648, "s": 0.9273767605633804, "ncat25k": 16, "os": 0.681648812297774, "y": 0.9705105633802817, "term": "scores"}, {"ncat": 58, "cat": 0, "cat25k": 0, "bg": 0.0004287388056667443, "x": 0.8908450704225352, "s": 0.04973591549295776, "ncat25k": 16, "os": 0.04461067854470685, "y": 0.621919014084507, "term": "nmt"}, {"ncat": 59, "cat": 1, "cat25k": 22, "bg": 8.810506058581202e-07, "x": 0.8912852112676056, "s": 0.8631161971830986, "ncat25k": 16, "os": 0.5171607723246054, "y": 0.8560739436619719, "term": "important"}, {"ncat": 59, "cat": 0, "cat25k": 0, "bg": 5.860936040757308e-07, "x": 0.891725352112676, "s": 0.04577464788732395, "ncat25k": 16, "os": 0.042204958941691684, "y": 0.13600352112676056, "term": "events"}, {"ncat": 59, "cat": 0, "cat25k": 0, "bg": 6.866891248926904e-06, "x": 0.8921654929577465, "s": 0.04577464788732395, "ncat25k": 16, "os": 0.042204958941691684, "y": 0.21830985915492956, "term": "achieved"}, {"ncat": 59, "cat": 0, "cat25k": 0, "bg": 1.9222376670941745e-05, "x": 0.8926056338028169, "s": 0.04577464788732395, "ncat25k": 16, "os": 0.042204958941691684, "y": 0.3125, "term": "ensemble"}, {"ncat": 59, "cat": 0, "cat25k": 0, "bg": 3.031499515049995e-06, "x": 0.8930457746478874, "s": 0.04577464788732395, "ncat25k": 16, "os": 0.042204958941691684, "y": 0.3358274647887324, "term": "joint"}, {"ncat": 59, "cat": 0, "cat25k": 0, "bg": 8.01541404877923e-06, "x": 0.8934859154929577, "s": 0.04577464788732395, "ncat25k": 16, "os": 0.042204958941691684, "y": 0.4529049295774648, "term": "ranking"}, {"ncat": 60, "cat": 1, "cat25k": 22, "bg": 7.866036853298212e-07, "x": 0.8939260563380281, "s": 0.8622359154929579, "ncat25k": 17, "os": 0.5158367784823175, "y": 0.8389084507042254, "term": "test"}, {"ncat": 60, "cat": 0, "cat25k": 0, "bg": 2.7793070400078927e-05, "x": 0.8943661971830986, "s": 0.04445422535211268, "ncat25k": 17, "os": 0.0399076337467385, "y": 0.032130281690140844, "term": "discourse"}, {"ncat": 60, "cat": 5, "cat25k": 111, "bg": 2.315013627327717e-06, "x": 0.894806338028169, "s": 0.9529049295774649, "ncat25k": 17, "os": 0.7803557670294844, "y": 0.9819542253521126, "term": "chinese"}, {"ncat": 60, "cat": 1, "cat25k": 22, "bg": 1.0005495641499483e-05, "x": 0.8952464788732394, "s": 0.8622359154929579, "ncat25k": 17, "os": 0.5158367784823175, "y": 0.866637323943662, "term": "texts"}, {"ncat": 60, "cat": 0, "cat25k": 0, "bg": 1.6650478701262659e-06, "x": 0.8956866197183099, "s": 0.04445422535211268, "ncat25k": 17, "os": 0.0399076337467385, "y": 0.2068661971830986, "term": "fine"}, {"ncat": 61, "cat": 0, "cat25k": 0, "bg": 9.94431524179657e-08, "x": 0.8961267605633803, "s": 0.04313380281690141, "ncat25k": 17, "os": 0.037715017166056686, "y": 0.19850352112676056, "term": "about"}, {"ncat": 61, "cat": 1, "cat25k": 22, "bg": 1.2107120344419846e-07, "x": 0.8965669014084507, "s": 0.8617957746478874, "ncat25k": 17, "os": 0.5145535000728788, "y": 0.8794014084507042, "term": "search"}, {"ncat": 62, "cat": 1, "cat25k": 22, "bg": 5.819052731369138e-07, "x": 0.8970070422535211, "s": 0.8600352112676057, "ncat25k": 17, "os": 0.5133090958289677, "y": 0.8274647887323944, "term": "media"}, {"ncat": 62, "cat": 1, "cat25k": 22, "bg": 5.098297399305895e-06, "x": 0.8974471830985915, "s": 0.8600352112676057, "ncat25k": 17, "os": 0.5133090958289677, "y": 0.855193661971831, "term": "describe"}, {"ncat": 63, "cat": 0, "cat25k": 0, "bg": 0.00035168123166582466, "x": 0.897887323943662, "s": 0.04181338028169015, "ncat25k": 17, "os": 0.03362951042235779, "y": 0.11751760563380281, "term": "outperforms"}, {"ncat": 63, "cat": 0, "cat25k": 0, "bg": 2.6829313631418506e-06, "x": 0.8983274647887324, "s": 0.04181338028169015, "ncat25k": 17, "os": 0.03362951042235779, "y": 0.13556338028169015, "term": "complex"}, {"ncat": 63, "cat": 0, "cat25k": 0, "bg": 5.745876753141671e-07, "x": 0.8987676056338029, "s": 0.04181338028169015, "ncat25k": 17, "os": 0.03362951042235779, "y": 0.23943661971830985, "term": "without"}, {"ncat": 64, "cat": 0, "cat25k": 0, "bg": 0.00010377766931569977, "x": 0.8992077464788732, "s": 0.040492957746478875, "ncat25k": 18, "os": 0.03172959499700456, "y": 0.04181338028169014, "term": "nlp"}, {"ncat": 64, "cat": 2, "cat25k": 44, "bg": 3.6609117134531186e-07, "x": 0.8996478873239436, "s": 0.9044894366197184, "ncat25k": 18, "os": 0.6001198232976998, "y": 0.9485035211267606, "term": "where"}, {"ncat": 64, "cat": 0, "cat25k": 0, "bg": 2.2881694506746904e-06, "x": 0.9000880281690141, "s": 0.040492957746478875, "ncat25k": 18, "os": 0.03172959499700456, "y": 0.15845070422535212, "term": "uses"}, {"ncat": 64, "cat": 0, "cat25k": 0, "bg": 8.192353909688899e-06, "x": 0.9005281690140845, "s": 0.040492957746478875, "ncat25k": 18, "os": 0.03172959499700456, "y": 0.27596830985915494, "term": "graph"}, {"ncat": 64, "cat": 1, "cat25k": 22, "bg": 6.856282865295249e-07, "x": 0.9009683098591549, "s": 0.8595950704225354, "ncat25k": 18, "os": 0.5109300811841505, "y": 0.8934859154929577, "term": "quality"}, {"ncat": 65, "cat": 1, "cat25k": 22, "bg": 3.2746379942274674e-07, "x": 0.9014084507042254, "s": 0.8591549295774648, "ncat25k": 18, "os": 0.5097923016693138, "y": 0.8367077464788732, "term": "them"}, {"ncat": 65, "cat": 0, "cat25k": 0, "bg": 9.846836249385521e-06, "x": 0.9018485915492958, "s": 0.03961267605633803, "ncat25k": 18, "os": 0.029920339204127955, "y": 0.335387323943662, "term": "introduce"}, {"ncat": 66, "cat": 0, "cat25k": 0, "bg": 1.4465163721544723e-06, "x": 0.9022887323943662, "s": 0.0369718309859155, "ncat25k": 18, "os": 0.02819840820529429, "y": 0.016285211267605633, "term": "various"}, {"ncat": 66, "cat": 0, "cat25k": 0, "bg": 5.2496793221454975e-06, "x": 0.9027288732394366, "s": 0.0369718309859155, "ncat25k": 18, "os": 0.02819840820529429, "y": 0.0682218309859155, "term": "annotation"}, {"ncat": 66, "cat": 2, "cat25k": 44, "bg": 3.2668121189140557e-07, "x": 0.903169014084507, "s": 0.9031690140845071, "ncat25k": 18, "os": 0.5959995860339105, "y": 0.954225352112676, "term": "most"}, {"ncat": 66, "cat": 0, "cat25k": 0, "bg": 3.1752133989964252e-06, "x": 0.9036091549295775, "s": 0.0369718309859155, "ncat25k": 18, "os": 0.02819840820529429, "y": 0.3785211267605634, "term": "architecture"}, {"ncat": 66, "cat": 0, "cat25k": 0, "bg": 1.1169430014678492e-05, "x": 0.9040492957746479, "s": 0.0369718309859155, "ncat25k": 18, "os": 0.02819840820529429, "y": 0.559419014084507, "term": "dialogue"}, {"ncat": 67, "cat": 0, "cat25k": 0, "bg": 7.284497122351827e-05, "x": 0.9044894366197183, "s": 0.035651408450704226, "ncat25k": 19, "os": 0.026560539410273365, "y": 0.022007042253521125, "term": "achieves"}, {"ncat": 67, "cat": 1, "cat25k": 22, "bg": 2.6005093020982588e-06, "x": 0.9049295774647887, "s": 0.8573943661971831, "ncat25k": 19, "os": 0.5076129395274376, "y": 0.852112676056338, "term": "target"}, {"ncat": 67, "cat": 1, "cat25k": 22, "bg": 1.082453086065391e-06, "x": 0.9053697183098591, "s": 0.8573943661971831, "ncat25k": 19, "os": 0.5076129395274376, "y": 0.8556338028169014, "term": "given"}, {"ncat": 67, "cat": 0, "cat25k": 0, "bg": 4.885099720379079e-06, "x": 0.9058098591549296, "s": 0.035651408450704226, "ncat25k": 19, "os": 0.026560539410273365, "y": 0.1382042253521127, "term": "achieve"}, {"ncat": 67, "cat": 4, "cat25k": 89, "bg": 8.578572779221971e-06, "x": 0.90625, "s": 0.9383802816901409, "ncat25k": 19, "os": 0.7094986799038129, "y": 0.9793133802816901, "term": "algorithm"}, {"ncat": 67, "cat": 0, "cat25k": 0, "bg": 1.1019478482457619e-06, "x": 0.9066901408450704, "s": 0.035651408450704226, "ncat25k": 19, "os": 0.026560539410273365, "y": 0.20862676056338028, "term": "space"}, {"ncat": 67, "cat": 1, "cat25k": 22, "bg": 8.470914057718567e-06, "x": 0.9071302816901409, "s": 0.8573943661971831, "ncat25k": 19, "os": 0.5076129395274376, "y": 0.8820422535211268, "term": "evaluate"}, {"ncat": 67, "cat": 1, "cat25k": 22, "bg": 0.00011606105835737759, "x": 0.9075704225352113, "s": 0.8573943661971831, "ncat25k": 19, "os": 0.5076129395274376, "y": 0.9357394366197183, "term": "lingual"}, {"ncat": 68, "cat": 0, "cat25k": 0, "bg": 1.878703965018532e-05, "x": 0.9080105633802817, "s": 0.03477112676056338, "ncat25k": 19, "os": 0.02500354299123242, "y": 0.11091549295774648, "term": "prediction"}, {"ncat": 69, "cat": 3, "cat25k": 67, "bg": 3.7920480457543466e-07, "x": 0.9084507042253521, "s": 0.9194542253521126, "ncat25k": 19, "os": 0.6506228212788971, "y": 0.9625880281690141, "term": "available"}, {"ncat": 69, "cat": 0, "cat25k": 0, "bg": 2.267441430591236e-05, "x": 0.9088908450704225, "s": 0.03345070422535212, "ncat25k": 19, "os": 0.0235243023230986, "y": 0.025088028169014086, "term": "annotated"}, {"ncat": 69, "cat": 14, "cat25k": 311, "bg": 0.0001860108625861561, "x": 0.909330985915493, "s": 0.9916373239436621, "ncat25k": 19, "os": 0.9541745102055514, "y": 0.9951584507042254, "term": "corpora"}, {"ncat": 69, "cat": 1, "cat25k": 22, "bg": 0.0001591464294930163, "x": 0.9097711267605634, "s": 0.8565140845070424, "ncat25k": 19, "os": 0.505553082742238, "y": 0.8455105633802817, "term": "classifier"}, {"ncat": 69, "cat": 0, "cat25k": 0, "bg": 0.0006851083265484441, "x": 0.9102112676056338, "s": 0.03345070422535212, "ncat25k": 19, "os": 0.0235243023230986, "y": 0.06602112676056338, "term": "subtasks"}, {"ncat": 69, "cat": 1, "cat25k": 22, "bg": 9.436974827590227e-08, "x": 0.9106514084507042, "s": 0.8565140845070424, "ncat25k": 19, "os": 0.505553082742238, "y": 0.8499119718309859, "term": "was"}, {"ncat": 69, "cat": 0, "cat25k": 0, "bg": 1.4731459897283296e-05, "x": 0.9110915492957746, "s": 0.03345070422535212, "ncat25k": 19, "os": 0.0235243023230986, "y": 0.11663732394366197, "term": "baseline"}, {"ncat": 70, "cat": 0, "cat25k": 0, "bg": 4.3878340581084503e-07, "x": 0.9115316901408451, "s": 0.03213028169014085, "ncat25k": 19, "os": 0.02211977434955925, "y": 0.1795774647887324, "term": "many"}, {"ncat": 70, "cat": 0, "cat25k": 0, "bg": 2.0805746523411772e-06, "x": 0.9119718309859155, "s": 0.03213028169014085, "ncat25k": 19, "os": 0.02211977434955925, "y": 0.2640845070422535, "term": "structure"}, {"ncat": 70, "cat": 0, "cat25k": 0, "bg": 4.051972427531645e-07, "x": 0.9124119718309859, "s": 0.03213028169014085, "ncat25k": 19, "os": 0.02211977434955925, "y": 0.320862676056338, "term": "high"}, {"ncat": 71, "cat": 2, "cat25k": 44, "bg": 3.9456630419058417e-07, "x": 0.9128521126760564, "s": 0.8987676056338029, "ncat25k": 20, "os": 0.5865966656501714, "y": 0.9397007042253521, "term": "then"}, {"ncat": 71, "cat": 1, "cat25k": 22, "bg": 3.4043100456461243e-05, "x": 0.9132922535211268, "s": 0.8560739436619719, "ncat25k": 20, "os": 0.5036031969239767, "y": 0.8393485915492958, "term": "vectors"}, {"ncat": 71, "cat": 0, "cat25k": 0, "bg": 2.3125830575605176e-06, "x": 0.9137323943661971, "s": 0.03169014084507043, "ncat25k": 20, "os": 0.020786989874023387, "y": 0.17517605633802816, "term": "improve"}, {"ncat": 72, "cat": 0, "cat25k": 0, "bg": 3.291716737716696e-05, "x": 0.9141725352112676, "s": 0.030809859154929578, "ncat25k": 20, "os": 0.019523053774830545, "y": 0.3789612676056338, "term": "dependency"}, {"ncat": 73, "cat": 0, "cat25k": 0, "bg": 2.548470557214185e-06, "x": 0.914612676056338, "s": 0.02904929577464789, "ncat25k": 20, "os": 0.01832514514397915, "y": 0.05017605633802817, "term": "input"}, {"ncat": 73, "cat": 0, "cat25k": 0, "bg": 4.133222830960387e-06, "x": 0.9150528169014085, "s": 0.02904929577464789, "ncat25k": 20, "os": 0.01832514514397915, "y": 0.09198943661971831, "term": "automatically"}, {"ncat": 73, "cat": 0, "cat25k": 0, "bg": 1.8919679068334875e-06, "x": 0.9154929577464789, "s": 0.02904929577464789, "ncat25k": 20, "os": 0.01832514514397915, "y": 0.12632042253521128, "term": "types"}, {"ncat": 73, "cat": 2, "cat25k": 44, "bg": 2.2635035642937928e-06, "x": 0.9159330985915493, "s": 0.8965669014084507, "ncat25k": 20, "os": 0.5831558813083031, "y": 0.9502640845070423, "term": "multiple"}, {"ncat": 73, "cat": 7, "cat25k": 155, "bg": 1.6789598004556275e-05, "x": 0.9163732394366197, "s": 0.9586267605633804, "ncat25k": 20, "os": 0.811569919131296, "y": 0.9872359154929577, "term": "pairs"}, {"ncat": 73, "cat": 1, "cat25k": 22, "bg": 4.3215351569207235e-07, "x": 0.9168133802816901, "s": 0.8556338028169015, "ncat25k": 20, "os": 0.5017547322496736, "y": 0.8714788732394366, "term": "through"}, {"ncat": 73, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.9172535211267606, "s": 0.02904929577464789, "ncat25k": 20, "os": 0.01832514514397915, "y": 0.3512323943661972, "term": "1"}, {"ncat": 74, "cat": 0, "cat25k": 0, "bg": 4.675498037999908e-07, "x": 0.917693661971831, "s": 0.02772887323943662, "ncat25k": 20, "os": 0.01719051734867677, "y": 0.0022007042253521128, "term": "user"}, {"ncat": 74, "cat": 0, "cat25k": 0, "bg": 0.0008061880379126266, "x": 0.9181338028169014, "s": 0.02772887323943662, "ncat25k": 20, "os": 0.01719051734867677, "y": 0.08450704225352113, "term": "convolutional"}, {"ncat": 74, "cat": 2, "cat25k": 44, "bg": 1.9818829301231626e-06, "x": 0.9185739436619719, "s": 0.8961267605633803, "ncat25k": 20, "os": 0.5814976315327754, "y": 0.9507042253521126, "term": "across"}, {"ncat": 74, "cat": 0, "cat25k": 0, "bg": 6.461606704702827e-05, "x": 0.9190140845070423, "s": 0.02772887323943662, "ncat25k": 20, "os": 0.01719051734867677, "y": 0.21566901408450703, "term": "textual"}, {"ncat": 75, "cat": 1, "cat25k": 22, "bg": 0.0, "x": 0.9194542253521126, "s": 0.855193661971831, "ncat25k": 21, "os": 0.5, "y": 0.8565140845070423, "term": "4"}, {"ncat": 75, "cat": 0, "cat25k": 0, "bg": 9.632609697443584e-06, "x": 0.9198943661971831, "s": 0.0272887323943662, "ncat25k": 21, "os": 0.016116498015113223, "y": 0.2147887323943662, "term": "modeling"}, {"ncat": 76, "cat": 1, "cat25k": 22, "bg": 6.889918334155903e-07, "x": 0.9203345070422535, "s": 0.11619718309859156, "ncat25k": 21, "os": 0.12324512132107279, "y": 0.8336267605633803, "term": "three"}, {"ncat": 76, "cat": 0, "cat25k": 0, "bg": 2.882761284758705e-06, "x": 0.920774647887324, "s": 0.026848591549295774, "ncat25k": 21, "os": 0.015100488933968925, "y": 0.35475352112676056, "term": "sense"}, {"ncat": 77, "cat": 0, "cat25k": 0, "bg": 1.7434197632397923e-07, "x": 0.9212147887323944, "s": 0.02596830985915493, "ncat25k": 21, "os": 0.01413996588733818, "y": 0.005721830985915493, "term": "they"}, {"ncat": 77, "cat": 0, "cat25k": 0, "bg": 1.4300681447900536e-06, "x": 0.9216549295774648, "s": 0.02596830985915493, "ncat25k": 21, "os": 0.01413996588733818, "y": 0.22359154929577466, "term": "several"}, {"ncat": 78, "cat": 4, "cat25k": 89, "bg": 5.253504159349801e-06, "x": 0.9220950704225352, "s": 0.9269366197183099, "ncat25k": 22, "os": 0.680043967938874, "y": 0.9740316901408451, "term": "speech"}, {"ncat": 78, "cat": 1, "cat25k": 22, "bg": 2.8425209908481256e-06, "x": 0.9225352112676056, "s": 0.11355633802816903, "ncat25k": 22, "os": 0.11875835927959238, "y": 0.8842429577464789, "term": "processing"}, {"ncat": 79, "cat": 0, "cat25k": 0, "bg": 2.6824821740995108e-06, "x": 0.922975352112676, "s": 0.025088028169014086, "ncat25k": 22, "os": 0.012375649393688848, "y": 0.11707746478873239, "term": "existing"}, {"ncat": 79, "cat": 0, "cat25k": 0, "bg": 1.8433068757353102e-06, "x": 0.9234154929577465, "s": 0.025088028169014086, "ncat25k": 22, "os": 0.012375649393688848, "y": 0.30369718309859156, "term": "simple"}, {"ncat": 80, "cat": 0, "cat25k": 0, "bg": 1.0734857343138909e-05, "x": 0.9238556338028169, "s": 0.023767605633802816, "ncat25k": 22, "os": 0.011567174808992331, "y": 0.01056338028169014, "term": "demonstrate"}, {"ncat": 80, "cat": 0, "cat25k": 0, "bg": 2.4711511253629945e-06, "x": 0.9242957746478874, "s": 0.023767605633802816, "ncat25k": 22, "os": 0.011567174808992331, "y": 0.19102112676056338, "term": "answer"}, {"ncat": 80, "cat": 0, "cat25k": 0, "bg": 0.0016394282493980227, "x": 0.9247359154929577, "s": 0.023767605633802816, "ncat25k": 22, "os": 0.011567174808992331, "y": 0.7205105633802817, "term": "semeval"}, {"ncat": 81, "cat": 0, "cat25k": 0, "bg": 2.144211232104024e-07, "x": 0.9251760563380281, "s": 0.02244718309859155, "ncat25k": 22, "os": 0.01080482308830144, "y": 0.006602112676056338, "term": "news"}, {"ncat": 81, "cat": 0, "cat25k": 0, "bg": 8.256580138879041e-07, "x": 0.9256161971830986, "s": 0.02244718309859155, "ncat25k": 22, "os": 0.01080482308830144, "y": 0.023327464788732395, "term": "non"}, {"ncat": 81, "cat": 2, "cat25k": 44, "bg": 9.219073191688234e-07, "x": 0.926056338028169, "s": 0.8934859154929577, "ncat25k": 22, "os": 0.5709225545412169, "y": 0.9537852112676056, "term": "source"}, {"ncat": 81, "cat": 0, "cat25k": 0, "bg": 1.0865242303152895e-06, "x": 0.9264964788732394, "s": 0.02244718309859155, "ncat25k": 22, "os": 0.01080482308830144, "y": 0.40933098591549294, "term": "topic"}, {"ncat": 82, "cat": 0, "cat25k": 0, "bg": 1.0434098251350111e-06, "x": 0.9269366197183099, "s": 0.022007042253521125, "ncat25k": 23, "os": 0.010086434627839702, "y": 0.10607394366197183, "term": "better"}, {"ncat": 82, "cat": 2, "cat25k": 44, "bg": 1.7003812629280214e-06, "x": 0.9273767605633803, "s": 0.892605633802817, "ncat25k": 23, "os": 0.5695449139174923, "y": 0.952024647887324, "term": "document"}, {"ncat": 83, "cat": 1, "cat25k": 22, "bg": 9.357981997972048e-07, "x": 0.9278169014084507, "s": 0.10783450704225353, "ncat25k": 23, "os": 0.10886056293343288, "y": 0.8481514084507042, "term": "learn"}, {"ncat": 83, "cat": 1, "cat25k": 22, "bg": 8.306315365686618e-08, "x": 0.9282570422535211, "s": 0.10783450704225353, "ncat25k": 23, "os": 0.10886056293343288, "y": 0.8675176056338029, "term": "all"}, {"ncat": 85, "cat": 0, "cat25k": 0, "bg": 7.353027046596132e-05, "x": 0.9286971830985915, "s": 0.021566901408450703, "ncat25k": 24, "os": 0.008174518094613281, "y": 0.18485915492957747, "term": "recurrent"}, {"ncat": 86, "cat": 1, "cat25k": 22, "bg": 2.673966164000984e-07, "x": 0.929137323943662, "s": 0.10255281690140845, "ncat25k": 24, "os": 0.10371108341060559, "y": 0.8525528169014085, "term": "when"}, {"ncat": 86, "cat": 0, "cat25k": 0, "bg": 9.764060559428444e-06, "x": 0.9295774647887324, "s": 0.02112676056338028, "ncat25k": 24, "os": 0.007611801740575241, "y": 0.17869718309859156, "term": "entity"}, {"ncat": 88, "cat": 7, "cat25k": 155, "bg": 0.0, "x": 0.9300176056338029, "s": 0.9507042253521127, "ncat25k": 24, "os": 0.7701039768969569, "y": 0.9876760563380281, "term": "2017"}, {"ncat": 89, "cat": 0, "cat25k": 0, "bg": 6.006690238089004e-06, "x": 0.9304577464788732, "s": 0.020246478873239437, "ncat25k": 25, "os": 0.006122060720455236, "y": 0.027288732394366196, "term": "accuracy"}, {"ncat": 89, "cat": 1, "cat25k": 22, "bg": 8.920152017428906e-07, "x": 0.9308978873239436, "s": 0.09903169014084508, "ncat25k": 25, "os": 0.09906884069611571, "y": 0.8578345070422535, "term": "previous"}, {"ncat": 89, "cat": 0, "cat25k": 0, "bg": 0.00011311713978863619, "x": 0.9313380281690141, "s": 0.020246478873239437, "ncat25k": 25, "os": 0.006122060720455236, "y": 0.38116197183098594, "term": "lexical"}, {"ncat": 90, "cat": 0, "cat25k": 0, "bg": 2.5931887965772907e-06, "x": 0.9317781690140845, "s": 0.019806338028169015, "ncat25k": 25, "os": 0.005686022402998425, "y": 0.07790492957746478, "term": "score"}, {"ncat": 91, "cat": 1, "cat25k": 22, "bg": 6.422647259520989e-06, "x": 0.9322183098591549, "s": 0.09066901408450705, "ncat25k": 25, "os": 0.09622552848208615, "y": 0.8587147887323944, "term": "automatic"}, {"ncat": 91, "cat": 0, "cat25k": 0, "bg": 4.54888154375043e-06, "x": 0.9326584507042254, "s": 0.01892605633802817, "ncat25k": 25, "os": 0.005277619675214451, "y": 0.23591549295774647, "term": "generation"}, {"ncat": 91, "cat": 0, "cat25k": 0, "bg": 3.182120338608516e-07, "x": 0.9330985915492958, "s": 0.01892605633802817, "ncat25k": 25, "os": 0.005277619675214451, "y": 0.30721830985915494, "term": "how"}, {"ncat": 92, "cat": 1, "cat25k": 22, "bg": 2.8888033430350533e-06, "x": 0.9335387323943662, "s": 0.09022887323943662, "ncat25k": 25, "os": 0.0948730821279763, "y": 0.8490316901408451, "term": "feature"}, {"ncat": 92, "cat": 2, "cat25k": 44, "bg": 1.153118495499311e-05, "x": 0.9339788732394366, "s": 0.8873239436619719, "ncat25k": 25, "os": 0.5572745843544699, "y": 0.9515845070422535, "term": "describes"}, {"ncat": 92, "cat": 0, "cat25k": 0, "bg": 1.1734595166256223e-06, "x": 0.934419014084507, "s": 0.018485915492957746, "ncat25k": 25, "os": 0.004895365736044821, "y": 0.2051056338028169, "term": "questions"}, {"ncat": 94, "cat": 1, "cat25k": 22, "bg": 1.2412138067606747e-06, "x": 0.9348591549295775, "s": 0.08890845070422536, "ncat25k": 26, "os": 0.09229699409016867, "y": 0.8402288732394366, "term": "study"}, {"ncat": 94, "cat": 4, "cat25k": 89, "bg": 2.6369395324606647e-06, "x": 0.9352992957746479, "s": 0.9190140845070423, "ncat25k": 26, "os": 0.6469967040130721, "y": 0.977112676056338, "term": "cross"}, {"ncat": 95, "cat": 0, "cat25k": 0, "bg": 5.246004984284556e-07, "x": 0.9357394366197183, "s": 0.018045774647887324, "ncat25k": 26, "os": 0.003891538841436726, "y": 0.018485915492957746, "term": "well"}, {"ncat": 95, "cat": 2, "cat25k": 44, "bg": 2.9307715827540984e-07, "x": 0.9361795774647887, "s": 0.8855633802816902, "ncat25k": 26, "os": 0.5540559010703673, "y": 0.9410211267605634, "term": "only"}, {"ncat": 96, "cat": 0, "cat25k": 0, "bg": 0.0001171657602593269, "x": 0.9366197183098591, "s": 0.017605633802816902, "ncat25k": 27, "os": 0.0036002162325312614, "y": 0.18794014084507044, "term": "embedding"}, {"ncat": 98, "cat": 1, "cat25k": 22, "bg": 2.1786895500137243e-07, "x": 0.9370598591549296, "s": 0.08538732394366197, "ncat25k": 27, "os": 0.08761087065493484, "y": 0.8463908450704225, "term": "time"}, {"ncat": 99, "cat": 0, "cat25k": 0, "bg": 6.112556240148134e-05, "x": 0.9375, "s": 0.016725352112676055, "ncat25k": 27, "os": 0.002839376684930228, "y": 0.2227112676056338, "term": "supervised"}, {"ncat": 99, "cat": 0, "cat25k": 0, "bg": 0.00014968373640845213, "x": 0.9379401408450704, "s": 0.016725352112676055, "ncat25k": 27, "os": 0.002839376684930228, "y": 0.38204225352112675, "term": "syntactic"}, {"ncat": 101, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.9383802816901409, "s": 0.016285211267605633, "ncat25k": 28, "os": 0.0024156734998398433, "y": 0.0, "term": "'s"}, {"ncat": 101, "cat": 1, "cat25k": 22, "bg": 1.4606484970012886e-06, "x": 0.9388204225352113, "s": 0.08318661971830986, "ncat25k": 28, "os": 0.08445421294148936, "y": 0.8287852112676056, "term": "social"}, {"ncat": 101, "cat": 1, "cat25k": 22, "bg": 6.546098674562948e-07, "x": 0.9392605633802817, "s": 0.08318661971830986, "ncat25k": 28, "os": 0.08445421294148936, "y": 0.832306338028169, "term": "research"}, {"ncat": 102, "cat": 0, "cat25k": 0, "bg": 1.6240373190401491e-06, "x": 0.9397007042253521, "s": 0.015845070422535214, "ncat25k": 28, "os": 0.002225918002303262, "y": 0.13732394366197184, "term": "event"}, {"ncat": 103, "cat": 0, "cat25k": 0, "bg": 3.9183956072122255e-07, "x": 0.9401408450704225, "s": 0.014964788732394365, "ncat25k": 28, "os": 0.002049689026384749, "y": 0.03829225352112676, "term": "its"}, {"ncat": 103, "cat": 1, "cat25k": 22, "bg": 1.474031083758039e-06, "x": 0.940580985915493, "s": 0.07746478873239437, "ncat25k": 28, "os": 0.08249912801937692, "y": 0.8622359154929577, "term": "problem"}, {"ncat": 103, "cat": 0, "cat25k": 0, "bg": 3.135272869951164e-05, "x": 0.9410211267605634, "s": 0.014964788732394365, "ncat25k": 28, "os": 0.002049689026384749, "y": 0.20026408450704225, "term": "answering"}, {"ncat": 105, "cat": 2, "cat25k": 44, "bg": 0.00219212882342095, "x": 0.9414612676056338, "s": 0.8816021126760564, "ncat25k": 29, "os": 0.5445580302854465, "y": 0.9366197183098591, "term": "tweets"}, {"ncat": 105, "cat": 2, "cat25k": 44, "bg": 5.753455763466507e-07, "x": 0.9419014084507042, "s": 0.8816021126760564, "ncat25k": 29, "os": 0.5445580302854465, "y": 0.9449823943661971, "term": "best"}, {"ncat": 106, "cat": 1, "cat25k": 22, "bg": 9.456782400581497e-07, "x": 0.9423415492957746, "s": 0.0761443661971831, "ncat25k": 29, "os": 0.0797659289973246, "y": 0.8441901408450704, "term": "while"}, {"ncat": 106, "cat": 0, "cat25k": 0, "bg": 1.2922511494741602e-06, "x": 0.9427816901408451, "s": 0.014524647887323945, "ncat25k": 29, "os": 0.0015939135506613522, "y": 0.1329225352112676, "term": "however"}, {"ncat": 108, "cat": 1, "cat25k": 22, "bg": 4.336522940776271e-07, "x": 0.9432218309859155, "s": 0.07482394366197184, "ncat25k": 30, "os": 0.07806394956814172, "y": 0.8340669014084507, "term": "than"}, {"ncat": 110, "cat": 1, "cat25k": 22, "bg": 5.873199216271831e-05, "x": 0.9436619718309859, "s": 0.07438380281690142, "ncat25k": 30, "os": 0.07644887642717374, "y": 0.8327464788732394, "term": "linguistic"}, {"ncat": 111, "cat": 1, "cat25k": 22, "bg": 1.3941500158320049e-06, "x": 0.9441021126760564, "s": 0.07394366197183098, "ncat25k": 31, "os": 0.07567168646415279, "y": 0.8477112676056338, "term": "human"}, {"ncat": 111, "cat": 6, "cat25k": 133, "bg": 6.244387556472679e-06, "x": 0.9445422535211268, "s": 0.932218309859155, "ncat25k": 31, "os": 0.6899994733740809, "y": 0.985475352112676, "term": "languages"}, {"ncat": 111, "cat": 2, "cat25k": 44, "bg": 6.949682972990735e-06, "x": 0.9449823943661971, "s": 0.8798415492957747, "ncat25k": 31, "os": 0.5396244652706941, "y": 0.9493838028169014, "term": "framework"}, {"ncat": 113, "cat": 4, "cat25k": 89, "bg": 7.462514664678457e-07, "x": 0.9454225352112676, "s": 0.9097711267605634, "ncat25k": 31, "os": 0.6179389387396802, "y": 0.9762323943661971, "term": "set"}, {"ncat": 113, "cat": 3, "cat25k": 67, "bg": 4.332800865723494e-06, "x": 0.945862676056338, "s": 0.8948063380281691, "ncat25k": 31, "os": 0.5794850916561903, "y": 0.9669894366197183, "term": "deep"}, {"ncat": 114, "cat": 0, "cat25k": 0, "bg": 4.124588626555404e-06, "x": 0.9463028169014085, "s": 0.014084507042253521, "ncat25k": 32, "os": 0.0007910228926071294, "y": 0.25704225352112675, "term": "relations"}, {"ncat": 115, "cat": 0, "cat25k": 0, "bg": 6.745056224237294e-07, "x": 0.9467429577464789, "s": 0.013204225352112676, "ncat25k": 32, "os": 0.0007224597248289832, "y": 0.029049295774647887, "term": "each"}, {"ncat": 115, "cat": 0, "cat25k": 0, "bg": 2.3000062468169665e-07, "x": 0.9471830985915493, "s": 0.013204225352112676, "ncat25k": 32, "os": 0.0007224597248289832, "y": 0.04621478873239437, "term": "but"}, {"ncat": 115, "cat": 16, "cat25k": 355, "bg": 4.500890729711205e-05, "x": 0.9476232394366197, "s": 0.9806338028169015, "ncat25k": 32, "os": 0.8912746907177886, "y": 0.9960387323943662, "term": "sentences"}, {"ncat": 116, "cat": 2, "cat25k": 44, "bg": 2.3751193829604088e-07, "x": 0.9480633802816901, "s": 0.8776408450704226, "ncat25k": 32, "os": 0.5358777836329859, "y": 0.9454225352112676, "term": "one"}, {"ncat": 117, "cat": 0, "cat25k": 0, "bg": 2.1690081712377448e-06, "x": 0.9485035211267606, "s": 0.012764084507042254, "ncat25k": 32, "os": 0.000601400069803415, "y": 0.11619718309859155, "term": "specific"}, {"ncat": 117, "cat": 1, "cat25k": 22, "bg": 2.4116602908499915e-07, "x": 0.948943661971831, "s": 0.0721830985915493, "ncat25k": 32, "os": 0.07138384402431047, "y": 0.8697183098591549, "term": "other"}, {"ncat": 118, "cat": 4, "cat25k": 89, "bg": 4.219561200913404e-07, "x": 0.9493838028169014, "s": 0.9080105633802817, "ncat25k": 33, "os": 0.611618295657589, "y": 0.9753521126760564, "term": "first"}, {"ncat": 118, "cat": 1, "cat25k": 22, "bg": 1.0773620068795265e-06, "x": 0.9498239436619719, "s": 0.07174295774647887, "ncat25k": 33, "os": 0.07072525985290973, "y": 0.889524647887324, "term": "end"}, {"ncat": 119, "cat": 0, "cat25k": 0, "bg": 5.852548771465176e-06, "x": 0.9502640845070423, "s": 0.011883802816901408, "ncat25k": 33, "os": 0.0004992421896440891, "y": 0.06205985915492958, "term": "context"}, {"ncat": 119, "cat": 0, "cat25k": 0, "bg": 4.309322674603932e-06, "x": 0.9507042253521126, "s": 0.011883802816901408, "ncat25k": 33, "os": 0.0004992421896440891, "y": 0.12147887323943662, "term": "multi"}, {"ncat": 119, "cat": 1, "cat25k": 22, "bg": 1.4979353769457632e-05, "x": 0.9511443661971831, "s": 0.07130281690140845, "ncat25k": 33, "os": 0.07008103428250212, "y": 0.8754401408450704, "term": "vector"}, {"ncat": 120, "cat": 1, "cat25k": 22, "bg": 0.0018867041928492352, "x": 0.9515845070422535, "s": 0.07042253521126761, "ncat25k": 33, "os": 0.06945061337333364, "y": 0.8305457746478874, "term": "twitter"}, {"ncat": 121, "cat": 1, "cat25k": 22, "bg": 1.780276131042624e-05, "x": 0.952024647887324, "s": 0.06910211267605634, "ncat25k": 33, "os": 0.0688334704812561, "y": 0.8362676056338029, "term": "trained"}, {"ncat": 121, "cat": 1, "cat25k": 22, "bg": 4.24261484078947e-07, "x": 0.9524647887323944, "s": 0.06910211267605634, "ncat25k": 33, "os": 0.0688334704812561, "y": 0.8485915492957746, "term": "been"}, {"ncat": 121, "cat": 0, "cat25k": 0, "bg": 1.290817571846266e-05, "x": 0.9529049295774648, "s": 0.011003521126760563, "ncat25k": 33, "os": 0.00041328893807673905, "y": 0.15580985915492956, "term": "representation"}, {"ncat": 121, "cat": 1, "cat25k": 22, "bg": 2.319404687144247e-06, "x": 0.9533450704225352, "s": 0.06910211267605634, "ncat25k": 33, "os": 0.0688334704812561, "y": 0.8899647887323944, "term": "natural"}, {"ncat": 121, "cat": 0, "cat25k": 0, "bg": 8.497599779202861e-05, "x": 0.9537852112676056, "s": 0.011003521126760563, "ncat25k": 33, "os": 0.00041328893807673905, "y": 0.4612676056338028, "term": "parsing"}, {"ncat": 123, "cat": 4, "cat25k": 89, "bg": 1.5304763176715696e-06, "x": 0.954225352112676, "s": 0.9066901408450705, "ncat25k": 34, "os": 0.6057409382450291, "y": 0.971830985915493, "term": "large"}, {"ncat": 123, "cat": 0, "cat25k": 0, "bg": 1.4329999142530133e-05, "x": 0.9546654929577465, "s": 0.01056338028169014, "ncat25k": 34, "os": 0.00034118342310057725, "y": 0.043133802816901406, "term": "approaches"}, {"ncat": 124, "cat": 0, "cat25k": 0, "bg": 1.104501323887353e-05, "x": 0.9551056338028169, "s": 0.010123239436619719, "ncat25k": 34, "os": 0.0003096712995044326, "y": 0.02640845070422535, "term": "relation"}, {"ncat": 124, "cat": 1, "cat25k": 22, "bg": 1.813147860394867e-05, "x": 0.9555457746478874, "s": 0.06778169014084508, "ncat25k": 34, "os": 0.0670568259995536, "y": 0.8670774647887324, "term": "experiments"}, {"ncat": 125, "cat": 0, "cat25k": 0, "bg": 6.994744037414383e-06, "x": 0.9559859154929577, "s": 0.009683098591549297, "ncat25k": 35, "os": 0.00028087354999523084, "y": 0.4779929577464789, "term": "sequence"}, {"ncat": 126, "cat": 13, "cat25k": 289, "bg": 5.3103843228285776e-05, "x": 0.9564260563380281, "s": 0.9652288732394366, "ncat25k": 35, "os": 0.8278412734620717, "y": 0.9929577464788732, "term": "corpus"}, {"ncat": 126, "cat": 1, "cat25k": 22, "bg": 5.401126917487771e-06, "x": 0.9568661971830986, "s": 0.06734154929577466, "ncat25k": 35, "os": 0.06593024488504512, "y": 0.8657570422535211, "term": "evaluation"}, {"ncat": 131, "cat": 3, "cat25k": 67, "bg": 0.00013736953738245421, "x": 0.957306338028169, "s": 0.8908450704225352, "ncat25k": 36, "os": 0.5628151721360224, "y": 0.9665492957746479, "term": "datasets"}, {"ncat": 131, "cat": 0, "cat25k": 0, "bg": 1.2927974680591194e-06, "x": 0.9577464788732394, "s": 0.009242957746478873, "ncat25k": 36, "os": 0.00015409421873957552, "y": 0.1637323943661972, "term": "level"}, {"ncat": 137, "cat": 2, "cat25k": 44, "bg": 6.241399161121828e-07, "x": 0.9581866197183099, "s": 0.8661971830985916, "ncat25k": 38, "os": 0.522965796174093, "y": 0.9480633802816901, "term": "into"}, {"ncat": 140, "cat": 0, "cat25k": 0, "bg": 6.2430104759722475e-06, "x": 0.9586267605633803, "s": 0.008802816901408451, "ncat25k": 39, "os": 5.971450373454257e-05, "y": 0.1153169014084507, "term": "attention"}, {"ncat": 140, "cat": 1, "cat25k": 22, "bg": 1.2865641414955943e-05, "x": 0.9590669014084507, "s": 0.06338028169014086, "ncat25k": 39, "os": 0.05909063078168825, "y": 0.8705985915492958, "term": "novel"}, {"ncat": 140, "cat": 1, "cat25k": 22, "bg": 6.139503750573856e-07, "x": 0.9595070422535211, "s": 0.06338028169014086, "ncat25k": 39, "os": 0.05909063078168825, "y": 0.8785211267605634, "term": "over"}, {"ncat": 142, "cat": 3, "cat25k": 67, "bg": 1.642376670820936e-05, "x": 0.9599471830985915, "s": 0.8860035211267606, "ncat25k": 39, "os": 0.5545350649916247, "y": 0.9634683098591549, "term": "detection"}, {"ncat": 142, "cat": 0, "cat25k": 0, "bg": 4.6427255152832715e-06, "x": 0.960387323943662, "s": 0.008362676056338027, "ncat25k": 39, "os": 4.799698338886493e-05, "y": 0.07702464788732394, "term": "networks"}, {"ncat": 146, "cat": 3, "cat25k": 67, "bg": 4.6224067600061924e-05, "x": 0.9608274647887324, "s": 0.8846830985915493, "ncat25k": 40, "os": 0.5518089043343593, "y": 0.9678697183098591, "term": "extraction"}, {"ncat": 146, "cat": 0, "cat25k": 0, "bg": 1.4334078476330657e-05, "x": 0.9612676056338029, "s": 0.007922535211267605, "ncat25k": 40, "os": 3.074588187668148e-05, "y": 0.1641725352112676, "term": "tasks"}, {"ncat": 148, "cat": 0, "cat25k": 0, "bg": 0.0011779641118906722, "x": 0.9617077464788732, "s": 0.0074823943661971835, "ncat25k": 41, "os": 2.4503197978364977e-05, "y": 0.06910211267605634, "term": "subtask"}, {"ncat": 149, "cat": 1, "cat25k": 22, "bg": 7.149979663193678e-07, "x": 0.9621478873239436, "s": 0.05985915492957747, "ncat25k": 41, "os": 0.05544314333759004, "y": 0.8309859154929577, "term": "work"}, {"ncat": 150, "cat": 14, "cat25k": 311, "bg": 2.2687106550756765e-05, "x": 0.9625880281690141, "s": 0.9577464788732395, "ncat25k": 41, "os": 0.8055468719122556, "y": 0.9955985915492958, "term": "sentence"}, {"ncat": 155, "cat": 1, "cat25k": 22, "bg": 3.984943646549027e-07, "x": 0.9630281690140845, "s": 0.0585387323943662, "ncat25k": 43, "os": 0.05326049134483818, "y": 0.829225352112676, "term": "their"}, {"ncat": 155, "cat": 2, "cat25k": 44, "bg": 5.857067373454281e-06, "x": 0.9634683098591549, "s": 0.8613556338028169, "ncat25k": 43, "os": 0.5145513317008997, "y": 0.9432218309859155, "term": "proposed"}, {"ncat": 157, "cat": 18, "cat25k": 400, "bg": 2.0293474251837005e-06, "x": 0.9639084507042254, "s": 0.9683098591549296, "ncat25k": 43, "os": 0.8509667585056973, "y": 0.9964788732394366, "term": "english"}, {"ncat": 162, "cat": 0, "cat25k": 0, "bg": 2.0972647298621078e-07, "x": 0.9643485915492958, "s": 0.007042253521126761, "ncat25k": 45, "os": 4.619541739581123e-06, "y": 0.014964788732394365, "term": "more"}, {"ncat": 163, "cat": 4, "cat25k": 89, "bg": 1.2682333110483603e-07, "x": 0.9647887323943662, "s": 0.8930457746478874, "ncat25k": 45, "os": 0.5703648682993019, "y": 0.9722711267605634, "term": "not"}, {"ncat": 163, "cat": 2, "cat25k": 44, "bg": 2.1271708833586904e-07, "x": 0.9652288732394366, "s": 0.10167253521126762, "ncat25k": 45, "os": 0.10238324281741801, "y": 0.9392605633802817, "term": "new"}, {"ncat": 164, "cat": 1, "cat25k": 22, "bg": 4.9157382872852784e-05, "x": 0.965669014084507, "s": 0.056338028169014086, "ncat25k": 45, "os": 0.05029659231714856, "y": 0.8745598591549296, "term": "representations"}, {"ncat": 165, "cat": 1, "cat25k": 22, "bg": 5.38150822321283e-07, "x": 0.9661091549295775, "s": 0.05545774647887325, "ncat25k": 46, "os": 0.04998776081892292, "y": 0.8296654929577465, "term": "also"}, {"ncat": 171, "cat": 2, "cat25k": 44, "bg": 2.21185132724485e-07, "x": 0.9665492957746479, "s": 0.09859154929577466, "ncat25k": 47, "os": 0.09749096370857036, "y": 0.9436619718309859, "term": "have"}, {"ncat": 175, "cat": 0, "cat25k": 0, "bg": 1.5300798551736575e-06, "x": 0.9669894366197183, "s": 0.006602112676056338, "ncat25k": 48, "os": 8.652650111962146e-07, "y": 0.07922535211267606, "term": "both"}, {"ncat": 176, "cat": 4, "cat25k": 89, "bg": 8.540199218270967e-07, "x": 0.9674295774647887, "s": 0.8890845070422536, "ncat25k": 49, "os": 0.5620104779974209, "y": 0.9731514084507042, "term": "used"}, {"ncat": 178, "cat": 2, "cat25k": 44, "bg": 0.00011383870762709852, "x": 0.9678697183098591, "s": 0.0897887323943662, "ncat25k": 49, "os": 0.09357854429291673, "y": 0.9379401408450704, "term": "dataset"}, {"ncat": 180, "cat": 5, "cat25k": 111, "bg": 5.13833254306936e-07, "x": 0.9683098591549296, "s": 0.8983274647887325, "ncat25k": 50, "os": 0.5849938553327245, "y": 0.9806338028169014, "term": "use"}, {"ncat": 181, "cat": 3, "cat25k": 67, "bg": 3.516760142517584e-07, "x": 0.96875, "s": 0.8732394366197184, "ncat25k": 50, "os": 0.5327689676686327, "y": 0.9652288732394366, "term": "has"}, {"ncat": 181, "cat": 4, "cat25k": 89, "bg": 5.303620976984894e-06, "x": 0.9691901408450704, "s": 0.8877640845070423, "ncat25k": 50, "os": 0.5590891697676429, "y": 0.9766725352112676, "term": "machine"}, {"ncat": 182, "cat": 11, "cat25k": 244, "bg": 1.2678932669053613e-05, "x": 0.9696302816901409, "s": 0.9388204225352113, "ncat25k": 50, "os": 0.7119706103059096, "y": 0.9920774647887324, "term": "translation"}, {"ncat": 185, "cat": 3, "cat25k": 67, "bg": 9.873337811434716e-07, "x": 0.9700704225352113, "s": 0.8719190140845071, "ncat25k": 51, "os": 0.5310237733151199, "y": 0.9643485915492958, "term": "such"}, {"ncat": 186, "cat": 3, "cat25k": 67, "bg": 6.985747961613262e-07, "x": 0.9705105633802817, "s": 0.8714788732394367, "ncat25k": 51, "os": 0.5305985056197325, "y": 0.9639084507042254, "term": "these"}, {"ncat": 187, "cat": 5, "cat25k": 111, "bg": 4.363864722693727e-06, "x": 0.9709507042253521, "s": 0.8952464788732395, "ncat25k": 52, "os": 0.5803370953585812, "y": 0.9815140845070423, "term": "methods"}, {"ncat": 190, "cat": 1, "cat25k": 22, "bg": 4.5208584717325505e-06, "x": 0.9713908450704225, "s": 0.04929577464788733, "ncat25k": 53, "os": 0.04333966662871108, "y": 0.8631161971830986, "term": "method"}, {"ncat": 191, "cat": 1, "cat25k": 22, "bg": 1.7169406717678822e-06, "x": 0.971830985915493, "s": 0.048415492957746484, "ncat25k": 53, "os": 0.043110385663145956, "y": 0.8617957746478874, "term": "systems"}, {"ncat": 191, "cat": 1, "cat25k": 22, "bg": 4.273930163491408e-06, "x": 0.9722711267605634, "s": 0.048415492957746484, "ncat25k": 53, "os": 0.043110385663145956, "y": 0.8886443661971831, "term": "knowledge"}, {"ncat": 193, "cat": 2, "cat25k": 44, "bg": 1.5262132526591095e-06, "x": 0.9727112676056338, "s": 0.08450704225352114, "ncat25k": 53, "os": 0.08616772680982382, "y": 0.9388204225352113, "term": "between"}, {"ncat": 198, "cat": 0, "cat25k": 0, "bg": 1.8435719086549786e-06, "x": 0.9731514084507042, "s": 0.005721830985915493, "ncat25k": 55, "os": 3.315311630780471e-08, "y": 0.11971830985915492, "term": "art"}, {"ncat": 198, "cat": 0, "cat25k": 0, "bg": 3.028860844747913e-06, "x": 0.9735915492957746, "s": 0.005721830985915493, "ncat25k": 55, "os": 3.315311630780471e-08, "y": 0.20334507042253522, "term": "question"}, {"ncat": 202, "cat": 3, "cat25k": 67, "bg": 2.3265464040405096e-06, "x": 0.9740316901408451, "s": 0.8675176056338029, "ncat25k": 56, "os": 0.5243355782573846, "y": 0.9691901408450704, "term": "training"}, {"ncat": 203, "cat": 0, "cat25k": 0, "bg": 1.5670607686191837e-07, "x": 0.9744718309859155, "s": 0.00528169014084507, "ncat25k": 56, "os": 1.5507955752891434e-08, "y": 0.006161971830985915, "term": "or"}, {"ncat": 203, "cat": 1, "cat25k": 22, "bg": 6.384885598655794e-05, "x": 0.9749119718309859, "s": 0.04533450704225352, "ncat25k": 56, "os": 0.040536944175513234, "y": 0.8397887323943662, "term": "propose"}, {"ncat": 203, "cat": 3, "cat25k": 67, "bg": 2.2902637469398647e-06, "x": 0.9753521126760564, "s": 0.866637323943662, "ncat25k": 56, "os": 0.5239751537997525, "y": 0.9661091549295775, "term": "different"}, {"ncat": 203, "cat": 2, "cat25k": 44, "bg": 4.648783763081834e-06, "x": 0.9757922535211268, "s": 0.07702464788732395, "ncat25k": 56, "os": 0.08184582981313199, "y": 0.9498239436619719, "term": "domain"}, {"ncat": 207, "cat": 2, "cat25k": 44, "bg": 4.200225429515253e-06, "x": 0.9762323943661971, "s": 0.07658450704225353, "ncat25k": 57, "os": 0.08023591346529374, "y": 0.9383802816901409, "term": "present"}, {"ncat": 209, "cat": 5, "cat25k": 111, "bg": 8.536456453102345e-05, "x": 0.9766725352112676, "s": 0.8912852112676057, "ncat25k": 58, "os": 0.567536993963095, "y": 0.9823943661971831, "term": "similarity"}, {"ncat": 210, "cat": 6, "cat25k": 133, "bg": 1.535584174725194e-07, "x": 0.977112676056338, "s": 0.8992077464788732, "ncat25k": 58, "os": 0.5885220326070257, "y": 0.9845950704225352, "term": "it"}, {"ncat": 212, "cat": 1, "cat25k": 22, "bg": 4.680913807993597e-06, "x": 0.9775528169014085, "s": 0.04401408450704226, "ncat25k": 59, "os": 0.03879981381847958, "y": 0.8279049295774648, "term": "words"}, {"ncat": 213, "cat": 0, "cat25k": 0, "bg": 2.1269838305790476e-05, "x": 0.9779929577464789, "s": 0.004841549295774648, "ncat25k": 59, "os": 3.2133285476909634e-09, "y": 0.008362676056338027, "term": "classification"}, {"ncat": 214, "cat": 0, "cat25k": 0, "bg": 1.8995483335635275e-06, "x": 0.9784330985915493, "s": 0.0044014084507042256, "ncat25k": 59, "os": 2.7343742270424798e-09, "y": 0.08758802816901408, "term": "network"}, {"ncat": 218, "cat": 1, "cat25k": 22, "bg": 9.66456742332882e-07, "x": 0.9788732394366197, "s": 0.043573943661971835, "ncat25k": 60, "os": 0.03772211922112878, "y": 0.8591549295774648, "term": "state"}, {"ncat": 219, "cat": 0, "cat25k": 0, "bg": 3.5026753162168097e-06, "x": 0.9793133802816901, "s": 0.003961267605633803, "ncat25k": 61, "os": 1.206780952944797e-09, "y": 0.011883802816901408, "term": "analysis"}, {"ncat": 229, "cat": 0, "cat25k": 0, "bg": 0.0021609991554173602, "x": 0.9797535211267606, "s": 0.003080985915492958, "ncat25k": 63, "os": 2.2254920128972344e-10, "y": 0.07482394366197183, "term": "embeddings"}, {"ncat": 229, "cat": 0, "cat25k": 0, "bg": 3.2759132030059665e-06, "x": 0.980193661971831, "s": 0.003080985915492958, "ncat25k": 63, "os": 2.2254920128972344e-10, "y": 0.11575704225352113, "term": "performance"}, {"ncat": 230, "cat": 0, "cat25k": 0, "bg": 4.931959691925438e-07, "x": 0.9806338028169014, "s": 0.0026408450704225356, "ncat25k": 64, "os": 1.8718127048344968e-10, "y": 0.07526408450704225, "term": "information"}, {"ncat": 237, "cat": 4, "cat25k": 89, "bg": 2.0093196430432149e-07, "x": 0.9810739436619719, "s": 0.8758802816901409, "ncat25k": 66, "os": 0.5342412306886446, "y": 0.9749119718309859, "term": "be"}, {"ncat": 238, "cat": 8, "cat25k": 178, "bg": 1.1143923775561378e-06, "x": 0.9815140845070423, "s": 0.9075704225352113, "ncat25k": 66, "os": 0.6104088621795842, "y": 0.9881161971830986, "term": "two"}, {"ncat": 246, "cat": 0, "cat25k": 0, "bg": 0.00019235490888163604, "x": 0.9819542253521126, "s": 0.002200704225352113, "ncat25k": 68, "os": 1.0628775637400167e-11, "y": 0.4339788732394366, "term": "sentiment"}, {"ncat": 256, "cat": 0, "cat25k": 0, "bg": 0.0, "x": 0.9823943661971831, "s": 0.0017605633802816902, "ncat25k": 71, "os": 1.6088796961355456e-12, "y": 0.795774647887324, "term": "semeval-2017"}, {"ncat": 276, "cat": 1, "cat25k": 22, "bg": 2.2371204031688316e-06, "x": 0.9828345070422535, "s": 0.03917253521126761, "ncat25k": 76, "os": 0.029737100598072774, "y": 0.8433098591549296, "term": "show"}, {"ncat": 290, "cat": 3, "cat25k": 67, "bg": 2.184296058652837e-06, "x": 0.983274647887324, "s": 0.08406690140845072, "ncat25k": 80, "os": 0.08601627383314281, "y": 0.9630281690140845, "term": "results"}, {"ncat": 290, "cat": 1, "cat25k": 22, "bg": 2.7863169775665003e-06, "x": 0.9837147887323944, "s": 0.03873239436619719, "ncat25k": 80, "os": 0.028291447556643257, "y": 0.8450704225352113, "term": "text"}, {"ncat": 295, "cat": 0, "cat25k": 0, "bg": 3.663256158625896e-06, "x": 0.9841549295774648, "s": 0.0013204225352112678, "ncat25k": 82, "os": 5.551115123125783e-16, "y": 0.01232394366197183, "term": "features"}, {"ncat": 314, "cat": 3, "cat25k": 67, "bg": 9.866755382388984e-06, "x": 0.9845950704225352, "s": 0.07526408450704226, "ncat25k": 87, "os": 0.07932566678960395, "y": 0.965669014084507, "term": "approach"}, {"ncat": 315, "cat": 2, "cat25k": 44, "bg": 5.102938787196338e-07, "x": 0.9850352112676056, "s": 0.056778169014084515, "ncat25k": 87, "os": 0.05239458919073109, "y": 0.940580985915493, "term": "can"}, {"ncat": 322, "cat": 1, "cat25k": 22, "bg": 2.8428461323921106e-07, "x": 0.985475352112676, "s": 0.03521126760563381, "ncat25k": 89, "os": 0.025462053606230606, "y": 0.8653169014084507, "term": "at"}, {"ncat": 353, "cat": 2, "cat25k": 44, "bg": 8.191892804444427e-06, "x": 0.9859154929577465, "s": 0.051936619718309866, "ncat25k": 98, "os": 0.046691674002688366, "y": 0.945862676056338, "term": "models"}, {"ncat": 356, "cat": 6, "cat25k": 133, "bg": 1.7788426591678128e-06, "x": 0.9863556338028169, "s": 0.8754401408450705, "ncat25k": 98, "os": 0.5341261944509809, "y": 0.9841549295774648, "term": "data"}, {"ncat": 368, "cat": 6, "cat25k": 133, "bg": 2.775028625922799e-06, "x": 0.9867957746478874, "s": 0.8727992957746479, "ncat25k": 102, "os": 0.5314534434502086, "y": 0.983274647887324, "term": "using"}, {"ncat": 388, "cat": 0, "cat25k": 0, "bg": 0.00016546127982167708, "x": 0.9872359154929577, "s": 0.0, "ncat25k": 107, "os": 0.0, "y": 0.08626760563380281, "term": "semantic"}, {"ncat": 402, "cat": 1, "cat25k": 22, "bg": 6.843803839497924e-06, "x": 0.9876760563380281, "s": 0.03125000000000001, "ncat25k": 111, "os": 0.020369086219536148, "y": 0.846830985915493, "term": "learning"}, {"ncat": 404, "cat": 6, "cat25k": 133, "bg": 1.011581360606784e-06, "x": 0.9881161971830986, "s": 0.8679577464788734, "ncat25k": 112, "os": 0.5243356348274272, "y": 0.9837147887323944, "term": "which"}, {"ncat": 406, "cat": 6, "cat25k": 133, "bg": 2.0751854549042623e-06, "x": 0.988556338028169, "s": 0.8670774647887324, "ncat25k": 112, "os": 0.5239752103284947, "y": 0.9850352112676056, "term": "system"}, {"ncat": 411, "cat": 7, "cat25k": 155, "bg": 6.190879369908293e-06, "x": 0.9889964788732394, "s": 0.8763204225352113, "ncat25k": 114, "os": 0.5349880546026948, "y": 0.9867957746478874, "term": "paper"}, {"ncat": 437, "cat": 10, "cat25k": 222, "bg": 6.4494740627042105e-06, "x": 0.9894366197183099, "s": 0.8904049295774648, "ncat25k": 121, "os": 0.5627342543674037, "y": 0.9903169014084507, "term": "language"}, {"ncat": 481, "cat": 11, "cat25k": 244, "bg": 4.1107687118817306e-07, "x": 0.9898767605633803, "s": 0.8899647887323944, "ncat25k": 133, "os": 0.5626680826179068, "y": 0.991637323943662, "term": "are"}, {"ncat": 499, "cat": 0, "cat25k": 0, "bg": 0.0001851341025735681, "x": 0.9903169014084507, "s": 0.0, "ncat25k": 138, "os": 0.0, "y": 0.07570422535211267, "term": "neural"}, {"ncat": 511, "cat": 4, "cat25k": 89, "bg": 6.783613913725343e-07, "x": 0.9907570422535211, "s": 0.06558098591549297, "ncat25k": 141, "os": 0.06478090265137731, "y": 0.9757922535211268, "term": "an"}, {"ncat": 522, "cat": 10, "cat25k": 222, "bg": 3.17598011682185e-07, "x": 0.9911971830985915, "s": 0.8824823943661972, "ncat25k": 144, "os": 0.5452087912917859, "y": 0.9907570422535211, "term": "by"}, {"ncat": 537, "cat": 9, "cat25k": 200, "bg": 4.798537412124527e-07, "x": 0.991637323943662, "s": 0.8750000000000001, "ncat25k": 148, "os": 0.5336690633747962, "y": 0.9889964788732394, "term": "from"}, {"ncat": 538, "cat": 4, "cat25k": 89, "bg": 4.293013863294226e-06, "x": 0.9920774647887324, "s": 0.06426056338028169, "ncat25k": 149, "os": 0.06148350102056299, "y": 0.9775528169014085, "term": "based"}, {"ncat": 541, "cat": 2, "cat25k": 44, "bg": 1.099530837481354e-05, "x": 0.9925176056338029, "s": 0.04005281690140846, "ncat25k": 150, "os": 0.03034627635993864, "y": 0.9427816901408451, "term": "word"}, {"ncat": 569, "cat": 4, "cat25k": 89, "bg": 5.098931216498187e-07, "x": 0.9929577464788732, "s": 0.0602992957746479, "ncat25k": 157, "os": 0.058088394354677, "y": 0.9744718309859155, "term": "as"}, {"ncat": 663, "cat": 0, "cat25k": 0, "bg": 8.22050708030539e-06, "x": 0.9933978873239436, "s": 0.0, "ncat25k": 183, "os": 0.0, "y": 0.021566901408450703, "term": "model"}, {"ncat": 742, "cat": 10, "cat25k": 222, "bg": 3.7664983395852865e-05, "x": 0.9938380281690141, "s": 0.8635563380281691, "ncat25k": 205, "os": 0.5175326299171519, "y": 0.9911971830985915, "term": "task"}, {"ncat": 773, "cat": 4, "cat25k": 89, "bg": 1.5557793853332696e-06, "x": 0.9942781690140845, "s": 0.04797535211267606, "ncat25k": 214, "os": 0.04260320656153305, "y": 0.9727112676056338, "term": "our"}, {"ncat": 816, "cat": 14, "cat25k": 311, "bg": 3.5275301278852224e-07, "x": 0.9947183098591549, "s": 0.8772007042253522, "ncat25k": 226, "os": 0.5355947696238966, "y": 0.9947183098591549, "term": "is"}, {"ncat": 822, "cat": 5, "cat25k": 111, "bg": 5.196014434386104e-07, "x": 0.9951584507042254, "s": 0.055897887323943664, "ncat25k": 227, "os": 0.050169368530515956, "y": 0.9810739436619719, "term": "with"}, {"ncat": 884, "cat": 12, "cat25k": 266, "bg": 5.550448059269142e-07, "x": 0.9955985915492958, "s": 0.8639964788732395, "ncat25k": 244, "os": 0.5180178197219906, "y": 0.9925176056338029, "term": "this"}, {"ncat": 1024, "cat": 9, "cat25k": 200, "bg": 6.076238885162441e-07, "x": 0.9960387323943662, "s": 0.07306338028169015, "ncat25k": 283, "os": 0.07286895444239039, "y": 0.9894366197183099, "term": "that"}, {"ncat": 1171, "cat": 13, "cat25k": 289, "bg": 6.313787959117789e-07, "x": 0.9964788732394366, "s": 0.08934859154929578, "ncat25k": 324, "os": 0.09243671006426474, "y": 0.9933978873239436, "term": "on"}, {"ncat": 1554, "cat": 22, "cat25k": 488, "bg": 5.312281239179934e-07, "x": 0.996919014084507, "s": 0.8653169014084509, "ncat25k": 430, "os": 0.5209065813569601, "y": 0.996919014084507, "term": "for"}, {"ncat": 1670, "cat": 14, "cat25k": 311, "bg": 2.4216955545861534e-06, "x": 0.9973591549295775, "s": 0.07086267605633804, "ncat25k": 462, "os": 0.06945100939010673, "y": 0.9942781690140845, "term": "we"}, {"ncat": 2077, "cat": 24, "cat25k": 533, "bg": 4.961329150762523e-07, "x": 0.9977992957746479, "s": 0.09110915492957747, "ncat25k": 574, "os": 0.0962917885371734, "y": 0.9973591549295775, "term": "in"}, {"ncat": 2260, "cat": 28, "cat25k": 622, "bg": 3.770264141610288e-07, "x": 0.9982394366197183, "s": 0.10211267605633804, "ncat25k": 625, "os": 0.10339823207725518, "y": 0.9982394366197183, "term": "to"}, {"ncat": 2490, "cat": 37, "cat25k": 821, "bg": 5.565297983705755e-07, "x": 0.9986795774647887, "s": 0.8683978873239437, "ncat25k": 688, "os": 0.524374801945089, "y": 0.9991197183098591, "term": "a"}, {"ncat": 2663, "cat": 34, "cat25k": 755, "bg": 4.149952804196269e-07, "x": 0.9991197183098591, "s": 0.8609154929577465, "ncat25k": 736, "os": 0.5139941408899462, "y": 0.9986795774647887, "term": "and"}, {"ncat": 3198, "cat": 45, "cat25k": 999, "bg": 4.93155301929523e-07, "x": 0.9995598591549296, "s": 0.8648767605633803, "ncat25k": 884, "os": 0.5204821310832143, "y": 0.9995598591549296, "term": "of"}, {"ncat": 4349, "cat": 68, "cat25k": 1510, "bg": 3.818299580790117e-07, "x": 1.0, "s": 0.8705985915492958, "ncat25k": 1202, "os": 0.5281874196223687, "y": 1.0, "term": "the"}], "docs": {"categories": ["Other venue", "BUCC"], "texts": ["Abusive Language Detection on Arabic Social Media.\n\nIn this paper, we present our work on detecting abusive language on Arabic social media. We extract a list of obscene words and hashtags using common patterns used in offensive and rude communications. We also classify Twitter users according to whether they use any of these words or not in their tweets. We expand the list of obscene words using this classification, and we report results on a newly created dataset of classified Arabic tweets (obscene, offensive, and clean). We make this dataset freely available for research, in addition to the list of obscene words and hashtags. We are also publicly releasing a large corpus of classified user comments that were deleted from a popular Arabic news site due to violations the site's rules and guidelines.", "Dimensions of Abusive Language on Twitter.\n\nIn this paper, we use a new categorical form of multidimensional register analysis to identify the main dimensions of functional linguistic variation in a corpus of abusive language, consisting of racist and sexist Tweets. By analysing the use of a wide variety of parts-of-speech and grammatical constructions, as well as various features related to Twitter and computer-mediated communication, we discover three dimensions of linguistic variation in this corpus, which we interpret as being related to the degree of interactive, antagonistic and attitudinal language exhibited by individual Tweets. We then demonstrate that there is a significant functional difference between racist and sexist Tweets, with sexists Tweets tending to be more interactive and attitudinal than racist Tweets.", "Constructive Language in News Comments.\n\nWe discuss the characteristics of constructive news comments, and present methods to identify them. First, we define the notion of constructiveness. Second, we annotate a corpus for constructiveness. Third, we explore whether available argumentation corpora can be useful to identify constructiveness in news comments. Our model trained on argumentation corpora achieves a top accuracy of 72.59% (baseline=49.44%) on our crowd-annotated test data. Finally, we examine the relation between constructiveness and toxicity. In our crowd-annotated data, 21.42% of the non-constructive comments and 17.89% of the constructive comments are toxic, suggesting that non-constructive comments are not much more toxic than constructive comments.", "Vectors for Counterspeech on Twitter.\n\nA study of conversations on Twitter found that some arguments between strangers led to favorable change in discourse and even in attitudes. The authors propose that such exchanges can be usefully distinguished according to whether individuals or groups take part on each side, since the opportunity for a constructive exchange of views seems to vary accordingly.", "Detecting Nastiness in Social Media.\n\nAlthough social media has made it easy for people to connect on a virtually unlimited basis, it has also opened doors to people who misuse it to undermine, harass, humiliate, threaten and bully others. There is a lack of adequate resources to detect and hinder its occurrence. In this paper, we  present our initial NLP approach to detect invective posts as a first step to eventually detect and deter cyberbullying. We crawl data containing profanities and then determine whether or not it contains invective. Annotations on this data are improved iteratively by in-lab annotations and crowdsourcing. We pursue different NLP approaches containing various typical and some newer techniques to distinguish the use of swear words in a neutral way from those instances in which they are used in an insulting way. We also show that this model not only works for our data set, but also can be successfully applied to different data sets.", "Rephrasing Profanity in Chinese Text.\n\nThis paper proposes a system that can detect and rephrase profanity in Chinese text.  Rather than just masking detected profanity, we want to revise the input sentence by using inoffensive words while keeping their original meanings.  29 of such rephrasing rules were invented after observing sentences on real-word social websites.  The overall accuracy of the proposed system is 85.56%", "Technology Solutions to Combat Online Harassment.\n\nThis work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes under-reported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to fill out forms providing context. In addition, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media companies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross-platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classifier and the dataset used to help the system learn what harassment looks like.", "Understanding Abuse: A Typology of Abusive Language Detection Subtasks.\n\nAs the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.", "Using Convolutional Neural Networks to Classify Hate-Speech.\n\nThe paper introduces a deep learning-based Twitter hate-speech text classification system. The classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sexism) and non-hate-speech. Four Convolutional Neural Network models were trained on resp. character 4-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams. The feature set was down-sized in the networks by max-pooling, and a softmax function used to classify tweets. Tested by 10-fold cross-validation, the model based on word2vec embeddings performed best, with higher precision than recall, and a 78.3% F-score.", "Illegal is not a Noun: Linguistic Form for Detection of Pejorative Nominalizations.\n\nThis paper focuses on a particular type of abusive language, targeting expressions in which typically neutral adjectives take on pejorative meaning when used as nouns - compare 'gay people' to 'the gays'. We first collect and analyze a corpus of hand-curated, expert-annotated pejorative nominalizations for four target adjectives: female, gay, illegal, and poor. We then collect a second corpus of automatically-extracted and POS-tagged, crowd-annotated tweets. For both corpora, we find support for the hypothesis that some adjectives, when nominalized, take on negative meaning. The targeted constructions are non-standard yet widely-used, and part-of-speech taggers mistag some nominal forms as adjectives. We implement a tool called NomCatcher to correct these mistaggings, and find that the same tool is effective for identifying new adjectives subject to transformation via nominalization into abusive language.", "Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary Words.\n\nCommon approaches  to text categorization essentially rely either on n-gram counts or on word embeddings. This presents important difficulties in highly dynamic or quickly-interacting environments, where the appearance of new words and/or varied misspellings is the norm. A paradigmatic example of this situation is abusive online behavior, with social networks and media platforms struggling to effectively combat uncommon or non-blacklisted hate words. To better deal with these issues in those fast-paced environments, we propose using the error signal of class-based language models as input to text classification algorithms. In particular, we train a next-character prediction model for any given class and then exploit the error of such class-based models to inform a neural network classifier. This way, we shift from the \u2018ability to describe' seen documents to the \u2018ability to predict' unseen content. Preliminary studies using out-of-vocabulary splits from abusive tweet data show promising results, outperforming competitive text categorization strategies by 4-11%.", "Deep Learning for User Comment Moderation.\n\nExperimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of EnglishWikipedia comments, we show that an RNN outperforms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.", "One-step and Two-step Classification for Abusive Language Detection on Twitter.\n\nAutomatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps.", "Legal Framework, Dataset and Annotation Schema for Socially Unacceptable Online Discourse Practices in Slovene.\n\nIn this paper we present the legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multicultural information society.", "Annotation of pain and anesthesia events for surgery-related processes and outcomes extraction.\n\nPain and anesthesia information are crucial elements to identifying surgery-related processes and outcomes. However pain is not consistently recorded in the electronic medical record. Even when recorded, the rich complex granularity of the pain experience may be lost. Similarly, anesthesia information is recorded using local electronic collection systems; though the accuracy and completeness of the information is unknown. We propose an annotation schema to capture pain, pain management, and anesthesia event information.", "Tagging Funding Agencies and Grants in Scientific Articles using Sequential Learning Models.\n\nIn this paper we present a solution for tagging funding bodies and grants in scientific articles using a combination of trained sequential learning models, namely conditional random fields (CRF), hidden markov models (HMM) and maximum entropy models (MaxEnt), on a benchmark set created in-house. We apply the trained models to address the BioASQ challenge 5c, which is a newly introduced task that aims to solve the problem of funding information extraction from scientific articles. Results in the dry-run data set of BioASQ task 5c show that the suggested approach can achieve a micro-recall of more than 85% in tagging both funding bodies and grants.", "Unsupervised Context-Sensitive Spelling Correction of Clinical Free-Text with Word and Character N-Gram Embeddings.\n\nWe present an unsupervised context-sensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings. Our method generates misspelling replacement candidates and ranks them according to their semantic fit, by calculating a weighted cosine similarity between the vectorized representation of a candidate and the misspelling context. We greatly outperform two baseline off-the-shelf spelling correction tools on a manually annotated MIMIC-III test set, and counter the frequency bias of an optimized noisy channel model, showing that neural embeddings can be successfully exploited to include context-awareness in a spelling correction model.", "Adapting Pre-trained Word Embeddings For Use In Medical Coding.\n\nWord embeddings are a crucial component in modern NLP. Pre-trained embeddings released by different groups have been a major reason for their popularity. However, they are trained on generic corpora, which limits their direct use for domain specific tasks. In this paper, we propose a method to add task specific information to pre-trained word embeddings. Such information can improve their utility. We add information from medical coding data, as well as the first level from the hierarchy of ICD-10 medical code set to different pre-trained word embeddings. We adapt CBOW algorithm from the word2vec package for our purpose. We evaluated our approach on five different pre-trained word embeddings. Both the original word embeddings, and their modified versions (the ones with added information) were used for automated review of medical coding. The modified word embeddings give an improvement in f-score by 1% on the 5-fold evaluation on a private medical claims dataset. Our results show that adding extra information is possible and beneficial for the task at hand.", "Unsupervised Domain Adaptation for Clinical Negation Detection.\n\nDetecting negated concepts in clinical texts is an important part of NLP information extraction systems. However, generalizability of negation systems is lacking, as cross-domain experiments suffer dramatic performance losses. We examine the performance of multiple unsupervised domain adaptation algorithms on clinical negation detection, finding only modest gains that fall well short of in-domain performance.", "Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models.\n\nBiomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex hand-crafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.", "Insights into Analogy Completion from the Biomedical Domain.\n\nAnalogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains.  Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs.  We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods.", "Neural Question Answering at BioASQ 5B.\n\nThis paper describes our submission to the 2017 BioASQ challenge. We participated in Task B, Phase B which is concerned with biomedical question answering (QA). We focus on factoid and list question, using an extractive QA model, that is, we restrict our system to output  substrings of the provided text snippets. At the core of our system, we use FastQA, a state-of-the-art neural QA system. We extended it with biomedical word embeddings and changed its answer layer to be able to answer list questions in addition to factoid questions. We pre-trained the model on a large-scale open-domain QA dataset, SQuAD, and then fine-tuned the parameters on the BioASQ training set. With our approach, we achieve state-of-the-art results on factoid questions and competitive results on list questions.", "Deep Learning for Biomedical Information Retrieval: Learning Textual Relevance from Click Logs.\n\nWe describe a Deep Learning approach to modeling the relevance of a document's text to a query, applied to biomedical literature. Instead of mapping each document and query to a common semantic space, we compute a variable-length difference vector between the query and document which is then passed through a deep convolution stage followed by a deep regression network to produce the estimated probability of the document's relevance to the query. Despite the small amount of training data, this approach produces a more robust predictor than computing similarities between semantic vector representations of the query and document, and also results in significant improvements over traditional IR text factors. In the future, we plan to explore its application in improving PubMed search.", "Extracting Personal Medical Events for User Timeline Construction using Minimal Supervision.\n\nIn this paper, we describe a system for automatic construction of user disease progression timelines from their posts in online support groups using minimal supervision. In recent years, several online support groups have been established which has led to a huge increase in the amount of patient-authored text available. Creating systems which can automatically extract important medical events and create disease progression timelines for users from such text can help in patient health monitoring as well as studying links between medical events and users' participation in support groups. Prior work in this domain has used manually constructed keyword sets to detect medical events. In this work, our aim is to perform medical event detection using minimal supervision in order to develop a more general timeline construction system. Our system achieves an accuracy of 55.17%, which is 92\\% of the performance achieved by a supervised baseline system.", "Tackling Biomedical Text Summarization: OAQA at BioASQ 5B.\n\nIn this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes-no and summary questions from biomedical data. We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise, non-redundant, query-oriented summary from multiple relevant documents. We make use of extractive summarization techniques to address this task and experiment with different biomedical ontologies and various algorithms including agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence compression. We propose a novel word embedding based tf-idf similarity metric and a soft positional constraint which improve our system performance. We evaluate our techniques on test batch 4 from the fourth edition of the challenge. Our best system achieves a ROUGE-2 score of 0.6534 and ROUGE-SU4 score of 0.6536.", "Detecting Dementia through Retrospective Analysis of Routine Blog Posts by Bloggers with Dementia.\n\nWe investigate if writers with dementia can be automatically distinguished from those without by analyzing linguistic markers in written text, in the form of blog posts. We have built a corpus of several thousand blog posts, some by people with dementia and others by people with loved ones with dementia. We use this dataset to train and test several machine learning methods, and achieve prediction performance at a level far above the baseline.", "Role-Preserving Redaction of Medical Records to Enable Ontology-Driven Processing.\n\nElectronic medical records (EMR) have largely replaced hand-written patient files in healthcare. The growing pool of EMR data presents a significant resource in medical research, but the U.S. Health Insurance Portability and Accountability Act (HIPAA) mandates redacting medical records before performing any analysis on the same. This process complicates obtaining medical data and can remove much useful information from the record. As part of a larger project involving ontology-driven medical processing, we employ a method of recognizing protected health information (PHI) that maps to ontological terms. We then use the relationships defined in the ontology to redact medical texts so that roles and semantics of terms are retained without compromising anonymity. The method is evaluated by clinical experts on several hundred medical documents, achieving up to a 98.8% f-score, and has already shown promise for retaining semantic information in later processing.", "Automated Preamble Detection in Dictated Medical Reports.\n\nDictated medical reports very often feature a preamble containing metainformation about the report such as patient and physician names, location and name of the clinic, date of procedure, and so on. In the medical transcription process, the preamble is usually omitted from the final report, as it contains information already available in the electronic medical record. We present a method which is able to automatically identify preambles in medical dictations. The method makes use of stateof- the-art NLP techniques including word embeddings and Bi-LSTMs and achieves preamble detection performance superior to humans.", "Initializing neural networks for hierarchical multi-label text classification.\n\nMany tasks in the biomedical domain require the assignment of one or more predefined labels to input text, where the labels are a part of a hierarchical structure (such as a taxonomy). The conventional approach is to use a one-vs.-rest (OVR) classification setup, where a binary classifier is trained for each label in the taxonomy or ontology where all instances not belonging to the class are considered negative examples. The main drawbacks to this approach are that dependencies between classes are not leveraged in the training and classification process, and the additional computational cost of training parallel classifiers. In this paper, we apply a new method for hierarchical multi-label text classification that initializes a neural network model final hidden layer such that it leverages label co-occurrence relations such as hypernymy. This approach elegantly lends itself to hierarchical classification. We evaluated this approach using two hierarchical multi-label text classification tasks in the biomedical domain using both sentence- and document-level classification. Our evaluation shows promising results for this approach.", "Creation and evaluation of a dictionary-based tagger for virus species and proteins.\n\next mining automatically extracts information from the literature with the goal of making it available for further analysis, for example by incorporating it into biomedical databases.  A key first step towards this goal is to identify and normalize the named entities, such as proteins and species, which are mentioned in text.  Despite the large detrimental impact that viruses have on human and agricultural health, very little previous text-mining work has focused on identifying virus species and proteins in the literature.  Here, we present an improved dictionary-based system for viral species and the first dictionary for viral proteins, which we benchmark on a new corpus of 300 manually annotated abstracts.  We achieve 81.0\\% precision and 72.7\\% recall at the task of recognizing and normalizing viral species and 76.2\\% precision and 34.9\\% recall on viral proteins.  These results are achieved despite the many challenges involved with the names of viral species and, especially, proteins. This work provides a foundation that can be used to extract more complicated relations about viruses from the literature.", "Biomedical Event Extraction using Abstract Meaning Representation.\n\nWe propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events/interactions in biomedical text. Our key contributions are: (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset and show promising results.", "Enhancing Automatic ICD-9-CM Code Assignment for \\\\Medical Texts with PubMed.\n\nAssigning a standard ICD-9-CM code to disease symptoms in medical texts is an important task in the medical domain. Automating this process could greatly reduce the costs. However, the effectiveness of an automatic ICD-9-CM code classifier faces a serious problem, which can be triggered by unbalanced training data. Frequent diseases often have more training data, which helps its classification to perform better than that of an infrequent disease. However, a disease's frequency does not necessarily reflect its importance. To resolve this training data shortage problem, we propose to strategically draw data from PubMed to enrich the training data when there is such need. We validate our method on the CMC dataset, and the evaluation results indicate that our method can significantly improve the code assignment classifiers' performance at the macro-averaging level.", "Characterization of Divergence in Impaired Speech of ALS Patients.\n\nApproximately 80% to 95% of patients with Amyotrophic Lateral Sclerosis (ALS) eventually develop speech impairments, such as defective articulation, slow laborious speech and hypernasality. The relationship between impaired speech and asymptomatic speech may be seen as a divergence from a baseline. This relationship can be characterized in terms of measurable combinations of phonological characteristics that are indicative of the degree to which the two diverge. We demonstrate that divergence measurements based on phonological characteristics of speech correlate with physiological assessments of ALS. Speech-based assessments offer benefits over commonly-used physiological assessments in that they are inexpensive, non-intrusive, and do not require trained clinical personnel for administering and interpreting the results.", "Protein Word Detection using Text Segmentation Techniques.\n\nLiterature in Molecular Biology is abundant with linguistic metaphors. There have been works in the past that attempt to draw parallels between linguistics and biology, driven by the fundamental premise that proteins have a language of their own. Since word detection is crucial to the decipherment of any  unknown language, we attempt to establish a problem mapping from natural language text to protein sequences at the level of words. Towards this end, we explore the use of an unsupervised text segmentation algorithm to the task of extracting ``biological words'' from protein sequences. In particular, we demonstrate the effectiveness of using domain knowledge to complement data driven approaches in the text segmentation task, as well as in its biological counterpart. We also propose a novel extrinsic evaluation measure for protein words through protein family classification.", "Deep Learning for Punctuation Restoration in Medical Reports.\n\nIn clinical dictation, speakers try to be as concise as possible to save time, often resulting in utterances without explicit punctuation commands.  Since the end product of a dictated report, e.g. an out-patient letter, does require correct orthography, including exact punctuation, the latter need to be restored, preferably by automated means.  This paper describes a method for punctuation restoration based on a state-of-the-art stack of NLP and machine learning techniques including B-RNNs with an attention mechanism and late fusion, as well as a feature extraction technique tailored to the processing of medical terminology using a novel vocabulary reduction model.  To the best of our knowledge, the resulting performance is superior to that reported in prior art on similar tasks.", "Investigating the Documentation of Electronic Cigarette Use in the Veteran Affairs Electronic Health Record: A Pilot Study.\n\nIn this paper, we present pilot work on characterising the documentation of electronic cigarettes (e-cigarettes) in the United States Veterans Administration Electronic Health Record.  The Veterans Health Administration is the largest health care system in the United States with 1,233 health care facilities nationwide, serving 8.9 million veterans per year.   We identified a random sample of 2000 Veterans Administration patients, coded as current tobacco users, from 2008 to 2014.                       Using simple keyword matching techniques combined with qualitative analysis, we investigated the prevalence and distribution of e-cigarette terms in these clinical notes, discovering that for current smokers, 11.9\\% of  patient records contain an e-cigarette related term.", "Automatic Diagnosis Coding of Radiology Reports: A Comparison of Deep Learning and Conventional Classification Methods.\n\nDiagnosis autocoding services and research intend to both improve the productivity of clinical coders and the accuracy of the coding. It is an important step in data analysis for funding and reimbursement, as well as health services planning and resource allocation. We investigate the applicability of deep learning at autocoding of radiology reports using International Classification of Diseases (ICD). Deep learning methods are known to require large training data. Our goal is to explore how to use these methods when the training data is sparse, skewed and relatively small, and how their effectiveness compares to conventional methods. We identify optimal parameters that could be used in setting up a convolutional neural network for autocoding with comparable results to that of conventional methods.", "Stacking With Auxiliary Features for Entity Linking in the Medical Domain.\n\nLinking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the semantics of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the Unified Medical Language System. Different approaches have different strengths: some are precision-oriented, some recall-oriented; some better at considering context but more prone to hallucination. The variety of techniques suggests that ensembling could outperform component technologies at this task. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best model produces state-of-the-art results on several medical datasets.", "Proactive Learning for Named Entity Recognition.\n\nThe goal of active learning is to minimise the cost of producing an annotated dataset, in which annotators are assumed to be perfect, i.e., they always choose the correct labels. However, in practice, annotators are not infallible, and they are likely to assign incorrect labels to some instances. Proactive learning is a generalisation of active learning that can model different kinds of annotators. Although proactive learning has been applied to certain labelling tasks, such as text classification, there is little work on its application to named entity (NE) tagging. In this paper, we propose a proactive learning method for producing NE annotated corpora, using two annotators with different levels of expertise, and who charge different amounts based on their levels of experience. To optimise both cost and annotation quality, we also propose a mechanism to present multiple sentences to annotators at each iteration. Experimental results for several corpora show that our method facilitates the construction of high-quality NE labelled datasets at minimal cost.", "Assessing the performance of Olelo, a real-time biomedical question answering application.\n\nQuestion answering (QA) can support physicians and biomedical researchers to find answers to their questions in the scientific literature. Such systems process large collections of documents in real time and include many natural language processing (NLP) procedures. We recently developed Olelo, a QA system for biomedicine which includes various NLP components, such as question processing, document and passage retrieval, answer processing and multi-document summarization. In this work, we present an evaluation of our system on the the fifth BioASQ challenge. We participated with the current state of the application and with an extension based on semantic role labeling that we are currently investigating. In addition to the BioASQ evaluation, we compared our system to other on-line biomedical QA systems in terms of the response time and the quality of the answers.", "External Evaluation of Event Extraction Classifiers for Automatic Pathway Curation: An extended study of the mTOR pathway.\n\nThis paper evaluates the impact of various event extraction systems on automatic pathway curation using the popular mTOR pathway. We quantify the impact of training data sets as well as different machine learning classifiers and show that some improve the quality of automatically extracted pathways.", "Macquarie University at BioASQ 5b -- Query-based Summarisation Techniques for Selecting the Ideal Answers.\n\nMacquarie University's contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a trivial system that selected the first $n$ snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.", "Extracting Drug-Drug Interactions with Attention CNNs.\n\nWe propose a novel attention mechanism for a Convolutional Neural Network (CNN)-based Drug-Drug Interaction (DDI) extraction model. CNNs have been shown to have a great potential on DDI extraction tasks; however, attention mechanisms, which emphasize important words in the sentence of a target-entity pair, have not been investigated with the CNNs despite the fact that attention mechanisms are shown to be effective for a general domain relation classification task. We evaluated our model on the Task 9.2 of the DDIExtraction-2013 shared task. As a result, our attention mechanism improved the performance of our base CNN-based DDI model, and the model achieved an F-score of 69.12%, which is competitive with the state-of-the-art models.", "End-to-End System for Bacteria Habitat Extraction.\n\nWe introduce an end-to-end system capable of named-entity detection, normalization and relation extraction for extracting information about bacteria and their habitats from biomedical literature. Our system is based on deep learning, CRF classifiers and vector space models. We train and evaluate the system on the BioNLP 2016 Shared Task Bacteria Biotope data. The official evaluation shows that the joint performance of our entity detection and relation extraction models outperforms the winning team of the Shared Task by 19pp on F1-score, establishing a new top score for the task. We also achieve state-of-the-art results in the normalization task. Our system is open source and freely available at https://github.com/TurkuNLP/BHE.", "A Multi-strategy Query Processing Approach for Biomedical Question Answering: USTB\\\\_PRIR at BioASQ 2017 Task 5B.\n\nThis paper describes the participation of USTB\\_PRIR team in the 2017 BioASQ 5B on question answering, including document retrieval, snippet retrieval, and concept retrieval task. We introduce different multimodal query processing strategies to enrich query terms and assign different weights to them. Specifically, sequential dependence model (SDM), pseudo-relevance feedback (PRF), fielded sequential dependence model (FSDM) and Divergence from Randomness model (DFRM) are respectively performed on different fields of PubMed articles, sentences extracted from relevant articles, the five terminologies or ontologies (MeSH, GO, Jochem, Uniprot and DO) to achieve better search performances. Preliminary results show that our systems outperform others in the document and snippet retrieval task in the first two batches.", "Automatic classification of doctor-patient questions for a virtual patient record query task.\n\nWe present the work-in-progress of automating the classification of doctor-patient questions in the context of a simulated consultation with a virtual patient. We classify questions according to the computational strategy (rule-based or other) needed for looking up data in the clinical record. We compare \u2018traditional' machine learning methods (Gaussian and Multinomial Naive Bayes, and Support Vector Machines) and a neural network classifier (FastText). We obtained the best results with the SVM using semantic annotations, whereas the neural classifier achieved promising results without it.", "A Biomedical Question Answering System in BioASQ 2017.\n\nQuestion answering, the identification of short accurate answers to users questions, is a longstanding challenge widely studied over the last decades in the open domain. However, it still requires further efforts in the biomedical domain. In this paper, we describe our participation in phase B of task 5b in the 2017 BioASQ challenge using our biomedical question answering system. Our system, dealing with four types of questions (i.e., yes/no, factoid, list, and summary), is based on (1) a dictionary-based approach for generating the exact answers of yes/no questions, (2) UMLS metathesaurus and term frequency metric for extracting the exact answers of factoid and list questions, and (3) the BM25 model and UMLS concepts for retrieving the ideal answers (i.e., paragraph-sized summaries). Preliminary results show that our system achieves good and competitive results in both exact and ideal answers extraction tasks as compared with the participating systems.", "Evaluating Feature Extraction Methods for Knowledge-based Biomedical Word Sense Disambiguation.\n\nIn this paper, we present an analysis of feature extraction methods via dimensionality reduction for the task of biomedical Word Sense Disambiguation (WSD). We modify the vector representations in the 2-MRD WSD algorithm, and evaluate four dimensionality reduction methods: Word Embeddings using Continuous Bag of Words and Skip Gram, Singular Value Decomposition (SVD), and Principal Component Analysis (PCA). We also evaluate the effects of vector size on the performance of each of these methods. Results are evaluated on five standard evaluation datasets (Abbrev.100, Abbrev.200, Abbrev.300, NLM-WSD, and MSH-WSD). We find that vector sizes of 100 are sufficient for all techniques except SVD, for which a vector size of 1500 is referred. We also show that SVD performs on par with Word Embeddings for all but one dataset.", "Identifying Comparative Structures in Biomedical Text.\n\nComparison sentences are very commonly used by authors in biomedical literature to report results of experiments. In such comparisons, authors typically make observations under two different scenarios. In this paper, we present a system to automatically identify such comparative sentences and their components i.e. the compared entities, the scale of the comparison and the aspect on which the entities are being compared. Our methodology is based on dependencies obtained by applying a parser to extract a wide range of comparison structures. We evaluated our system for its effectiveness in identifying comparisons and their components. The system achieved a F-score of 0.87 for comparison sentence identification and 0.77-0.81 for identifying its components.", "Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors.\n\nVector space methods that measure semantic similarity and relatedness often rely on distributional information such as co--occurrence frequencies or statistical measures of association to weight the importance of particular co--occurrences. In this paper, we extend these methods by incorporating a measure of semantic similarity based on a human curated taxonomy into a second--order vector representation. This results in a measure of semantic relatedness that combines both the contextual  information available in a corpus--based vector space representation with the semantic knowledge found in a biomedical ontology. Our results show that incorporating semantic similarity into a second order co-occurrence matrices improves correlation with human judgments for both similarity and relatedness, and that our method compares favorably to various different word embedding methods that have recently been evaluated on the same reference standards we have used.", "Detecting Personal Medication Intake in Twitter: An Annotated Corpus and Baseline Classification System.\n\nSocial media sites (e.g., Twitter) have been used for surveillance of drug safety at the population level, but studies that focus on the effects of medications on specific sets of individuals have had to rely on other sources of data. Mining social media data for this in-formation would require the ability to distinguish indications of personal medication in-take in this media. Towards that end, this paper presents an annotated corpus that can be used to train machine learning systems to determine whether a tweet that mentions a medication indicates that the individual posting has taken that medication at a specific time. To demonstrate the utility of the corpus as a training set, we present baseline results of supervised classification.", "Detecting mentions of pain and acute confusion in Finnish clinical text.\n\nWe study and compare two different approaches to the task of automatic assignment of predefined classes to clinical free-text narratives. In the first approach this is treated as a traditional mention-level named-entity recognition task, while the second approach treats it as a sentence-level multi-label classification task. Performance comparison across these two approaches is conducted in the form of sentence-level evaluation and state-of-the-art methods for both approaches are evaluated. The experiments are done on two data sets consisting of Finnish clinical text, manually annotated with respect to the topics pain and acute confusion. Our results suggest that the mention-level named-entity recognition approach outperforms sentence-level classification overall, but the latter approach still manages to achieve the best prediction scores on several annotation classes.", "Representation of complex terms in a vector space structured by an ontology for a normalization task.\n\nWe propose in this paper a semi-supervised method for labeling terms of texts with concepts of a domain ontology. The method generates continuous vector representations of complex terms in a semantic space structured by the ontology. The proposed method relies on a distributional semantics approach, which generates initial vectors for each of the extracted terms. Then these vectors are embedded in the vector space constructed from the structure of the ontology. This embedding is carried out by training a linear model. Finally, we apply a distance calculation to determine the proximity between vectors of terms and vectors of concepts and thus to assign ontology labels to terms. We have evaluated the quality of these representations for a normalization task by using the concepts of an ontology as semantic labels. Normalization of terms is an important step to extract a part of the information containing in texts, but the vector space generated might find other applications. The performance of this method is comparable to that of the state of the art for this task of standardization, opening up encouraging prospects.", "Painless Relation Extraction with Kindred.\n\nRelation extraction methods are essential for creating robust text mining tools to help researchers find useful knowledge in the vast published literature. Easy-to-use and generalizable methods are needed to encourage an ecosystem in which researchers can easily use shared resources and build upon each others' methods. We present the Kindred Python package for relation extraction. It builds upon methods from the most successful tools in the recent BioNLP Shared Task to predict high-quality predictions with low computational cost. It also integrates with PubAnnotation, PubTator, and BioNLP Shared Task data in order to allow easy development and application of relation extraction models.", "Clinical Event Detection with Hybrid Neural Architecture.\n\nEvent detection from clinical notes has been traditionally solved with rule based and statistical natural language processing (NLP) approaches that require extensive domain knowledge and feature engineering. In this paper, we have explored the feasibility of approaching this task with recurrent neural networks, clinical word embeddings and introduced a hybrid architecture to improve detection for entities with smaller representation in the dataset. A comparative analysis is also done which reveals the complementary behavior of neural networks and conditional random fields in clinical entity detection.", "Deep learning for extracting protein-protein interactions from biomedical literature.\n\nState-of-the-art methods for protein-protein interaction (PPI) extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information. But how to incorporate such knowledge in the recent deep learning methods remains an open question. In this paper, we propose a multichannel dependency-based convolutional neural network model (McDepCNN). It applies one channel to the embedding vector of each word in the sentence, and another channel to the embedding vector of the head of the corresponding word. Therefore, the model can use richer information obtained from different channels. Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN provides up to 6% F1-score improvement over rich feature-based methods and single-kernel methods. In addition, McDepCNN achieves 24.4% relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12% improvement in F1-score over kernel-based methods on ``difficult'' instances. These results suggest that McDepCNN generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences.", "Noise Reduction Methods for Distantly Supervised Biomedical Relation Extraction.\n\nDistant supervision has been applied to automatically generate labeled data for biomedical relation extraction. Noise exists in both positively and negatively-labeled data and affects the performance of supervised machine learning methods. In this paper, we propose three novel heuristics based on the notion of proximity, trigger word and confidence of patterns to leverage lexical and syntactic information to reduce the level of noise in the distantly labeled data. Experiments on three different tasks, extraction of protein-protein-interaction, miRNA-gene regulation relation and protein-localization event, show that the proposed methods can improve the F-score over the baseline by 6, 10 and 14 points for the three tasks, respectively. We also show that when the models are configured to output high-confidence results, high precisions can be obtained using the proposed methods, making them promising for facilitating manual curation for databases.", "Target word prediction and paraphasia classification in spoken discourse.\n\nWe present a system for automatically detecting and classifying phonologically anomalous productions in the speech of individuals with aphasia. Working from transcribed discourse samples, our system identifies neologisms, and uses a combination of string alignment and language models to produce a lattice of plausible words that the speaker may have intended to produce. We then score this lattice according to various features, and attempt to determine whether the anomalous production represented a phonemic error or a genuine neologism. This approach has the potential to be expanded to consider other types of paraphasic errors, and could be applied to a wide variety of screening and therapeutic applications.", "BioCreative VI Precision Medicine Track: creating a training corpus for mining protein-protein interactions affected by mutations.\n\nThe Precision Medicine Track in BioCre-ative VI aims to bring together the Bi-oNLP community for a novel challenge focused on mining the biomedical litera-ture in search of mutations and protein-protein interactions (PPI). In order to support this track with an effective train-ing dataset with limited curator time, the track organizers carefully reviewed Pub-Med articles from two different sources: curated public PPI databases, and the re-sults of state-of-the-art public text mining tools. We detail here the data collection, manual review and annotation process and describe this training corpus charac-teristics. We also describe a corpus per-formance baseline. This analysis will provide useful information to developers and researchers for comparing and devel-oping innovative text mining approaches for the BioCreative VI challenge and other Precision Medicine related applica-tions.", "Results of the fifth edition of the BioASQ Challenge.\n\nThe goal of the BioASQ challenge is to engage researchers into creating cuttingedge biomedical information systems. Specifically, it aims at the promotion of systems and methodologies that are able to deal with a plethora of different tasks in the biomedical domain. This is achieved through the organization of challenges. The fifth challenge consisted of three tasks: semantic indexing, question answering and a new task on information extraction. In total, 29 teams with more than 95 systems participated in the challenge. Overall, as in previous years, the best systems were able to outperform the strong baselines. This suggests that stateof- the art systems are continuously improving, pushing the frontier of research.", "Representations of Time Expressions for Temporal Relation Extraction with Convolutional Neural Networks.\n\nToken sequences are often used as the input for Convolutional Neural Networks (CNNs) in natural language processing. However, they might not be an ideal representation for time expressions, which are long, highly varied, and semantically complex. We describe a method for representing time expressions with single pseudo-tokens for CNNs. With this method, we establish a new state-of-the-art result for a clinical temporal relation extraction task.", "Toward Automated Early Sepsis Alerting: Identifying Infection Patients from Nursing Notes.\n\nSevere sepsis and septic shock are conditions that affect millions of patients and have close to 50% mortality rate. Early identification of at-risk patients significantly improves outcomes. Electronic surveillance tools have been developed to monitor structured Electronic Medical Records and automatically recognize early signs of sepsis. However, many sepsis risk factors (e.g. symptoms and signs of infection) are often captured only in free text clinical notes. In this study, we developed a method for automatic monitoring of nursing notes for signs and symptoms of infection. We utilized a creative approach to automatically generate an annotated dataset. The dataset was used to create a Machine Learning model that achieved an F1-score ranging from 79 to 96%.", "Automatic Extraction of Parallel Speech Corpora from Dubbed Movies.\n\nThis paper presents a methodology to extract parallel speech corpora based on any language pair from dubbed movies, together with an application framework in which some corresponding prosodic parameters are extracted. The obtained parallel corpora are especially suitable for speech-to-speech translation applications when a prosody transfer between source and target languages is desired.", "A parallel collection of clinical trials in Portuguese and English.\n\nParallel collections of documents are crucial resources for training and evaluating machine translation (MT) systems. Even though large collections are available for certain domains and language pairs, these are still scarce in the biomedical domain. We developed a parallel corpus of clinical trials in Portuguese and English. The documents are derived from the Brazilian Clinical Trials Registry and the corpus currently contains a total of 1188 documents. In this paper, we describe the corpus construction and discuss the quality of the translation and the sentence alignment that we obtained.", "Acquisition of Translation Lexicons for Historically Unwritten Languages via Bridging Loanwords.\n\nWith the advent of informal electronic communications such as social media, colloquial languages that were historically unwritten are being written for the first time in heavily code-switched environments. We present a method for inducing portions of translation lexicons through the use of expert knowledge in these settings where there are approximately zero resources available other than a language informant, potentially not even large amounts of monolingual data. We investigate inducing a Moroccan Darija-English translation lexicon via French loanwords bridging into English and find that a useful lexicon is induced for human-assisted translation and statistical machine translation.", "Sentence Alignment using Unfolding Recursive Autoencoders.\n\nIn this paper, we propose a novel two step algorithm for sentence alignment in monolingual corpora using Unfolding Recursive Autoencoders. First, we use unfolding recursive auto-encoders (RAE) to learn feature vectors for phrases in syntactical tree of the sentence. To compare two sentences we use a similarity matrix which has dimensions proportional to the size of the two sentences. Since the similarity matrix generated to compare two sentences has varying dimension due to different sentence lengths, a dynamic pooling layer is used to map it to a matrix of fixed dimension. The resulting matrix is used to calculate the similarity scores between the two sentences. The second step of the algorithm captures the contexts in which the sentences occur in the document by using a dynamic programming algorithm for global alignment.", "Weighted Set-Theoretic Alignment of Comparable Sentences.\n\nThis article presents the STACCw system for the BUCC 2017 shared task on parallel sentence extraction from comparable corpora. The original STACC approach, based on set-theoretic operations over bags of words, had been previously shown to be efficient and portable across domains and alignment scenarios. Wedescribe an extension of this approach with a new weighting scheme and show that it provides significant improvements on the datasets provided for the shared task.", "BUCC 2017 Shared Task: a First Attempt Toward a Deep Learning Framework for Identifying Parallel Sentences in Comparable Corpora.\n\nThis paper describes our participation in BUCC 2017 shared task: identifying parallel sentences in comparable corpora. Our goal is to leverage continuous vector representations and distributional semantics with a minimal use of external preprocessing and postprocessing tools. We report experiments that were conducted after transmitting our results.", "zNLP: Identifying Parallel Sentences in Chinese-English Comparable Corpora.\n\nThis paper describes the zNLP system for the BUCC 2017 shared task. Our system identifies parallel sentence pairs in Chinese-English comparable corpora by translating word-by-word Chinese sentences into English, using the search engine Solr to select near-parallel sentences and then by using an SVM classifier to identify true parallel sentences from the previous results. It obtains an F1-score of 45% (resp. 32%) on the test (training) set.", "BUCC2017: A Hybrid Approach for Identifying Parallel Sentences in Comparable Corpora.\n\nA Statistical Machine Translation (SMT) system is always trained using large parallel corpus to produce effective translation. Not only is the corpus scarce, it also involves a lot of manual labor and cost. Parallel corpus can be prepared by employing comparable corpora where a pair of corpora is in two different languages pointing to the same domain. In the present work, we try to build a parallel corpus for French-English language pair from a given comparable corpus. The data and the problem set are provided as part of the shared task organized by BUCC 2017. We have proposed a system that first translates the sentences by heavily relying on Moses and then group the sentences based on sentence length similarity. Finally, the one to one sentence selection was done based on Cosine Similarity algorithm.", "Users and Data: The Two Neglected Children of Bilingual Natural Language Processing Research.\n\nDespite numerous studies devoted to mining parallel material from bilingual data, we have yet to see the resulting technologies wholeheartedly adopted by professional translators and terminologists alike. I argue that this state of affairs is mainly due to two factors: the emphasis published authors put on models (even though data is as important), and the conspicuous lack of concern for actual end-users.", "Overview of the Second BUCC Shared Task: Spotting Parallel Sentences in Comparable Corpora.\n\nThis paper presents the BUCC 2017 shared task on parallel sentence extraction from comparable corpora.  It recalls the design of the datasets, presents their final construction and statistics and the methods used to evaluate system results. 13 runs were submitted to the shared task by 4 teams, covering three of the four proposed language pairs: French-English (7 runs), German-English (3 runs), and Chinese-English (3 runs). The best F-scores as measured against the gold standard were 0.84 (German-English), 0.80 (French-English), and 0.43 (Chinese-English).  Because of the design of the dataset, in which not all gold parallel sentence pairs are known, these are only minimum values. We examined manually a small sample of the false negative sentence pairs for the most precise French-English runs and estimated the number of parallel sentence pairs not yet in the provided gold standard.  Adding them to the gold standard leads to revised estimates for the French-English F-scores of at most +1.5pt.  This suggests that the BUCC 2017 datasets provide a reasonable approximate evaluation of the parallel sentence spotting task.", "Deep Investigation of Cross-Language Plagiarism Detection Methods.\n\nThis paper is a deep investigation of cross-language plagiarism detection methods on a new recently introduced open dataset, which contains parallel and com- parable collections of documents with multiple characteristics (different genres, languages and sizes of texts). We investigate cross-language plagiarism detection methods for 6 language pairs on 2 granularities of text units in order to draw robust conclusions on the best methods while deeply analyzing correlations across document styles and languages.", "Toward a Comparable Corpus of Latvian, Russian and English Tweets.\n\nTwitter has become a rich source for linguistic data. Here, a possibility of building a trilingual Latvian-Russian-English corpus of tweets from Riga, Latvia is investigated. Such a corpus, once constructed, might be of great use for multiple purposes including  training machine translation models, examining cross-lingual phenomena and studying the population of Riga. This pilot study shows that it is feasible to build such a resource by collecting and analysing a pilot corpus, which is made publicly available and can be used to construct a large comparable corpus.", "Investigating Patient Attitudes Towards the use of Social Media Data to Augment Depression Diagnosis and Treatment: a Qualitative Study.\n\nIn this paper, we use qualitative research methods to investigate the attitudes of social media users towards the (opt-in) integration of social media data with routine mental health care and diagnosis. Our investigation was based on secondary analysis of a series of five focus groups with Twitter users, including three groups consisting of participants with a self-reported history of depression, and two groups consisting of participants without a self reported history of depression. Our results indicate that, overall, research participants were enthusiastic about the possibility of using social media (in conjunction with automated Natural Language Processing algorithms) for mood tracking under the supervision of a mental health practitioner. However, for at least some participants, there was skepticism related to how well social media represents the mental health of users, and hence its usefulness in the clinical context.", "Detecting Anxiety through Reddit.\n\nPrevious investigations into detecting mental illnesses through social media have predominately focused on detecting depression through Twitter corpora. In this paper, we study anxiety disorders through personal narratives collected through the popular social media website, Reddit. We build a substantial data set of typical and anxiety-related posts, and we apply N-gram language modeling, vector embeddings, topic analysis, and emotional norms to generate features that accurately classify posts related to binary levels of anxiety. We achieve an accuracy of 91% with vector-space word embeddings, and an accuracy of 98% when combined with lexicon-based features.", "A Cross-modal Review of Indicators for Depression Detection Systems.\n\nAutomatic detection of depression has attracted increasing attention from researchers in psychology, computer science, linguistics, and related disciplines. As a result, promising depression detection systems have been reported. This paper surveys these efforts by presenting the first cross-modal review of depression detection systems and discusses best practices and most promising approaches to this task.", "Natural-language Interactive Narratives in Imaginal Exposure Therapy for Obsessive-Compulsive Disorder.\n\nObsessive-compulsive disorder (OCD) is an anxiety-based disorder that affects around 2.5% of the population. A common treatment for OCD is exposure therapy, where the patient repeatedly confronts a feared experience, which has the long-term effect of decreasing their anxiety. Some exposures consist of reading and writing stories about an imagined anxiety-provoking scenario. In this paper, we present a technology that enables patients to interactively contribute to exposure stories by supplying natural language input (typed or spoken) that advances a scenario. This interactivity could potentially increase the patient's sense of immersion in an exposure and contribute to its success. We introduce the NLP task behind processing inputs to predict new events in the scenario, and describe our initial approach. We then illustrate the future possibility of this work with an example of an exposure scenario authored with our application.", "Monitoring Tweets for Depression to Detect At-risk Users.\n\nWe propose an automated system that can identify at-risk users from their public social media activity, more specifically, from Twitter. The data that we collected is from the \\#BellLetsTalk campaign, which is a wide-reaching, multi-year program designed to break the silence around mental illness and support mental health across Canada. To achieve our goal, we trained a user-level classifier that can detect at-risk users that achieves a reasonable precision and recall. We also trained a tweet-level classifier that predicts if a tweet indicates depression. This task was much more difficult due to the imbalanced data. In the dataset that we labeled, we came across 5% depression tweets and 95% non-depression tweets. To handle this class imbalance, we used undersampling methods. The resulting classifier had high recall, but low precision. Therefore, we only use this classifier to compute the estimated percentage of depressed tweets and to add this value as a feature for the user-level classifier.", "A Corpus Analysis of Social Connections and Social Isolation in Adolescents Suffering from Depressive Disorders.\n\nSocial connection and social isolation are associated with depressive symptoms, particularly in adolescents and young adults, but how these concepts are documented in clinical notes is unknown. This pilot study aimed to identify the topics relevant to social connection and isolation by analyzing 145 clinical notes from patients with depression diagnosis. We found that providers, including physicians, nurses, social workers, and psychologists, document descriptions of both social connection and social isolation.", "Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language.\n\nMany psychological phenomena occur in small time windows, measured in minutes or hours. However, most computational linguistic techniques look at data on the order of weeks, months, or years. We explore micropatterns in sequences of messages occurring over a short time window for their prevalence and power for quantifying psychological phenomena, specifically, patterns in affect. We examine affective micropatterns in social media posts from users with anxiety, eating disorders, panic attacks, schizophrenia, suicidality, and matched controls.", "In your wildest dreams: the language and psychological features of dreams.\n\nIn this paper, we provide the first quantified exploration of the structure of the language of dreams, their linguistic style and emotional content. We present a collection of digital dream logs as a viable corpus for the growing study of mental health through the lens of language, complementary to the work done examining more traditional social media. This paper is largely exploratory in nature to lay the groundwork for subsequent research in mental health, rather than optimizing a particular text classification task.", "Detecting and Explaining Crisis.\n\nIndividuals on social media may reveal themselves to be in various states of crisis (e.g. suicide, self-harm, abuse, or eating disorders). Detecting crisis from social media text automatically and accurately can have profound consequences. However, detecting a general state of crisis without explaining why has limited applications. An explanation in this context is a coherent, concise subset of the text that rationalizes the crisis detection. We explore several methods to detect and explain crisis using a combination of neural and non-neural techniques. We evaluate these techniques on a unique data set obtained from Koko, an anonymous emotional support network available through various messaging applications. We annotate a small subset of the samples labeled with crisis with corresponding explanations. Our best technique significantly outperforms the baseline for detection and explanation.", "A Dictionary-Based Comparison of Autobiographies by People and Murderous Monsters.\n\nPeople typically assume that killers are mentally ill or fundamentally different from the rest of humanity. Similarly, people often associate mental health conditions (such as schizophrenia or autism) with violence and otherness - treatable perhaps, but not empathically understandable. We take a dictionary approach to explore word use in a set of autobiographies, comparing the narratives of 2 killers (Adolf Hitler and Elliot Rodger) and 39 non-killers. Although results suggest several dimensions that differentiate these autobiographies - such as sentiment, temporal orientation, and references to death - they appear to reflect subject matter rather than psychology per se. Additionally, the Rodger text shows roughly typical developmental arcs in its use of words relating to friends, family, sex, and affect. From these data, we discuss the challenges of understanding killers and people in general.", "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling.\n\nWe introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves competitive performance on English, even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset. We also consider Chinese, Czech and Spanish where our approach also achieves competitive results. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e., syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on standard out-of-domain test sets.", "An Artificial Language Evaluation of Distributional Semantic Models.\n\nRecent studies of distributional semantic models have set up a competition between word embeddings obtained from predictive neural networks and word vectors obtained from abstractive count-based models. This paper is an attempt to reveal the underlying contribution of additional training data and post-processing steps on each type of model in word similarity and relatedness inference tasks. We do so by designing an artificial language framework, training a predictive and a count-based model on data sampled from this grammar, and evaluating the resulting word vectors in paradigmatic and syntagmatic tasks defined with respect to the grammar.", "Cross-language Learning with Adversarial Neural Networks.\n\nWe address the problem of cross-language adaptation for question-question similarity reranking in community question answering, with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language. In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages. The evaluation results show sizable improvements for our cross-language adversarial neural network (CLANN) model over a strong non-adversarial system.", "Neural Domain Adaptation for Biomedical Question Answering.\n\nFactoid question answering (QA) has recently benefited from the development of deep learning (DL) systems. Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD (ca. 100,000 questions) for Wikipedia articles. However, these systems have not yet been applied to QA in more specific domains, such as biomedicine, because datasets are generally too small to train a DL system from scratch. For example, the BioASQ dataset for biomedical QA comprises less then 900 factoid (single answer) and list (multiple answers) QA instances. In this work, we adapt a neural QA system trained on a large open-domain dataset (SQuAD, source) to a biomedical dataset (BioASQ, target) by employing various transfer learning techniques. Our network architecture is based on a state-of-the-art QA system, extended with biomedical word embeddings and a novel mechanism to answer list questions. In contrast to existing biomedical QA systems, our system does not rely on domain-specific ontologies, parsers or entity taggers, which are expensive to create. Despite this fact, our systems achieve state-of-the-art results on factoid questions and competitive results on list questions.", "Modeling Context Words as Regions: An Ordinal Regression Approach to Word Embedding.\n\nVector representations of word meaning have found many applications in the field of natural language processing. Word vectors intuitively represent the average context in which a given word tends to occur, but they cannot explicitly model the diversity of these contexts. Although region representations of word meaning offer a natural alternative to word vectors, only few methods have been proposed that can effectively learn word regions. In this paper, we propose a new word embedding model which is based on SVM regression. We show that the underlying ranking interpretation of word contexts is sufficient to match, and sometimes outperform, the performance of popular methods such as Skip-gram. Furthermore, we show that by using a quadratic kernel, we can effectively learn word regions, which outperform existing unsupervised models for the task of hypernym detection.", "Learning Word Representations with Regularization from Prior Knowledge.\n\nConventional word embeddings are trained with specific criteria (e.g., based on language modeling or co-occurrence) inside a single information source, disregarding the opportunity for further calibration using external knowledge. This paper presents a unified framework that leverages pre-learned or external priors, in the form of a regularizer, for enhancing conventional language model-based embedding learning. We consider two types of regularizers. The first type is derived from topic distribution by running LDA on unlabeled data. The second type is based on dictionaries that are created with human annotation efforts. To effectively learn with the regularizers, we propose a novel data structure, trajectory softmax, in this paper. The resulting embeddings are evaluated by word similarity and sentiment classification. Experimental results show that our learning framework with regularization from prior knowledge improves embedding quality across multiple datasets, compared to a diverse collection of baseline methods.", "German in Flux: Detecting Metaphoric Change via Word Entropy.\n\nThis paper explores the information-theoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on language change. We build the first diachronic test set for German as a standard for metaphoric change annotation. Our model is unsupervised, language-independent and generalizable to other processes of semantic change.", "A Joint Model for Semantic Sequences: Frames, Entities, Sentiments.\n\nUnderstanding stories -- sequences of events -- is a crucial yet challenging natural language understanding task. These events typically carry multiple aspects of semantics including actions, entities and emotions. Not only does each individual aspect contribute to the meaning of the story, so does the interaction among these aspects. Building on this intuition, we propose to jointly model important aspects of semantic knowledge -- frames, entities and sentiments -- via a semantic language model. We achieve this by first representing these aspects' semantic units at an appropriate level of abstraction and then using the resulting vector representations for each semantic aspect to learn a joint representation via a neural language model. We show that the joint semantic language model is of high quality and can generate better semantic sequences than models that operate on the word level. We further demonstrate that our joint model can be applied to story cloze test and shallow discourse parsing tasks with improved performance and that each semantic aspect contributes to the model.", "A Supervised Approach to Extractive Summarisation of Scientific Papers.\n\nAutomatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.", "Collaborative Partitioning for Coreference Resolution.\n\nThis paper presents a collaborative partitioning algorithm---a novel ensemble-based approach to coreference resolution. Starting from the all-singleton partition, we search for a solution close to the ensemble's outputs in terms of a task-specific similarity measure. Our approach assumes a loose integration of individual components of the ensemble and can therefore combine arbitrary coreference resolvers, regardless of their models. Our experiments on the CoNLL dataset show that collaborative partitioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the best coreference performance reported so far in the literature (MELA v08 score of 64.47).", "Learning from Relatives: Unified Dialectal Arabic Segmentation.\n\nArabic dialects do not just share a common koin\u00e9, but there are shared pan-dialectal linguistic phenomena that allow computational models for dialects to learn from each other. In this paper we build a unified segmentation model where the training data for different dialects are combined and a single model is trained. The model yields higher accuracies than dialect-specific models, eliminating the need for dialect identification before segmentation. We also measure the degree of relatedness between four major Arabic dialects by testing how a segmentation model trained on one dialect performs on the other dialects. We found that linguistic relatedness is contingent with geographical proximity. In our experiments we use SVM-based ranking and bi-LSTM-CRF sequence labeling.", "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics.\n\nCoreference evaluation metrics are hard to optimize directly as they are non-differentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.", "Knowledge Tracing in Sequential Learning of Inflected Vocabulary.\n\nWe present a feature-rich knowledge tracing method that captures a student's acquisition and retention of knowledge during a foreign language phrase learning task. We model the student's behavior as making predictions under a log-linear model, and adopt a neural gating mechanism to model how the student updates their log-linear parameters in response to feedback.  The gating mechanism allows the model to learn complex patterns of retention and acquisition for each feature, while the log-linear parameterization results in an interpretable knowledge state. We collect human data and evaluate several versions of the model.", "Neural Structural Correspondence Learning for Domain Adaptation.\n\nWe introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs). Our model is a three-layer NN that learns to encode the non-pivot features of an input example into a low dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. We experiment with the task of cross-domain sentiment classification on 16 domain pairs and show substantial improvements over strong baselines.", "Leveraging Eventive Information for Better Metaphor Detection and Classification.\n\nMetaphor detection has been both challenging and rewarding in natural language processing applications. This study offers a new approach based on eventive information in detecting metaphors by leveraging the Chinese writing system, which is a culturally bound ontological system organized according to the basic concepts represented by radicals. As such, the information represented is available in all Chinese text without pre-processing. Since metaphor detection is another culturally based conceptual representation, we hypothesize that sub-textual information can facilitate the identification and classification of the types of metaphoric events denoted in Chinese text. We propose a set of syntactic conditions crucial to event structures to improve the model based on the classification of radical groups. With the proposed syntactic conditions, the model achieves a performance of 0.8859 in terms of F-scores, making 1.7% of improvement than the same classifier with only Bag-of-word features. Results show that eventive information can improve the effectiveness of metaphor detection. Event information is rooted in every language, and thus this approach has a high potential to be applied to metaphor detection in other languages.", "Learning Contextual Embeddings for Structural Semantic Similarity using Categorical Information.\n\nTree kernels (TKs) and neural networks are two effective approaches for automatic feature engineering. In this paper, we combine them by modeling context word similarity in semantic TKs. This way, the latter can operate subtree matching by applying neural-based similarity on tree lexical nodes. We study how to learn representations for the words in context such that TKs can exploit more focused information. We found that neural embeddings produced by current methods do not provide a suitable contextual similarity. Thus, we define a new approach based on a Siamese Network, which produces word representations while learning a binary text similarity. We set the latter considering examples in the same category as similar. The experiments on question and sentiment classification show that our semantic TK highly improves previous results.", "Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring.\n\nNeural network models have recently been applied to the task of automatic essay scoring, giving promising results. Existing work used recurrent neural networks and convolutional neural networks to model input essays, giving grades based on a single vector representation of the essay. On the other hand, the relative advantages of RNNs and CNNs have not been compared. In addition, different parts of the essay can contribute differently for scoring, which is not captured by existing models. We ad- dress these issues by building a hierarchical sentence-document model to represent essays, using the attention mechanism to automatically decide the relative weights of words and sentences. Results show that our model outperforms the previous state- of-the-art methods, demonstrating the effectiveness of the attention mechanism.", "A Probabilistic Generative Grammar for Semantic Parsing.\n\nWe present a generative model of natural language sentences and demonstrate its application to semantic parsing. In the generative process, a logical form sampled from a prior, and conditioned on this logical form, a grammar probabilistically generates the output sentence. Grammar induction using MCMC is applied to learn the grammar given a set of labeled sentences with corresponding logical forms. We develop a semantic parser that finds the logical form with the highest posterior probability exactly. We obtain strong results on the GeoQuery dataset and achieve state-of-the-art F1 on Jobs.", "Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation.\n\nPairwise ranking methods are the most widely used discriminative training approaches for structure prediction problems in natural language processing (NLP). Decomposing the problem of ranking hypotheses into pairwise comparisons enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder learning. We propose a listwise learning framework for structure prediction problems such as machine translation. Our framework directly models the entire translation list's ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.", "Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts.\n\nThis paper presents a novel approach to character identification, that is an entity linking task that maps mentions to characters in dialogues from TV show transcripts. We first augment and correct several cases of annotation errors in an existing corpus so the corpus is clearer and cleaner for statistical learning. We also introduce the agglomerative convolutional neural network that takes groups of features and learns mention and mention-pair embeddings for coreference resolution. We then propose another neural model that employs the embeddings learned and creates cluster embeddings for entity linking. Our coreference resolution model shows comparable results to other state-of-the-art systems. Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.", "Learning Stock Market Sentiment Lexicon and Sentiment-Oriented Word Vector from StockTwits.\n\nPrevious studies have shown that investor sentiment indicators can predict stock market change.  A domain-specific sentiment lexicon and sentiment-oriented word embedding model would help the sentiment analysis in financial domain and stock market. In this paper, we present a new approach to learning stock market lexicon from StockTwits, a popular financial social network for investors to share ideas.  It learns word polarity by predicting message sentiment, using a neural net-work.  The sentiment-oriented word embeddings are learned from tens of millions of StockTwits posts, and this is the first study presenting sentiment-oriented word embeddings for stock market. The experiments of predicting investor sentiment show that our lexicon outperformed other lexicons built by the state-of-the-art methods, and the sentiment-oriented word vector was much better than the general word embeddings.", "Multilingual Semantic Parsing And Code-Switching.\n\nExtending semantic parsing systems to new domains and languages is a highly expensive, time-consuming process, so making effective use of existing resources is critical. In this paper, we describe a transfer learning method using crosslingual word embeddings in a sequence-to-sequence model.  On the NLmaps corpus, our approach achieves state-of-the-art accuracy of 85.7% for English.  Most importantly, we observed a consistent improvement for German compared with several baseline domain adaptation techniques.  As a by-product of this approach, our models that are trained on a combination of English and German utterances perform reasonably well on code-switching utterances which contain a mixture of English and German, even though the training data does not contain any such. As far as we know, this is the first study of code-switching in semantic parsing. We manually constructed the set of code-switching test utterances for the NLmaps corpus and achieve 78.3% accuracy on this dataset.", "Idea density for predicting Alzheimer's disease from transcribed speech.\n\nIdea Density (ID) measures the rate at which ideas or elementary predications are expressed in an utterance or in a text. Lower ID is found to be associated with an increased risk of developing Alzheimer's disease (AD) (Snowdon et al., 1996; Engelman et al., 2010). ID has been used in two different versions: propositional idea density (PID) counts the expressed ideas and can be applied to any text while semantic idea density (SID) counts pre-defined information content units and is naturally more applicable to normative domains, such as picture description tasks. In this paper, we develop DEPID, a novel dependency-based method for computing PID, and its version DEPID-R that enables to exclude repeating ideas---a feature characteristic to AD speech.  We conduct the first comparison of automatically extracted PID and SID in the diagnostic classification task on two different AD datasets covering both closed-topic and free-recall domains. While SID performs better on the normative dataset, adding PID leads to a small but significant improvement (+1.7 F-score). On the free-topic dataset, PID performs better than SID as expected (77.6 vs 72.3 in F-score) but adding the features derived from the word embedding clustering underlying the automatic SID increases the results considerably, leading to an F-score of 84.8.", "An Automatic Approach for Document-level Topic Model Evaluation.\n\nTopic models jointly learn topics and document-level topic distribution.  Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading.  We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.", "Joint Prediction of Morphosyntactic Categories for Fine-Grained Arabic Part-of-Speech Tagging Exploiting Tag Dictionary Information.\n\nPart-of-speech (POS) tagging for morphologically rich languages such as Arabic is a challenging problem because of their enormous tag sets. One reason for this is that in the tagging scheme for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. Previous approaches in Arabic POS tagging applied one model for each morphosyntactic tagging task, without utilizing shared information between the tasks. In this paper, we propose an approach that utilizes this information by jointly modeling multiple morphosyntactic tagging tasks with a multi-task learning framework. We also propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model with tag dictionary information results in an accuracy of 91.38% on the Penn Arabic Treebank data set, with an absolute improvement of 2.11% over the current state-of-the-art tagger.", "A phoneme clustering algorithm based on the obligatory contour principle.\n\nThis paper explores a divisive hierarchical clustering algorithm based on the well-known Obligatory Contour Principle in phonology.  The purpose is twofold: to see if such an algorithm could be used for unsupervised classification of phonemes or graphemes in corpora, and to investigate whether this purported universal constraint really holds for several classes of phonological distinctive features.  The algorithm achieves very high accuracies in an unsupervised setting of inferring a consonant-vowel distinction, and also has a strong tendency to detect coronal phonemes in an unsupervised fashion. Remaining classes, however, do not correspond as neatly to phonological distinctive feature splits.  While the results offer only mixed support for a universal Obligatory Contour Principle, the algorithm can be very useful for many NLP tasks due to the high accuracy in revealing consonant/vowel/coronal distinctions.", "Neural Sequence-to-sequence Learning of Internal Word Structure.\n\nLearning internal word structure has recently been recognized as an important step in various multilingual processing tasks and in theoretical language comparison. In this paper, we present a neural encoder-decoder model for learning canonical morphological segmentation. Our model combines character-level sequence-to-sequence transformation with a language model over canonical segments. We obtain up to 4% improvement over a strong character-level encoder-decoder baseline for three languages. Our model outperforms the previous state-of-the-art for two languages, while eliminating the need for external resources such as large dictionaries. Finally, by comparing the performance of encoder-decoder and classical statistical machine translation systems trained with and without corpus counts, we show that including corpus counts is beneficial to both approaches.", "Should Neural Network Architecture Reflect Linguistic Structure?.\n\nI explore the hypothesis that conventional neural network models (e.g., recurrent neural networks) are incorrectly biased for making linguistically sensible generalizations when learning, and that a better class of models is based on architectures that reflect hierarchical structures for which considerable behavioral evidence exists. I focus on the problem of modeling and representing the meanings of sentences. On the generation front, I introduce recurrent neural network grammars (RNNGs), a joint, generative model of phrase-structure trees and sentences. RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire (top-down, left-to-right) syntactic derivation history, thus relaxing context-free independence assumptions, while retaining a bias toward explaining decisions via ``syntactically local'' conditioning contexts. Experiments show that RNNGs obtain better results in generating language than models that don't exploit linguistic structure. On the representation front, I explore unsupervised learning of syntactic structures based on distant semantic supervision using a reinforcement-learning algorithm. The learner seeks a syntactic structure that provides a compositional architecture that produces a good representation for a downstream semantic task. Although the inferred structures are quite different from traditional syntactic analyses, the performance on the downstream tasks surpasses that of systems that use sequential RNNs and tree-structured RNNs based on treebank dependencies. This is joint work with Adhi Kuncoro, Dani Yogatama, Miguel Ballesteros, Phil Blunsom, Ed Grefenstette, Wang Ling, and Noah A. Smith.", "Rational Distortions of Learners' Linguistic Input.\n\nLanguage acquisition can be modeled as a statistical inference problem: children use sentences and sounds in their input to infer linguistic structure. However, in many cases, children learn from data whose statistical structure is distorted relative to the language they are learning.  Such distortions can arise either in the input itself, or as a result of children's immature strategies for encoding their input.  This work examines several cases in which the statistical structure of children's input differs from the language being learned.  Analyses show that these distortions of the input can be accounted for with a statistical learning framework by carefully considering the inference problems that learners solve during language acquisition", "Parsing for Grammatical Relations via Graph Merging.\n\nThis paper is concerned with building deep grammatical relation (GR) analysis using data-driven approach. To deal with this problem, we propose graph merging, a new perspective, for building flexible dependency graphs: Constructing complex graphs via constructing simple subgraphs. We discuss two key problems in this perspective: (1) how to decompose a complex graph into simple subgraphs, and (2) how to combine subgraphs into a coherent complex graph. Experiments demonstrate the effectiveness of graph merging. Our parser reaches state-of-the-art performance and is significantly better than two transition-based parsers.", "Exploring the Syntactic Abilities of RNNs with Multi-task Learning.\n\nRecent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syn- tactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models.", "Embedding Words and Senses Together via Joint Knowledge-Enhanced Training.\n\nWord embeddings are widely used in Natural Language Processing, mainly due to their success in capturing semantic information from massive corpora. However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector. We address this issue by proposing a new model which learns word and sense embeddings jointly. Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to state-of-the-art word- and sense-based models.", "Zero-Shot Relation Extraction via Reading Comprehension.\n\nWe show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "The Covert Helps Parse the Overt.\n\nThis paper is concerned with whether deep syntactic information can help surface parsing, with a particular focus on empty categories. We design new algorithms to produce dependency trees in which empty elements are allowed, and evaluate the impact of information about empty category on parsing overt elements. Such information is helpful to reduce the approximation error in a structured parsing model, but increases the search space for inference and accordingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing.", "Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks.\n\nNatural language generation (NLG) is a critical component in a spoken dialogue system. This paper presents a Recurrent Neural Network based Encoder-Decoder architecture, in which an LSTM-based decoder is introduced to select, aggregate semantic elements produced by an attention mechanism over the input elements, and to produce the required utterances. The proposed generator can be jointly trained both sentence planning and surface realization to produce natural language sentences. The proposed model was extensively evaluated on four different NLG datasets. The experimental results showed that the proposed generators not only consistently outperform the previous methods across all the NLG domains but also show an ability to generalize from a new, unseen domain and learn from multi-domain datasets.", "Named Entity Disambiguation for Noisy Text.\n\nWe address the task of Named Entity Disambiguation (NED) for noisy text. We present WikilinksNED, a large-scale NED dataset of text fragments from the web, which is significantly noisier and more challenging than existing news-based datasets. To capture the limited and noisy local context surrounding each mention, we design a neural model and train it with a novel method for sampling informative negative examples. We also describe a new way of initializing word and entity embeddings that significantly improves performance. Our model significantly outperforms existing state-of-the-art methods on WikilinksNED while achieving comparable performance on a smaller newswire dataset.", "Encoding of phonology in a recurrent neural model of grounded speech.\n\nWe study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.", "Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification.\n\nFor many applications of question answering (QA), being able to explain why a given model chose an answer is critical.  However, the lack of labeled data for answer justifications makes learning this difficult and expensive.  Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications, where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either. We propose a neural network architecture for QA that reranks answer justifications as an intermediate (and human-interpretable) step in answer selection. Our approach is informed by a set of features designed to combine both learned representations and explicit features to capture the connection between questions, answers, and answer justifications. We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9% rated highly relevant) and answer selection (+6% P@1).", "Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text.\n\nThe task of relation classification in the biomedical domain is complex due to the presence of samples obtained from heterogeneous sources such as research articles, discharge summaries, or electronic health records. It is also a constraint for classifiers which employ manual feature engineering. In this paper, we propose a convolutional recurrent neural network (CRNN) architecture that combines RNNs and CNNs in sequence to solve this problem. The rationale behind our approach is that CNNs can effectively identify coarse-grained local features in a sentence, while RNNs are more suited for long-term dependencies. We compare our CRNN model with several baselines on two biomedical datasets, namely the i2b2-2010 clinical relation extraction challenge dataset, and the SemEval-2013 DDI extraction dataset. We also evaluate an attentive pooling technique and report its performance in comparison with the conventional max pooling method. Our results indicate that the proposed model achieves state-of-the-art performance on both datasets.", "Feature Selection as Causal Inference: Experiments with Text Classification.\n\nThis paper proposes a matching technique for learning causal associations between word features and class labels in document classification. The goal is to identify more meaningful and generalizable features than with only correlational approaches. Experiments with sentiment classification show that the proposed method identifies interpretable word associations with sentiment and improves classification performance in a majority of cases. The proposed feature selection method is particularly effective when applied to out-of-domain data.", "Automatic Selection of Context Configurations for Improved Class-Specific Word Representations.\n\nThis paper is concerned with identifying contexts useful for training word representation models for different word classes such as adjectives (A), verbs (V), and nouns (N). We introduce a simple yet effective framework for an automatic selection of class-specific context configurations. We construct a context configuration space based on universal dependency relations between words, and efficiently search this space with an adapted beam search algorithm. In word similarity tasks for each word class, we show that our framework is both effective and efficient. Particularly, it improves the Spearman's rho correlation with human scores on SimLex-999 over the best previously proposed class-specific contexts by 6 (A), 6 (V) and 5 (N) rho points. With our selected context configurations, we train on only 14% (A), 26.2% (V), and 33.6% (N) of all dependency-based contexts, resulting in a reduced training time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages..", "The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task.\n\nA writer's style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.", "Graph-based Neural Multi-Document Summarization.\n\nWe propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.", "Learning What is Essential in Questions.\n\nQuestion answering (QA) systems are easily  distracted by  irrelevant or redundant words in questions, especially when faced with  long or multi-sentence                          questions  in difficult  domains. This paper introduces and studies  the  notion  of essential  question  terms with  the goal  of improving such QA  solvers. We                          illustrate the importance            of essential question  terms  by showing  that  humans'  ability  to  answer questions drops significantly when essential  terms  are eliminated  from  questions.We then develop a classifier that reliably (90%  mean  average  precision) identifies and ranks essential terms in questions. Finally, we use the classifier to demonstrate that                          the  notion  of  question term essentiality allows state-of-the-art  QA  solver for  elementary-level  science  questions to make better and more informed decisions,improving performance by up to 5%.We also  introduce  a  new  dataset  of  over 2,200 crowd-sourced essential terms annotated science questions.", "Making Neural QA as Simple as Possible but not Simpler.\n\nRecent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.", "Swanson linking revisited: Accelerating literature-based discovery across domains using a conceptual influence graph.\n\nWe introduce a modular approach for literature-based discovery consisting of a machine reading and knowledge assembly component that together produce a graph of influence relations (e.g., ``A promotes B'') from a collection of publications.  A search engine is used to explore direct and indirect influence chains. Query results are substantiated with textual evidence, ranked according to their relevance, and presented in both a table-based view, as well as a network graph visualization.  Our approach operates in both domain-specific settings, where there are knowledge bases and ontologies available to guide reading, and in multi-domain settings where such resources are absent. We demonstrate that this deep reading and search system reduces the effort needed to uncover ``undiscovered public knowledge'', and that with the aid of this tool a domain expert was able to drastically reduce her model building time from months to two days.", "Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ.\n\nScattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents.  Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them.   Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.", "Benben: A Chinese Intelligent Conversational Robot.\n\nRecently, conversational robot is widely used in mobile terminals as the virtual assistant or companion. The goals of prevalent conversational robots mainly focus on four categories, namely chit-chat, task completion, question answering and recommendation. In this paper, we present a Chinese intelligent conversational robot, Benben, which is designed to achieve these goals in a unified architecture. Moreover, it also has some featured functions such as diet map, implicit feedback based conversation, interactive machine reading, news recommendation, etc. Since the release of Benben at June 6, 2016, there are 2,505 users (till Feb 22, 2017) and 11,107 complete human-robot conversations, which totally contain 198,998 single turn conversation pairs.", "Zara Returns: Improved Personality Induction and Adaptation by an Empathetic Virtual Agent.\n\nVirtual agents need to adapt their personality to the user in order to become more empathetic. To this end, we developed Zara the Supergirl, an interactive empathetic agent, using a modular approach. In this paper, we describe the enhanced personality module with improved recognition from speech and text using deep learning frameworks. From raw audio, an average F-score of 69.6 was obtained from real-time personality assessment using a Convolutional Neural Network (CNN) model. From text, we improved personality recognition results with a CNN model on top of pre-trained word embeddings and obtained an average F-score of 71.0. Results from our Human-Agent Interaction study confirmed our assumption that people have different agent personality preferences. We use insights from this study to adapt our agent to user personality.", "Hafez: an Interactive Poetry Generation System.\n\nHafez is an automatic poetry generation system that integrates a Recurrent Neural Network (RNN) with a Finite State Acceptor (FSA). It generates sonnets given arbitrary topics. Furthermore, Hafez enables users to revise and polish generated poems by adjusting various style configurations. Experiments demonstrate that such ``polish'' mechanisms consider the user's intention and lead to a better poem. For evaluation, we build a web interface where users can rate the quality of each poem from 1 to 5 stars. We also speed up the whole system by a factor of 10, via vocabulary pruning and GPU computation, so that adequate feedback can be collected at a fast pace. Based on such feedback, the system learns to adjust its parameters to improve poetry quality. .", "SuperAgent: A Customer Service Chatbot for E-commerce Websites.\n\nConventional customer service chatbots are usually built upon human dialogs, yet confronted with the problems of data scale and privacy. In this paper, we present SuperAgent, which is a customer service chatbot leveraging large-scale and public available e-commerce data. Distinct from existing counterparts, SuperAgent takes advantage of data from in-page product descriptions as well as user-generated content from e-commerce websites, which is more practical and cost-effective to answer repetitive questions, thereby freeing up support staffs to answer much higher value questions. We demonstrate SuperAgent as an add-on extension to mainstream web browsers and show its usefulness to user's online shopping experience.", "End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights.\n\nAdvanced attention mechanisms are an important part of successful neural network approaches for non-factoid answer selection because they allow the models to focus on few important segments within rather long answer texts. Analyzing attention mechanisms is thus crucial for understanding strengths and weaknesses of particular models. We present an extensible, highly modular service architecture that enables to transform neural network models for non-factoid answer selection into fully featured end-to-end question answering systems. The primary objective of our system is to enable researchers a way to interactively explore and compare attention-based neural networks for answer selection. Our interactive user interface helps researchers to better understand the capabilities of the different approaches and can aid qualitative analyses. The source-code of our system is publicly available.", "Extended Named Entity Recognition API and Its Applications in Language Education.\n\nWe present an Extended Named Entity Recognition API to recognize various types of entities and classify the entities into 200 different categories. Each entity is classified into a hierarchy of entity categories, in which the categories near the root are more general than the categories near the leaves of the hierarchy. This category information can be used in various applications such as language educational applications, online news services and recommendation engines. We show an application of the proposed API in a Japanese online news service for Japanese language studying users.", "Interactive Visual Analysis of Transcribed Multi-Party Discourse.\n\nWe present a first web-based Visual Analytics framework for the analysis of multiparty discourse data using verbatim text transcripts. Our framework supports a broad range of server-based processing steps, ranging from data mining and statistical analysis to deep linguistic parsing of English and German. On the client-side, browser-based Visual Analytics components enable multiple perspectives on the analyzed data. These interactive visualizations allow exploratory content analysis, argumentation pattern review, and speaker interaction modeling.", "PyDial: A Multi-domain Statistical Dialogue System Toolkit.\n\nStatistical Spoken Dialogue Systems have been around for many years. However, access to these systems has always been difficult as there is still no publicly available end-to-end system implementation. To alleviate this, we present CU-PyDial, an open-source end-to-end statistical spoken dialogue system toolkit which provides implementations of statistical approaches for all dialogue system modules. Moreover, it has been extended to provide multi-domain conversational functionality. It offers easy configuration, easy extensibility, and domain-independent implementations of the respective dialogue system modules. The toolkit is available for download under the Apache 2.0 license.", "UCCAApp: Web-application for Syntactic and Semantic Phrase-based Annotation.\n\nWe present UCCAApp, an open-source, flexible web-application for syntactic and semantic phrase-based annotation in general, and for UCCA annotation in particular. UCCAApp supports a variety of formal properties that have proven useful for syntactic and semantic representation, such as discontiguous phrases, multiple parents and empty elements, making it useful to a variety of other annotation schemes with similar formal properties. UCCAApp's user interface is intuitive and user friendly, so as to support annotation by users with no background in linguistics or formal representation. Indeed, a pilot version of the application has been successfully used in the compilation of the UCCA Wikipedia treebank by annotators with no previous linguistic training. The application and all accompanying resources are released as open source under the GNU public license, and are available online along with a live demo.", "Semedico: A Comprehensive Semantic Search Engine for the Life Sciences.\n\nSemedico is a semantic search engine designed to support literature search in the life sciences by integrating the semantics of the domain at all stages of the search process - from query formulation via query processing up to the presentation of results. Semedico excels with an ad-hoc search approach which directly reflects relevance in terms of information density of entities and relations among them (events) and, a truly unique feature, ranks interaction events by certainty information reflecting the degree of factuality of the encountered event.", "Olelo: A Question Answering Application for Biomedicine.\n\nDespite the importance of the biomedical domain, there are few reliable applications to support researchers and physicians for retrieving particular facts that fit their needs. Users typically rely on search engines that only support keyword- and filter-based searches. We present Olelo, a question answering system for biomedicine. Olelo is built on top of an in-memory database, integrates domain resources, such as document collections and terminologies, and uses various natural language processing components. Olelo is fast, intuitive and easy to use. We evaluated the systems on two use cases: answering questions related to a particular gene and on the BioASQ benchmark. Olelo is available at: https://hpi.de/plattner/olelo.", "Exploring Diachronic Lexical Semantics with JeSemE.\n\nRecent advances in distributional semantics combined with the availability of large-scale diachronic corpora offer new research avenues for the Digital Humanities. JeSemE, the Jena Semantic Explorer, renders assistance to a non-technical audience to investigate diachronic semantic topics. JeSemE runs as a website with query options and interactive visualizations of results, as well as a REST API for access to the underlying diachronic data sets.", "Life-iNet: A Structured Network-Based Knowledge Exploration and Analytics System for Life Sciences.\n\nSearch engines running on scientific literature have been widely used by life scientists to find publications related to their research. However, existing search engines in life-science domain, such as PubMed, have limitations when applied to exploring and analyzing factual knowledge (e.g., disease-gene associations) in massive text corpora. These limitations are mainly due to the problems that factual information exists as an unstructured form in text, and also keyword and MeSH term-based queries cannot effectively imply semantic relations between entities. This demo paper presents the LifeNet system to address the limitations in existing search engines on facilitating life sciences research. LifeNet automatically constructs structured networks of factual knowledge from large amounts of background documents, to support efficient exploration of structured factual knowledge in the unstructured literature. It also provides functionalities for finding distinctive entities for given entity types, and generating hypothetical facts to assist literature-based knowledge discovery (e.g., drug target prediction).", "Annotating tense, mood and voice for English, French and German.\n\nWe present the first open-source tool for annotating morphosyntactic tense, mood and voice for English, French and German verbal complexes. The annotation is based on a set of language-specific rules, which are applied on dependency trees and leverage information about lemmas, morphological properties and POS-tags of the verbs. Our tool has an average accuracy of about 76%. The tense, mood and voice features are useful both as features in computational modeling and for corpus-linguistic research.", "Automating Biomedical Evidence Synthesis: RobotReviewer.\n\nWe present RobotReviewer, an open-source web-based system that uses machine learning and NLP to semi-automate biomedical evidence synthesis, to aid the practice of Evidence-Based Medicine. RobotReviewer processes full-text journal articles (PDFs) describing randomized controlled trials (RCTs). It appraises the reliability of RCTs and extracts text describing key trial characteristics (e.g., descriptions of the population) using novel NLP methods. RobotReviewer then automatically generates a report synthesising this information. Our goal is for RobotReviewer to automatically extract and synthesise the full-range of structured data needed to inform evidence-based practice.", "ESTEEM: A Novel Framework for Qualitatively Evaluating and Visualizing Spatiotemporal Embeddings in Social Media.\n\nAnalyzing and visualizing large amounts of social media communications and contrasting short-term conversation changes over time and geolocations is extremely important for commercial and government applications. Earlier approaches for large-scale text stream summarization used dynamic topic models and trending words. Instead, we rely on text embeddings -- low-dimensional word representations in a continuous vector space where similar words are embedded nearby each other. This paper presents ESTEEM, a novel tool for visualizing and evaluating spatiotemporal embeddings learned from streaming social media texts. Our tool allows users to monitor and analyze query words and their closest neighbors with an interactive interface. We used state-of-the-art techniques to learn embeddings and developed a visualization to represent dynamically changing relations between words in social media over time and other dimensions. This is the first interactive visualization of streaming text representations learned from social media texts that also allows users to contrast differences across multiple dimensions of the data.", "OpenNMT: Open-Source Toolkit for Neural Machine Translation.\n\nWe describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.", "WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation.\n\nDespite important progress in the area of intelligent systems, most such systems still lack commonsense knowledge that appears crucial for enabling smarter, more human-like decisions. In this paper, we present a system based on a series of algorithms to distill fine-grained disambiguated commonsense knowledge from massive amounts of text. Our WebChild 2.0 knowledge base is one of the largest commonsense knowledge bases available, describing over 2 million disambiguated concepts and activities, connected by over 18 million assertions.", "RelTextRank: An Open Source Framework for Building Relational Syntactic-Semantic Text Pair Representations.\n\nWe present a highly-flexible UIMA-based pipeline for developing structural kernel-based systems for relational learning from text, i.e., for generating training and test data for ranking, classifying short text pairs or measuring similarity between pieces of text. For example, the proposed pipeline can represent an input question and answer sentence pairs as syntactic-semantic structures, enriching them with relational information,  e.g., links between question class, focus and named entities, and serializes them as training and test files for the tree kernel-based reranking framework. The pipeline generates a number of dependency and shallow chunk-based representations shown to achieve competitive results in previous work. It also enables easy evaluation of the models thanks to cross-validation facilities.", "Tracing armed conflicts with diachronic word embedding models.\n\nRecent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting `cultural' semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the `anchor words' method which outperforms previous approaches on this set.", "The Rich Event Ontology.\n\nIn this paper we describe a new lexical semantic resource, The Rich Event On-tology, which provides an independent conceptual backbone to unify existing semantic role labeling (SRL) schemas and augment them with event-to-event causal and temporal relations.        By unifying the FrameNet, VerbNet, Automatic Content Extraction, and Rich Entities, Relations and Events resources, the ontology serves as a shared hub for the disparate annotation schemas and therefore enables the combination of SRL training data into a larger, more diverse corpus.  By adding temporal and causal relational information not found in any of the independent resources, the ontology facilitates reasoning on and across documents, revealing relationships between events that come together in temporal and causal chains to build more complex scenarios.  We envision the open resource serving as a valuable tool for both moving from the ontology to text to query for event types and scenarios of interest, and for moving from text to the ontology to access interpretations of events using the combined semantic information housed there.", "Integrating Decompositional Event Structures into Storylines.\n\nStoryline research links together events in stories and specifies shared participants in those stories. In these analyses, an atomic event is assumed to be a single clause headed by a single verb. However, many analyses of verbal semantics assume a decompositional analysis of events expressed in single clauses. We present a formalization of a decompositional analysis of events in which each participant in a clausal event has their own temporally extended subevent, and the subevents are related through causal and other interactions. This decomposition allows us to represent storylines as an evolving set of interactions between participants over time.", "Event Detection and Semantic Storytelling: Generating a Travelogue from a large Collection of Personal Letters.\n\nWe present an approach at identifying a specific class of events, movement action events (MAEs), in a data set that consists of ca. 2,800 personal letters exchanged by the German architect Erich Mendelsohn and his wife, Luise. A backend system uses these and other semantic analysis results as input for an authoring environment that digital curators can use to produce new pieces of digital content. In our example case, the human expert will receive recommendations from the system with the goal of putting together a travelogue, i.e., a description of the trips and journeys undertaken by the couple. We describe the components and architecture and also apply the system to news data.", "On the Creation of a Security-Related Event Corpus.\n\nThis paper reports on an effort of creating a corpus of structured information on security-related events automatically extracted from on-line news, part of which has been manually curated. The main motivation behind this effort is to provide material to the NLP community working on event extraction that could be used both for training and evaluation purposes.", "Improving Shared Argument Identification in Japanese Event Knowledge Acquisition.\n\nEvent knowledge represents the knowledge of causal and temporal relations between events. Shared arguments of event knowledge encode patterns of role shifting in successive events. A two-stage framework was proposed for the task of Japanese event knowledge acquisition, in which related event pairs are first extracted, and shared arguments are then identified to form the complete event knowledge. This paper focuses on the second stage of this framework, and proposes a method to improve the shared argument identification of related event pairs. We constructed a gold dataset for shared argument learning. By evaluating our system on this gold dataset, we found that our proposed model outperformed the baseline models by a large margin.", "Event Detection Using Frame-Semantic Parser.\n\nRecent methods for Event Detection focus on Deep Learning for automatic feature generation and feature ranking. However, most of those approaches fail to exploit rich semantic information, which results in relatively poor recall. This paper is a small \\& focused contribution, where we introduce an Event Detection and classification system, based on deep semantic information retrieved from a frame-semantic parser. Our experiments show that our system achieves higher recall than state-of-the-art systems. Further, we claim that enhancing our system with deep learning techniques like feature ranking can achieve even better results, as it can benefit from both approaches.", "Inference of Fine-Grained Event Causality from Blogs and Films.\n\nHuman understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80% of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire.", "newsLens: building and visualizing long-ranging news stories.\n\nWe propose a method to aggregate and organize a large, multi-source dataset of news articles into a collection of major stories, and automatically name and visualize these stories in a working system. The approach is able to run online, as new articles are added, processing 4 million news articles from 20 news sources, and extracting 80000 major stories, some of which span several years. The visual interface consists of lanes of timelines, each annotated with information that is deemed important for the story, including extracted quotations. The working system allows a user to search and navigate 8 years of story information.", "The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction.\n\nThis paper reports on the Event StoryLine Corpus (ESC) v1.0, a new benchmark dataset for the temporal and causal relation detection. By developing this dataset, we also introduce a new task, the StoryLine Extraction from news data, which aims at extracting and classifying events relevant for stories, from across news documents spread in time and clustered around a single seminal event or topic. In addition to describing the dataset, we also report on three baselines systems whose results show the complexity of the task and suggest directions for the development of more robust systems.", "The Circumstantial Event Ontology (CEO).\n\nIn this paper we describe the ongoing work on the Circumstantial Event Ontology (CEO), a newly developed ontology for calamity events that models semantic circumstantial relations between event classes. The circumstantial relations are designed manually, based on the shared properties of each event class. We discuss and contrast two types of event circumstantial relations: semantic circumstantial relations and episodic circumstantial relations. Further, we show the metamodel and the current contents of the ontology and outline the evaluation of the CEO.", "Detecting Changes in Twitter Streams using Temporal Clusters of Hashtags.\n\nDetecting events from social media data has important applications in public security, political issues, and public health. Many studies have focused on detecting specific or unspecific events from Twitter streams. However, not much attention has been paid to detecting changes, and their impact, in online conversations related to an event. We propose methods for detecting such changes, using clustering of temporal profiles of hashtags, and three change point detection algorithms. The methods were tested on two Twitter datasets: one covering the 2014 Ottawa shooting event, and one covering the Sochi winter Olympics. We compare our approach to a baseline consisting of detecting change from raw counts in the conversation. We show that our method produces large gains in change detection accuracy on both datasets.", "Inducing Event Types and Roles in Reverse: Using Function to Discover Theme.\n\nWith growing interest in automated event extraction, there is an increasing need to overcome the labor costs of hand-written event templates, entity lists, and annotated corpora. In the last few years, more inductive approaches have emerged, seeking to discover unknown event types and roles in raw text. The main recent efforts use probabilistic generative models, as in topic modeling, which are formally concise but do not always yield stable or easily interpretable results. We argue that event schema induction can benefit from greater structure in the process and in linguistic features that distinguish words' functions and themes. To maximize our use of limited data, we reverse the typical schema induction steps and introduce new similarity measures, building an intuitive process for inducing the structure of unknown events.", "Plotting Markson's ``Mistress''.\n\nThe post-modern novel ``Wittgenstein's Mistress'' by David Markson (1988) presents the reader with a very challenging non-linear narrative, that itself appears to one of the novel's themes. We present a distant reading of this work designed to complement a close reading of it by David Foster Wallace (1990).   Using a combination of text analysis, entity recognition and networks, we plot repetitive structures in the novel's narrative relating them to its critical analysis.", "A Dataset for Sanskrit Word Segmentation.\n\nThe last decade saw a surge in digitisation efforts for ancient manuscripts in Sanskrit. Due to various linguistic peculiarities inherent to the language, even the preliminary tasks such as word segmentation are non-trivial in Sanskrit. Elegant models for Word Segmentation in Sanskrit are indispensable for further syntactic and semantic processing of the manuscripts. Current works in word segmentation for Sanskrit, though commendable in their novelty, often have variations in their objective and evaluation criteria. In this work, we set the record straight. We formally define the objectives and the requirements for the word segmentation task. In order to encourage research in the field and to alleviate the time and effort required in pre-processing, we release a dataset of 115,000 sentences for word segmentation. For each sentence in the dataset we include the input character sequence, ground truth segmentation, and additionally lexical and morphological information about all the phonetically possible segments for the given sentence. In this work, we also discuss the linguistic considerations made while generating the candidate space of the possible segments.", "Machine Translation and Automated Analysis of the Sumerian Language.\n\nThis paper presents a newly funded international project for machine translation and automated analysis of ancient cuneiform languages where NLP specialists and Assyriologists collaborate to create an information retrieval system for Sumerian. This research is conceived in response to the need to translate large numbers of administrative texts that are only available in transcription, in order to make them accessible to a wider audience. The methodology includes creation of a specialized NLP pipeline and also the use of linguistic linked open data to increase access to the results.", "An End-to-end Environment for Research Question-Driven Entity Extraction and Network Analysis.\n\nThis paper presents an approach to extract co-occurrence networks from literary texts. It is a deliberate decision not to aim for a fully automatic pipeline, as the literary research questions need to guide both the definition of the nature of the things that co-occur as well as how to decide co-occurrence. We showcase the approach on a Middle High German romance, \\parz. Manual inspection and discussion shows the huge impact various choices  have.", "Investigating the Relationship between Literary Genres and Emotional Plot Development.\n\nLiterary genres are commonly viewed as being defined in terms of content and stylistic features. In this paper, we focus on one particular class of lexical features, namely emotion information, and investigate the hypothesis that emotion-related information correlates with particular genres. Us- ing genre classification as a testbed, we compare a model that computes lexicon- based emotion scores globally for complete stories with a model that tracks emotion arcs through stories on a subset of Project Gutenberg with five genres. Our main findings are: (a), the global emotion model is competitive with a large-vocabulary bag-of-words genre classifier (80%F1); (b), the emotion arc model shows a lower performance (59 % F1) but shows complementary behavior to the global model, as indicated by a very good performance of an oracle model (94 % F1) and an improved performance of an ensemble model (84 % F1); (c), genres differ in the extent to which stories follow the same emotional arcs, with particularly uniform behavior for anger (mystery) and fear (ad- ventures, romance, humor, science fiction).", "Modeling intra-textual variation with entropy and surprisal: topical vs. stylistic patterns.\n\nWe present a data-driven approach to investigate intra-textual variation by combining entropy and surprisal. With this approach we detect linguistic variation based on phrasal lexico-grammatical patterns across sections of research articles. Entropy is used to detect patterns typical of specific sections. Surprisal is used to differentiate between more and less informationally-loaded patterns as well as type of information (topical vs. stylistic). While we here focus on research articles in biology/genetics, the methodology is especially interesting for digital humanities scholars, as it can be applied to any text type or domain and combined with additional variables (e.g. time, author or social group).", "Finding a Character's Voice: Stylome Classification on Literary Characters.\n\nWe investigate in this paper the problem of classifying the stylome of characters in a literary work. Previous research in the field of authorship attribution has shown that the writing style of an author can be characterized and distinguished from that of other authors automatically. In this paper we take a look at the less approached problem of how the styles of different characters can be distinguished, trying to verify if an author managed to create believable characters with individual styles. We present the results of some initial experiments developed on the novel ``Liaisons Dangereuses'', showing that a simple bag of words model can be used to classify the characters.", "Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets.\n\nEnjambment takes place when a syntactic unit is broken up across two lines of poetry, giving rise to different stylistic effects. In Spanish literary studies, there are unclear points about the types of stylistic effects that can arise, and under which linguistic conditions. To systematically gather evidence about this, we developed a system to automatically identify enjambment (and its type) in Spanish. For evaluation, we manually annotated a reference corpus covering different periods. As a scholarly corpus to apply the tool, from public HTML sources we created a diachronic corpus covering four centuries of sonnets (3750 poems), and we analyzed the occurrence of enjambment across stanzaic boundaries in different periods. Besides, we found examples that highlight limitations in current definitions of enjambment.", "An Ontology-Based Method for Extracting and Classifying Domain-Specific Compositional Nominal Compounds.\n\nIn this paper, we present our preliminary study on an ontology-based method to extract and classify compositional nominal compounds in specific domains of knowledge. This method is based on the assumption that, applying a conceptual model to represent knowledge domain, it is possible to improve the extraction and classification of lexicon occurrences for that domain in a semi-automatic way. We explore the possibility of extracting and classifying a specific construction type (nominal compounds) spanning a specific domain (Cultural Heritage) and a specific language (Italian).", "Speeding up corpus development for linguistic research: language documentation and acquisition in Romansh Tuatschin.\n\nIn this paper, we present ongoing work for developing language resources and basic NLP tools for an undocumented variety of Romansh, in the context of a language documentation and language acquisition project. Our tools are meant to improve the speed and reliability of corpus annotations for noisy data involving large amounts of code-switching, occurrences of child-speech and orthographic noise. Being able to increase the efficiency of language resource development for language documentation and acquisition research also constitutes a step towards solving the data sparsity issues with which researchers have been struggling.", "Lexical Correction of Polish Twitter Political Data.\n\nLanguage processing architectures are often evaluated in near-to-perfect conditions with respect to processed content. The tools which perform sufficiently well on electronic press, books and other type of non-interactive content may poorly handle littered, colloquial and multilingual textual data which make the majority of communication today. This paper aims at investigating how Polish Twitter data (in a slightly controlled `political' flavour) differs from expectation of linguistic tools and how they could be corrected to be ready for processing by standard language processing chains available for Polish. The setting includes specialised components for spelling correction of tweets as well as hashtag and username decoding.", "Metaphor Detection in a Poetry Corpus.\n\nMetaphor is indispensable in poetry. It showcases the poet's creativity, and contributes to the overall emotional pertinence of the poem while honing its specific rhetorical impact. Previous work on metaphor detection relies on either rule-based or statistical models, none of them applied to poetry. Our method focuses on metaphor detection in a poetry corpus. It combines rule-based and statistical models (word embeddings) to develop a new classification system. Our system has achieved a precision of 0.759 and a recall of 0.804 in identifying one type of metaphor in poetry.", "Annotation Challenges for Reconstructing the Structural Elaboration of Middle Low German.\n\nIn this paper, we present the annotation challenges we have encountered when working on a historical language that was undergoing elaboration processes. We especially focus on syntactic ambiguity and gradience in Middle Low German, which causes uncertainty to some extent. Since current annotation tools consider construction contexts and the dynamics of the grammaticalization only partially, we plan to extend CorA - a web-based annotation tool for historical and other non-standard language data - to capture elaboration phenomena and annotator unsureness. Moreover, we seek to interactively learn morphological as well as syntactic annotations.", "Distantly Supervised POS Tagging of Low-Resource Languages under Extreme Data Sparsity: The Case of Hittite.\n\nThis paper presents a statistical approach to automatic morphosyntactic annotation of Hittite transcripts. Hittite is an extinct Indo-European language using the cuneiform script. There are currently no morphosyntactic annotations available for Hittite, so we explored methods of distant supervision. The annotations were projected from parallel German translations of the Hittite texts. In order to reduce data sparsity, we applied stemming of German and Hittite texts. As there is no off-the-shelf Hittite stemmer, a stemmer for Hittite was developed for this purpose. The resulting annotation projections were used to train a POS tagger, achieving an accuracy of 69% on a test sample. To our knowledge, this is the first attempt of statistical POS tagging of a cuneiform language.", "Phonological Soundscapes in Medieval Poetry.\n\nThe oral component of medieval poetry was integral to its performance and reception. Yet many believe that the medieval voice has been forever lost, and any attempts at rediscovering it are doomed to failure due to scribal practices, manuscript mouvance, and linguistic normalization in editing practices. This paper offers a method to abstract from this noise and better understand relative differences in phonological soundscapes by considering syllable qualities. The presented syllabification method and soundscape analysis offer themselves as cross-disciplinary tools for low-resource languages. As a case study, we examine medieval German lyric and argue that the heavily debated lyrical \u2018I' follows a unique trajectory through soundscapes, shedding light on the performance and practice of these poets.", "When does a compliment become sexist? Analysis and classification of ambivalent sexism using twitter data.\n\nSexism is prevalent in today's society, both offline and online, and poses a credible threat to social equality with respect to gender. According to ambivalent sexism theory (Glick and Fiske, 1996), it comes in two forms: Hostile and Benevolent. While hostile sexism is characterized by an explicitly negative attitude, benevolent sexism is more subtle. Previous works on computationally detecting sexism present online are restricted to identifying the hostile form. Our objective is to investigate the less pronounced form of sexism demonstrated online. We achieve this by creating and analyzing a dataset of tweets that exhibit benevolent sexism. By using Support Vector Machines (SVM), sequence-to-sequence models and FastText classifier, we classify tweets into \u2018Hostile', \u2018Benevolent' or \u2018Others' class depending on the kind of sexism they exhibit. We have been able to achieve an F1-score of 87.22% using FastText classifier. Our work helps analyze and understand the much prevalent ambivalent sexism in social media.", "Language-independent Gender Prediction on Twitter.\n\nIn this paper we present a set of experiments and analyses on predicting gender of Twitter users based on language-independent features extracted either from the text or the metadata of users' tweets. We perform our experiments on the TwiSty dataset containing manual gender annotations for users speaking six different languages. Our classification results show that, while the prediction model based on language-independent features performs worse than the bag-of-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances.", "Cross-Lingual Classification of Topics in Political Texts.\n\nIn this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we exploit continuous semantic text representations and induce a joint multilingual semantic vector spaces to enable supervised learning using manually-coded sentences across different languages. Our experimental results show that classifiers trained on multilingual data yield performance boosts over monolingual topic classification.", "Mining Social Science Publications for Survey Variables.\n\nResearch in Social Science is usually based on survey data where individual research questions relate to observable concepts (variables). However, due to a lack of standards for data citations a reliable identification of the variables used is often difficult. In this paper, we present a work-in-progress study that seeks to provide a solution to the variable detection task based on supervised machine learning algorithms, using a linguistic analysis pipeline to extract a rich feature set, including terminological concepts and similarity metric scores. Further, we present preliminary results on a small dataset that has been specifically designed for this task, yielding a significant increase in performance over the random baseline.", "Linguistic Markers of Influence in Informal Interactions.\n\nThere has been a long standing interest in understanding `Social Influence' both in Social Sciences and in Computational Linguistics. In this paper, we present a novel approach to study and measure interpersonal influence in daily interactions. Motivated by the basic principles of influence, we attempt to identify indicative linguistic features of the posts in an online knitting community. We present the scheme used to operationalize and label the posts as influential or non-influential. Experiments with the identified features show an improvement in the classification accuracy of influence by 3.15\\%. Our results illustrate the important correlation between the structure of the language and its potential to influence others.", "Non-lexical Features Encode Political Affiliation on Twitter.\n\nPrevious work on classifying Twitter users' political alignment has mainly focused on lexical and social network features. This study provides evidence that political affiliation is also reflected in features which have been previously overlooked: users' discourse patterns (proportion of Tweets that are retweets or replies) and their rate of use of capitalization and punctuation. We find robust differences between politically left- and right-leaning communities with respect to these discourse and sub-lexical features, although they are not enough to train a high-accuracy classifier.", "Personality Driven Differences in Paraphrase Preference.\n\nPersonality plays a decisive role in how people behave in different scenarios, including online social media. Researchers have used such data to study how personality can be predicted from language use. In this paper, we study phrase choice as a particular stylistic linguistic difference, as opposed to the mostly topical differences identified previously. Building on previous work on demographic preferences, we quantify differences in paraphrase choice from a massive Facebook data set with posts from over 115,000 users. We quantify the predictive power of phrase choice in user profiling and use phrase choice to study psycholinguistic hypotheses. This work is relevant to future applications that aim to personalize text generation to specific personality types.", "Modelling Participation in Small Group Social Sequences with Markov Rewards Analysis.\n\nWe explore a novel computational approach for analyzing member participation in small group social sequences. Using a complex state representation combining information about dialogue act types, sentiment expression, and participant roles, we explore which sequence states are associated with high levels of member participation. Using a Markov Rewards framework, we associate particular states with immediate positive and negative rewards, and employ a Value Iteration algorithm to calculate the expected value of all states. In our findings, we focus on discourse states belonging to team leaders and project managers which are either very likely or very unlikely to lead to participation from the rest of the group members.", "Code-Switching as a Social Act: The Case of Arabic Wikipedia Talk Pages.\n\nCode-switching has been found to have social motivations in addition to syntactic constraints. In this work, we explore the social effect of code-switching in an online community. We present a task from the Arabic Wikipedia to capture language choice, in this case code-switching between Arabic and other languages, as a predictor of social influence in collaborative editing. We find that code-switching is positively associated with Wikipedia editor success, particularly borrowing technical language on pages with topics less directly related to Arabic-speaking regions.", "How Does Twitter User Behavior Vary Across Demographic Groups?.\n\nDemographically-tagged social media messages are a common source of data for computational social science.  While these messages can indicate differences in beliefs and behaviors between demographic groups, we do not have a clear understanding of how different demographic groups use platforms such as Twitter.  This paper presents a preliminary analysis of how groups' differing behaviors may confound analyses of the groups themselves.  We analyzed one million Twitter users by first inferring demographic attributes, and then measuring several indicators of Twitter behavior. We find differences in these indicators across demographic groups, suggesting that there may be underlying differences in how different demographic groups use Twitter.", "Ideological Phrase Indicators for Classification of Political Discourse Framing on Twitter.\n\nPoliticians carefully word their statements in order to influence how others view an issue, a political strategy called framing. Simultaneously, these frames may also reveal the beliefs or positions on an issue of the politician. Simple language features such as unigrams, bigrams, and trigrams are important indicators for identifying the general frame of a text, for both longer congressional speeches and shorter tweets of politicians. However, tweets may contain multiple unigrams across different frames which limits the effectiveness of this approach. In this paper, we present a joint model which uses both linguistic features of tweets and ideological phrase indicators extracted from a state-of-the-art embedding-based model to predict the general frame of political tweets.", "community2vec: Vector representations of online communities encode semantic relationships.\n\nVector embeddings of words have been shown to encode meaningful semantic relationships that enable solving of complex analogies. This vector embedding concept has been extended successfully to many different domains and in this paper we both create and visualize vector representations of an unstructured collection of online communities based on user participation. Further, we quantitatively and qualitatively show that these representations allow solving of semantically meaningful community analogies and also other more general types of relationships. These results could help improve community recommendation engines and also serve as a tool for sociological studies of community relatedness.", "Telling Apart Tweets Associated with Controversial versus Non-Controversial Topics.\n\nIn this paper, we evaluate the predictability of tweets associated with controversial versus non-controversial topics. As a first step, we crowd-sourced the scoring of a predefined set of topics on a Likert scale from non-controversial to controversial. Our feature set entails and goes beyond sentiment features, e.g., by leveraging empathic language and other features that have been previously used but are new for this particular study. We find focusing on the structural characteristics of tweets to be beneficial for this task. Using a combination of emphatic, language-specific, and Twitter-specific features for supervised learning resulted in 87% accuracy (F1) for cross-validation of the training set and 63.4% accuracy when using the test set. Our analysis shows that features specific to Twitter or social media, in general, are more prevalent in tweets on controversial topics than in non-controversial ones. To test the premise of the paper, we conducted two additional sets of experiments, which led to mixed results. This finding will inform our future investigations into the relationship between language use on social media and the perceived controversiality of topics.", "Stronger Baselines for Trustable Results in Neural Machine Translation.\n\nInterest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios.  New research regularly introduces architectural and algorithmic improvements that lead to significant gains over ``vanilla'' NMT implementations.  However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-the-art production and shared-task systems.  As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use.  In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems.  Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed.  We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen.  This indicates that choosing a strong baseline is crucial for reporting reliable experimental results.", "An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation.\n\nTraining of neural machine translation (NMT) models usually uses mini-batches for efficiency purposes. During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation. Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed. However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared. This work investigates mini-batch creation strategies with experiments over two different datasets. Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling.", "Cost Weighting for Neural Machine Translation Domain Adaptation.\n\nIn this paper, we propose a new domain adaptation technique for neural machine translation called cost weighting, which is appropriate for adaptation scenarios in which a small in-domain data set and a large general-domain data set are available. Cost weighting incorporates a domain classifier into the neural machine translation training algorithm, using features derived from the encoder representation in order to distinguish in-domain from out-of-domain data. Classifier probabilities are used to weight sentences according to their domain similarity when updating the parameters of the neural translation model. We compare cost weighting to two traditional domain adaptation techniques developed for statistical machine translation: data selection and sub-corpus weighting. Experiments on two large-data tasks show that both the traditional techniques and our novel proposal lead to significant gains, with cost weighting outperforming the traditional methods.", "Detecting Untranslated Content for Neural Machine Translation.\n\nDespite its promise, neural machine translation (NMT) has a serious problem in that source content may be mistakenly left untranslated. The ability to detect untranslated content is important for the practical use of NMT. We evaluate two types of probability with which to detect untranslated content: the cumulative attention (ATN) probability and back translation (BT) probability from the target sentence to the source sentence. Experiments on detecting untranslated content in Japanese-English patent translations show that ATN and BT are each more effective than random choice, BT is more effective than ATN, and the combination of the two provides further improvements. We also confirmed the effectiveness of using ATN and BT to rerank the n-best NMT outputs.", "Beam Search Strategies for Neural Machine Translation.\n\nThe basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German to English and Chinese to English without losing any translation quality.", "Six Challenges for Neural Machine Translation.\n\nWe explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.", "Domain Aware Neural Dialogue System.\n\nChat systems have increasingly grown in importance with the widespread use of technology. This has spurred the development of open-domain dialog systems through neural models. These focus on generating long, relevant and informative responses to user queries. We present a novel approach to build a domain-aware neural dialog system to achieve the same. Our approach is based on the observation that each utterance in a conversation lies in a specific domain, the shift in domains is smooth and that the context in a conversation plays a vital role in deciding the response. To incorporate this observation, we propose two domain-aware models : a non-neural ensemble and a recurrent neural network based model that together with domain-specific sequence-to-sequence models generate pertinent responses. We evaluate these models on automatic metrics and compare the results to that of the Seq2Seq model. This is a submission to the extended abstract track.", "Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation.\n\nParallel corpora are often not as parallel as one might assume: non-literal translations and noisy translations abound, even in curated corpora routinely used for training and evaluation. We use a cross-lingual textual entailment system to distinguish sentence pairs that are parallel in meaning from those that are not, and show that filtering out divergent examples from training improves translation quality.", "Analyzing Neural MT Search and Model Performance.\n\nIn this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex search algorithm is necessary. Furthermore, we investigate the question if more complex models which might only be applicable during rescoring are promising. By separating the search space and the modeling using n-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small $n$-best list of $50$ hypotheses already contain notably better translations.", "An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation.\n\nRecently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a score function for the encoder states in all positions at each decoding step, the attention model greatly increases the computational complexity. In this paper, we investigate the adequate vision span of attention models in the context of machine translation, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term ``vision span''' means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50% with modest loss in accuracy on English-Japanese and German-English translation tasks.", "Interactive Beam Search for Visualizing Neural Machine Translation (extended abstract).\n\nWhile neural machine translation has been successful due to significantly better translation quality, it is hard to understand how and why such translation has been generated. To help the analysis, we propose an interactive beam search decoder, a webbased tool for visualizing and analyzing decoder behavior. We also propose methods and tool interfaces to modify attention layer of decoder and to observe its corresponding change. Finally, we show preliminary analysis of NMT behavior found with the tool we propose. This is a submission to the extended abstract track.", "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.\n\nWe present a simple and effective approach to incorporating syntactic structure into NMT  using graph-convolutional networks (GCNs). GCNs can produce representations of words that are sensitive to their syntactic neighborhoods. We evaluate the effectiveness of our approach on English-German and English-Czech translation tasks for different types of encoders and observe +1.2 and +0.7 BLEU point improvements, respectively, from using syntactic GCNs on top of BiRNNs. This is a submission to the extended abstract track.", "A Constituent-Centric Neural Architecture for Reading Comprehension.\n\nReading comprehension (RC), aiming to understand natural texts and answer questions therein, is a challenging task. In this paper, we study the RC problem on the Stanford Question Answering Dataset (SQuAD). Observing from the training set that most correct answers are centered around constituents in the parse tree, we design a constituent-centric neural architecture where the generation of candidate answers and their representation learning are both based on constituents and guided by the parse tree. Under this architecture, the search space of candidate answers can be greatly reduced without sacrificing the coverage of correct answers and the syntactic, hierarchical and compositional structure among constituents can be well captured, which contributes to better representation learning of the candidate answers. On SQuAD, our method achieves the state of the art performance and the ablation study corroborates the effectiveness of individual modules.", "Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding.\n\nIntegrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.", "Morphological Inflection Generation with Hard Monotonic Attention.\n\nWe present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft (Bahdanau, 2014) attention models for the task, shedding some light on the features such models extract.", "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection.\n\nThe state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.", "Improved Neural Relation Detection for Knowledge Base Question Answering.\n\nRelation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules.\n\nExtracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and find that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment on benchmark datasets and tweets data shows that SynTime outperforms state-of-the-art methods.", "Neural Belief Tracker: Data-Driven Dialogue State Tracking.\n\nOne of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.", "Linguistic analysis of differences in portrayal of movie characters.\n\nWe examine differences in portrayal of characters in movies using psycholinguistic and graph theoretic measures computed directly from screenplays. Differences are examined with respect to characters' gender, race, age and other metadata. Psycholinguistic metrics are extrapolated to dialogues in movies using a linear regression model built on a set of manually annotated seed words. Interesting patterns are revealed about relationships between genders of production team and the gender ratio of characters. Several correlations are noted between gender, race, age of characters and the linguistic metrics.", "Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts.\n\nMild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts.  In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments.", "Neural End-to-End Learning for Computational Argumentation Mining.\n\nWe investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning `natural' subtasks, in a multi-task learning setup, improves performance.", "A Convolutional Encoder Model for Neural Machine Translation.\n\nThe prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. We present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT'15 English-German we outperform several recently published results. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM.", "Adversarial Training for Unsupervised Bilingual Lexicon Induction.\n\nWord embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.", "Semantic Dependency Parsing via Book Embedding.\n\nWe model a dependency graph as a book, a particular kind of topological space, for semantic dependency parsing. The spine of the book is made up of a sequence of words, and each page contains a subset of noncrossing arcs. To build a semantic graph for a given sentence, we design new Maximum Subgraph algorithms to generate noncrossing graphs on each page, and a Lagrangian Relaxation-based algorithm tocombine pages into a book. Experiments demonstrate the effectiveness of the bookembedding framework across a wide range of conditions. Our parser obtains comparable results with a state-of-the-art transition-based parser.", "Multimodal Word Distributions.\n\nWord embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information.  To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely  expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.", "Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification.\n\nImplicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.", "Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability.\n\nKnowing the quality of reading comprehension (RC) datasets is important for the development of natural-language understanding systems. In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability. We applied these classes to six existing datasets, including MCTest and SQuAD, and highlighted the characteristics of the datasets according to each metric and the correlation between the two classes. Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer.", "Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation.\n\nThis paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.", "Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms.\n\nThis paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F1 score on ACE 2005 dataset.", "Deep Neural Machine Translation with Linear Associative Unit.\n\nDeep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural \u00a0Machine Translation (NMT) with its capability in modeling complex functions and capturing complex linguistic structures. However NMT with deep architecture in its encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often makes the optimization much more difficult. To address this problem we propose a novel linear associative units (LAU) \u00a0to reduce the gradient propagation path inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs uses linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time  The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported on results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.", "Learning to Ask: Neural Question Generation for Reading Comprehension.\n\nWe study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline;  it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (\\ie, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).", "Towards a Seamless Integration of Word Senses into Downstream NLP Applications.\n\nLexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.", "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction.\n\nUntil now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as ``Good'' or ``Acceptable'' in at least 95\\% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.", "A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data.\n\nPrevious studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chinese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods.", "Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering.\n\nWord embeddings have become widely-used in document analysis. While a large number of models for mapping words to vector spaces have been developed, it remains undetermined how much net gain can be achieved over traditional approaches based on bag-of-words. In this paper, we propose a new document clustering approach by combining any word embedding with a state-of-the-art algorithm for clustering empirical distributions. By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way. The new clustering method is easy to use and consistently outperforms other methods on a variety of data sets. More importantly, the method provides an effective framework for determining when and how much word embeddings contribute to document analysis. Experimental results with multiple embedding models are reported.", "Attention-over-Attention Neural Networks for Reading Comprehension.\n\nCloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces ``attended attention'' for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test.", "Context-Dependent Sentiment Analysis in User-Generated Videos.\n\nMultimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10% performance improvement over the state of the art and high robustness to generalizability.", "Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems.\n\nRecently emerged intelligent assistants on smartphones and home electronics (e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific task-oriented spoken dialogue systems and open-domain non-task-oriented ones. To realize such hybrid dialogue systems, this paper investigates determining whether or not a user is going to have a chat with the system. To address the lack of benchmark datasets for this task, we construct a new dataset consisting of 15,160 utterances collected from the real log data of a commercial intelligent assistant (and will release the dataset to facilitate future research activity). In addition, we investigate using tweets and Web search queries for handling open-domain user utterances, which characterize the task of chat detection. Experimental experiments demonstrated that, while simple supervised methods are effective, the use of the tweets and search queries further improves the F$\\_1$-score from 86.21 to 87.53.", "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution.\n\nMost existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1\\% F-score on OntoNotes 5.0 data.", "A Transition-Based Directed Acyclic Graph Parser for UCCA.\n\nWe present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.", "Deep Pyramid Convolutional Neural Networks for Text Categorization.\n\nThis paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text.  In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data.  However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications.  Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data.  Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much.  We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.", "Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning.\n\nGenerating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.", "Transductive Non-linear Learning for Chinese Hypernym Prediction.\n\nFinding the correct hypernyms for entities\u00a0is essential for taxonomy learning, fine-grained entity categorization, query understanding, etc. Due to the flexibility\u00a0of the Chinese language, it is challenging\u00a0to identify hypernyms in Chinese accurately. Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in\u00a0the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding\u00a0arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.", "Topical Coherence in LDA-based Models through Induced Segmentation.\n\nThis paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.", "Prerequisite Relation Learning for Concepts in MOOCs.\n\nWhat prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts. Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0% by F1-score) comparing with existing methods.", "Learning Structured Natural Language Representations for Semantic Parsing.\n\nWe introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are dif- ferent from linguistically motivated ones.", "Vancouver Welcomes You! Minimalist Location Metonymy Resolution.\n\nNamed entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.", "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.\n\nJoint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.", "Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG).\n\nOpinionated Natural Language Generation (ONLG) is a new, challenging, task that aims to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users' agendas, consisting of topics and sentiments, and based on wide-coverage automatically-acquired generative grammars. We compare three types of grammatical representations that we design for ONLG, which interleave different layers of linguistic information and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima'an, 2008) inspired grammar gets better language model scores than lexicalized grammars `a la Collins (2003), and that the latter gets better human-evaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.", "Polish evaluation dataset for compositional distributional semantics models.\n\nThe paper presents a procedure of building an evaluation dataset. for the validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish.", "Unsupervised Text Segmentation Based on Native Language Characteristics.\n\nMost work on segmenting text does so on the basis of topic changes, but it can be of interest to segment by other, stylistically expressed characteristics such as change of authorship or native language.  We propose a Bayesian unsupervised text segmentation approach to the latter.  While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation.", "Skip-Gram - Zipf + Uniform = Vector Additivity.\n\nIn recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected ``side-effect'' of such models is that their vectors often exhibit compositionality, i.e., \\emph{adding} two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., ``man'' + ``royal'' = ``king''. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.", "A Nested Attention Neural Hybrid Model for Grammatical Error Correction.\n\nGrammatical error correction (GEC) systems strive to correct both global errors inword order and usage, and local errors inspelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC.Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous  neural models for GEC as measured on the standard CoNLL-14 benchmark dataset.Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective incorrecting local errors that involve small edits in orthography.", "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders.\n\nWhile recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder from word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that capture the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved through introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence of discourse-level decision-making.", "An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge.\n\nWith the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the  promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.", "Topically Driven Neural Language Model.\n\nLanguage models are typically applied at the sentence level, without access to the broader document context.  We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence.  Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model.  Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.", "A Local Detection Approach for Named Entity Recognition and Mention Detection.\n\nIn this paper, we study a novel approach for named entity recognition (NER) and mention detection (MD) in natural language processing. Instead of treating NER as a sequence labeling problem, we propose a new local detection approach, which relies on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Subsequently, a simple feedforward neural network (FFNN) is learned to either reject or predict entity label for each individual text fragment. The proposed method has been evaluated in several popular NER and MD tasks, including CoNLL 2003 NER task and  TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our method has yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labeling methods.", "Enhanced LSTM for Natural Language Inference.\n\nReasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.", "An Unsupervised Neural Attention Model for Aspect Extraction.\n\nAspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.", "Semi-supervised Multitask Learning for Sequence Labeling.\n\nWe propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.", "Found in Translation: Reconstructing Phylogenetic Language Trees from Translations.\n\nTranslation has played an important role in trade, law, commerce, politics, and literature for thousands of years. Translators have always tried to be invisible; ideal translations should look as if they were written originally in the target language. We show that traces of the source language remain in the translation product to the extent that it is possible to uncover the history of the source language by looking only at the translation. Specifically, we automatically reconstruct phylogenetic language trees from monolingual texts (translated from several source languages). The signal of the source language is so powerful that it is retained even after two phases of translation. This strongly indicates that source language interference is the most dominant characteristic of translated texts, overshadowing the more subtle signals of universal properties of translation.", "Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs.\n\nWe study the Maximum Subgraph problem in deep dependency parsing. We consider two restrictions to deep dependency graphs: (a) 1-endpoint-crossing and (b) pagenumber-2. Our main contribution is an exact algorithm that ob- tains maximum subgraphs satisfying both restrictions simultaneously in time O(n5). Moreover, ignoring one linguistically-rare structure descreases the complexity to O(n4). We also extend our quartic-time algorithm into a practical parser with a discriminative disambiguation model and evaluate its performance on four linguistic data sets used in semantic dependency parsing.", "Improved Word Representation Learning with Sememes.\n\nSememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.", "Learning to Generate Market Comments from Stock Prices.\n\nThis paper presents a novel encoder-decoder model for automatically generating market comments from stock prices. The model first encodes both short- and long-term series of stock prices so that it can mention short- and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts.", "A Neural Local Coherence Model.\n\nWe propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range en- tity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.", "Adversarial Multi-Criteria Learning for Chinese Word Segmentation.\n\nDifferent linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.", "Linguistically Regularized LSTM for Sentiment Classification.\n\nThis paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.", "Selective Encoding for Abstractive Sentence Summarization.\n\nWe propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.", "Gated Self-Matching Networks for Reading Comprehension and Question Answering.\n\nIn this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.", "Neural Joint Model for Transition-based Chinese Syntactic Analysis.\n\nWe present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features.", "Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors.\n\nSolving cold-start problem in review spam detection is an urgent and significant task. It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work. This paper proposes a novel neural network model to detect review spam for cold-start problem, by learning to represent the new reviewers' review with jointly embedded textual and behavioral information. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way.", "Modeling Source Syntax for Neural Machine Translation.\n\nEven though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.", "Jointly Extracting Relations with Class Ties via Effective Deep Ranking.\n\nConnections between relations in relation extraction, which we call class ties, are common. In distantly supervised scenario, one entity tuple may have multiple relation facts. Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training. Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving state-of-the-art performance.", "Neural Word Segmentation with Rich Pretraining.\n\nNeural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.", "Flexible and Creative Chinese Poetry Generation Using Neural Memory.\n\nIt has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.", "Neural Relation Extraction with Multi-lingual Attention.\n\nRelation extraction has been widely used for finding unknown relational facts from plain text. Most existing methods focus on exploiting mono-lingual data for relation extraction, ignoring massive information from the texts in various languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs mono-lingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. Experimental results on real-world datasets show that, our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines.", "Automatically Labeled Data Generation for Large Scale Event Extraction.\n\nModern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.", "Adversarial Multi-task Learning for Text Classification.\n\nNeural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at \\url{https://nlp.fudan.edu.cn/data/}", "Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis.\n\nThe performance of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to the joint modeling of interactions between multiple predicates. However, this approach relies heavily on syntactic information predicted by parsers, and suffers from errorpropagation. To remedy this problem, we introduce a model that uses grid-type recurrent neural networks. The proposed model automatically induces features sensitive to multi-predicate interactions from the word sequence information of a sentence. Experiments on the NAIST Text Corpus demonstrate that without syntactic information, our model outperforms previous syntax-dependent models.", "Discourse Mode Identification in Essays.\n\nDiscourse modes play an important role in writing composition and evaluation. This paper presents a study on the manual and automatic identification of narration,exposition, description, argument and emotion expressing sentences in narrative essays. We annotate a corpus to study the characteristics of discourse modes and describe a neural sequence labeling model for identification. Evaluation results show that discourse modes can be identified automatically with an average F1-score of 0.7. We further demonstrate that discourse modes can be used as features that improve automatic essay scoring (AES). The impacts of discourse modes for AES are also discussed.", "Learning attention for historical text normalization by learning to pronounce.\n\nAutomated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how MTL works.", "Argument Mining with Structured SVMs and RNNs.\n\nWe propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.", "Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots.\n\nWe study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.", "CANE: Context-Aware Network Embedding for Relation Modeling.\n\nNetwork embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from \\url{https://github.com/thunlp/CANE}.", "Sequence-to-Dependency Neural Machine Translation.\n\nNowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and Japanese-English translation tasks.", "Creating Training Corpora for NLG Micro-Planners.\n\nIn this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers.  We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016's. We show that while Wen et al.'s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task.", "Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition.\n\nWe present a method for populating fine-grained classes (e.g., ``1950s American jazz musicians'') with instances (e.g., Charles Mingus ). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a >10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.", "Predicting Native Language from Gaze.\n\nA fundamental question in language learning concerns the role of a speaker's first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism.", "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network.\n\nCognitive NLP systems- i.e., NLP systems that make use of behavioral data - augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like Sentiment Analysis and Sarcasm Detection, and that even the extraction and choice of features should be delegated to the learning system.  We introduce a framework to automatically extract cognitive features from the eye-movement/gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i)  CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.", "Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts.\n\nUnderstanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics\u2014cooccurrence within documents and prevalence correlation over time\u2014our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other's prevalence over time, and yet rarely cooccur, almost like a ``cold war'' scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.", "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion.\n\nWe present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.", "Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization.\n\nWe present a  new supervised framework that learns to estimate automatic Pyramid scores and uses them for optimization-based extractive multi-document summarization. For learning automatic Pyramid scores, we developed a method for automatic training data generation which is based on a genetic algorithm using automatic Pyramid as the fitness function. Our experimental evaluation shows that  our new framework significantly outperforms strong baselines regarding automatic Pyramid, and that there is much room for improvement in comparison with the upper-bound for automatic Pyramid.", "Affect-LM: A Neural Language Model for Customizable Affective Text Generation.\n\nHuman verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research effort in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generation of conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM can generate naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.", "Universal Dependencies Parsing for Colloquial Singaporean English.\n\nSinglish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.", "Learning Semantic Correspondences in Technical Documentation.\n\nWe consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation.  We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates.  Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.", "A* CCG Parsing with a Supertag and Dependency Factored Model.\n\nWe propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing.", "Generating Contrastive Referring Expressions.\n\nThe referring expressions (REs) produced by a natural language generation (NLG) system can be misunderstood by the hearer, even when they are semantically correct. In an interactive setting, the NLG system can try to recognize such misunderstandings and correct them. We present an algorithm for generating corrective REs that use contrastive focus (``no, the BLUE button'') to emphasize the information the hearer most likely misunderstood. We show empirically that these contrastive REs are preferred over REs without contrast marking.", "Neural Discourse Structure for Text Categorization.\n\nWe show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization.  Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task.  Experiments consider variants of the approach and illustrate its strengths and weaknesses.", "Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks.\n\nWe present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%.  AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more.  The AMR parser does not rely on a syntactic pre-parse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.", "The State of the Art in Semantic Representation.\n\nSemantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.", "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation.\n\nWe introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.", "Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models.\n\nVolatility prediction\u2014an essential concept in financial markets\u2014has recently been addressed using sentiment analysis methods. We investigate the sentiment of annual disclosures of companies in stock markets to forecast volatility. We specifically explore the use of recent Information Retrieval (IR) term weighting models that are effectively extended by related terms using word embeddings. In parallel to textual information, factual market data have been widely used as the mainstream approach to forecast market risk. We therefore study different fusion methods to combine text and market data resources. Our word embedding-based approach significantly outperforms state-of-the-art methods. In addition, we investigate the characteristics of the reports of the companies in different financial sectors.", "Learning bilingual word embeddings with (almost) no bilingual data.\n\nMost methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.", "Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback.\n\nIn this paper, we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback. Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS. Our methods complement fully automatic methods in producing high-quality summaries with a minimum number of iterations and feedbacks. We conduct multiple simulation-based experiments and analyze the effect of feedback-based concept selection in the ILP setup in order to maximize the user-desired content in the summary.", "From Characters to Words to in Between: Do We Capture Morphology?.\n\nWords can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.", "FOIL it! Find One mismatch between Image and Language caption.\n\nIn this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and `foil' captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (`foil word'). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption  classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.", "Joint CTC/attention decoding for end-to-end speech recognition.\n\nEnd-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and context-dependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources.", "Obtaining referential word meanings from visual and distributional information: Experiments on object naming.\n\nWe investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.", "Chunk-based Decoder for Neural Machine Translation.\n\nChunks (or phrases) once played a pivotal role in machine translation. By using a chunk rather than a word as the basic translation unit, local (intra-chunk) and global (inter-chunk) word orders and dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the decoders used for neural machine translation (NMT). In this paper, we propose chunk-based decoders for (NMT), each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies while the word-level decoder decides the local word order in a chunk. To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the word-level decoder then uses as a basis to predict the words inside the chunk. Experimental results show that our proposed decoders can significantly improve translation performance in a WAT '16 English-to-Japanese translation task.", "Understanding and Predicting Empathic Behavior in Counseling Therapy.\n\nCounselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants' engagement, participants' verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to  build accurate counselor empathy classifiers with accuracies of up to 80%.", "Representations of language in a model of visually grounded speech signal.\n\nWe present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.", "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules.\n\nMorphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that  'inexpensive' is a rephrasing for 'expensive' or may not associate 'acquire' with 'acquires'. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.", "What do Neural Machine Translation Models Learn about Morphology?.\n\nNeural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.", "Cross-lingual Distillation for Text Classification.\n\nCross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.", "Riemannian Optimization for Skip-Gram Negative Sampling.\n\nSkip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in ``word2vec'' software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling.\n\nInteractive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, go- ing beyond existing single word anchor algorithms\u2014an approach we call ``Tan- dem Anchors''. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interac- tive and non-interactive approaches. Tan- dem anchors are faster and more intuitive than existing interactive approaches.", "EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks.\n\nAccurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets.  In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutick's 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.", "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning.\n\nEnd-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors.  We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset (Bordes and Weston, 2016), and outperform two commercially deployed customer-facing dialog systems at our company.", "Learning Character-level Compositionality with Visual Features.\n\nPrevious work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry topical content which resulting in embeddings that are coherent in visual space.", "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling.\n\nFixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the ``bursty'' distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.", "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling.\n\nRecurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.", "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.\n\nSolving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.", "Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication.\n\nTo enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs. Grounded verb semantics are acquired mainly based on the parallel data of the use of a verb phrase and its corresponding sequences of primitive actions demonstrated by humans. The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored. To address this limitation, this paper presents a new interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn models for grounded verb semantics. The proposed approach uses reinforcement learning to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the long-term reward. Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties. Compared to previous work, the models acquired from interactive learning result in a 48% to 145% performance gain when applied in new situations.", "Abstractive Document Summarization with a Graph-Based Attentional Neural Model.\n\nAbstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.", "Semi-supervised sequence tagging with bidirectional language models.\n\nPre-trained word embeddings learned from unlabeled text have become a stan- dard component of neural network archi- tectures for NLP tasks. However, in most cases, the recurrent network that oper- ates on word-level representations to pro- duce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidi- rectional language models to NLP sys- tems and apply it to sequence labeling tasks. We evaluate our model on two stan- dard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search.\n\nWe present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model's output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.", "Semantic Word Clusters Using Signed Spectral Clustering.\n\nVector space representations of words capture many aspects of word similarity, but such methods tend to produce vector spaces in which antonyms (as well as synonyms) are close to each other. For spectral clustering using such word embeddings, words are points in a vector space where synonyms are linked with positive weights, while antonyms are linked with negative weights. We present a new signed spectral normalized graph cut algorithm, {\\em signed clustering}, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words that simultaneously capture distributional and synonym relations. By using randomized spectral decomposition (Halko et al., 2011) and sparse matrices, our method is both fast and scalable. We validate our clusters using datasets containing human judgments of word pair similarities and show the benefit of using our word clusters for sentiment prediction.", "Detecting annotation noise in automatically labelled data.\n\nWe introduce a method for error detection in automatically annotated text, aimed at supporting the creation of high-quality language resources at affordable cost. Our method combines an unsupervised generative model with human supervision from active learning. We test our approach on in-domain and out-of-domain data in two languages, in AL simulations and in a real world setting. For all settings, the results show that our method is able to detect annotation errors with high precision and high recall.", "Learning Word-Like Units from Joint Audio-Visual Analysis.\n\nGiven a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "Robust Incremental Neural Semantic Graph Parsing.\n\nParsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.", "A Multidimensional Lexicon for Interpersonal Stancetaking.\n\nThe sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself. Stancetaking underlies a wide range of interactional phenomena, relating to formality, politeness, affect, and subjectivity. We present a computational approach to stancetaking, in which we build a theoretically-motivated lexicon of stance markers, and then use multidimensional analysis to identify a set of underlying stance dimensions. We validate these dimensions intrinscially and extrinsically, showing that they are internally coherent, match pre-registered hypotheses, and correlate with social phenomena.", "Semi-Supervised QA with Generative Domain-Adaptive Nets.\n\nWe study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the \\textit{Generative Domain-Adaptive Nets}. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.", "Multi-Task Video Captioning with Video and Entailment Generation.\n\nVideo captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.", "Adversarial Adaptation of Synthetic or Stale Data.\n\nTwo types of data shift common in practice are 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift). Both cause a distribution mismatch between training and evaluation, leading to a model that overfits the flawed training data and performs poorly on the test data. We propose a solution to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain. To this end, we use and build on several recent advances in neural domain adaptation such as adversarial training (Ganinet al., 2016) and domain separation network (Bousmalis et al., 2016), proposing a new effective adversarial training scheme. In both supervised and unsupervised adaptation scenarios, our approach yields clear improvement over strong baselines.", "Aggregating and Predicting Sequence Labels from Crowd Annotations.\n\nDespite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text. Given such annotations, we consider two complementary tasks:  (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a model that can predict sequences in unannotated text. For aggregation, we propose a novel Hidden Markov Model variant. To predict sequences in unannotated text, we propose a neural approach using Long Short Term Memory. We evaluate a suite of methods across two different applications and text genres: Named-Entity Recognition in news articles and Information Extraction from biomedical abstracts. Results show improvement over strong baselines. Our source code and data are available online.", "Deep Multitask Learning for Semantic Dependency Parsing.\n\nWe present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron,  our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches---one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/Noahs-ARK/NeurboParser.", "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning.\n\nBandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.", "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder.\n\nMost neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.", "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision.\n\nHarnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural ``programmer'', i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic ``computer'', i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.", "Using Global Constraints and Reranking to Improve Cognates Detection.\n\nGlobal constraints and reranking have not been used in cognates detection research to date. We propose methods for using global constraints by performing rescoring of the score matrices produced by state of the art cognates detection systems. Using global constraints to perform rescoring is complementary to state of the art methods for performing cognates detection and results in significant performance improvements beyond current state of the art performance on publicly available datasets with different language pairs and various conditions such as different levels of baseline state of the art performance and different data size conditions, including with more realistic large data size conditions than have been evaluated with in the past.", "Beyond Binary Labels: Political Ideology Prediction of Twitter Users.\n\nAutomatic political orientation prediction from social media posts has to date proven successful only in distinguishing between publicly declared liberals and conservatives in the US. This study examines users' political ideology using a seven-point scale which enables us to identify politically moderate and neutral users --- groups which are of particular interest to political scientists and pollsters. Using a novel data set with political ideology labels self-reported through surveys, our goal is two-fold: a) to characterize the groups of politically engaged users through language use on Twitter; b) to build a fine-grained model that predicts political ideology of unseen users. Our results identify differences in both political leaning and engagement and the extent to which each group tweets using political keywords. Finally, we demonstrate how to improve ideology prediction accuracy by exploiting the relationships between the user groups.", "MalwareTextDB: A Database for Annotated Malware Articles.\n\nCybersecurity risks and malware threats are becoming increasingly dangerous and common. Despite the severity of the problem, there has been few NLP efforts focused on tackling cybersecurity. In this paper, we discuss the construction of a new database for annotated malware texts. An annotation framework is introduced based on the MAEC vocabulary for defining malware characteristics, along with a database consisting of 39 annotated APT reports with a total of 6,819 sentences. We also use the database to construct models that can potentially help cybersecurity researchers in their data collection and analytics efforts.", "A Corpus of Annotated Revisions for Studying Argumentative Writing.\n\nThis paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer's purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction.", "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access.\n\nThis paper proposes KB-InfoBot - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ``soft'' posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.", "Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction.\n\nWe propose a novel geolocation prediction model using a complex neural network. Geolocation prediction in social media has attracted many researchers to use information of various types. Our model unifies text, metadata, and user network representations with an attention mechanism to overcome previous ensemble approaches. In an evaluation using two open datasets, the proposed model exhibited a maximum 3.8% increase in accuracy and a maximum of 6.6% increase in accuracy@161 against previous models. We further analyzed several intermediate layers of our model, which revealed that their states capture some statistical characteristics of the datasets.", "Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees.\n\nWe present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. We show that attention along with long short term memory (LSTM) network can extract semantic relations between entity mentions without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora show that our model significantly outperforms feature-based joint model by Li and Ji (2014). We also compare our model with an end-to-end tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our model performs within 1\\% on entity mentions and 2\\% on relations. Our fine-grained analysis also shows that our model performs significantly better on Agent-Artifact relations, while SPTree performs better on Physical and Part-Whole relations.", "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses.\n\nAutomatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem.  Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores.   We show that the ADEM model's predictions correlate significantly,  and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training,                    an important step for automatic dialogue evaluation.", "Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance.\n\nWe propose a perspective on dialogue that focuses on relative information contributions of conversation partners as a key to successful communication. We predict the success of collaborative task in English and Danish corpora of task-oriented dialogue. Two features are extracted from the frequency domain representations of  the lexical entropy series of each interlocutor, power spectrum overlap (PSO) and relative phase (RP). We find that PSO is a negative predictor of task success, while RP is a positive one. An SVM with these features significantly improved on previous task success prediction models. Our findings suggest that the strategic distribution of information density between interlocutors  is relevant to task success.", "Deep Semantic Role Labeling: What Works and What's Next.\n\nWe introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.", "Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks.\n\nWe introduce a composite deep neural network architecture for supervised and language independent context sensitive lemmatization. The proposed method considers the task as to identify the correct edit tree representing the transformation between a word-lemma pair. To find the lemma of a surface word, we exploit two successive bidirectional gated recurrent structures - the first one is used to extract the character level dependencies and the next one captures the contextual information of the given word. The key advantages of our model compared to the state-of-the-art lemmatizers such as Lemming and Morfette are - (i) it is independent of human decided features (ii) except the gold lemma, no other expensive morphological attribute is required for joint learning. We evaluate the lemmatizer on nine languages - Bengali, Catalan, Dutch, Hindi, Hungarian, Italian, Latin, Romanian and Spanish. It is found that except Bengali, the proposed method outperforms Lemming and Morfette on the other languages. To train the model on Bengali, we develop a gold lemma annotated dataset (having 1,702 sentences with a total of 20,257 word tokens), which is an additional contribution of this work.", "Automatically Generating Rhythmic Verse with Neural Networks.\n\nWe propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machine-generated poem to be the best amongst all evaluated.", "Coarse-to-Fine Question Answering for Long Documents.\n\nWe present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models. While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them over long documents is prohibitively slow because it is difficult to parallelize over sequences. Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences. We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WikiReading dataset and on a new dataset, while speeding up the model by 3.5x-6.7x.", "Neural Machine Translation via Binary Code Prediction.\n\nIn this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.", "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction.\n\nLabeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.", "Leveraging Knowledge Bases in LSTMs for Improving Machine Reading.\n\nThis paper focuses on how to take advantage of external knowledge bases (KBs) to improve recurrent neural networks for machine reading. Traditional methods that exploit knowledge from KBs encode knowledge as discrete indicator features. Not only do these features generalize poorly, but they require task-specific feature engineering to achieve good performance. We propose KBLSTM, a novel neural model that leverages continuous representations of KBs to enhance the learning of recurrent neural networks for machine reading. To effectively integrate background knowledge with information from the currently processed text, our model employs an attention mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset.", "Gated-Attention Readers for Text Comprehension.\n\nIn this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \\\\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention.", "Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique.\n\nWord-level language detection is necessary for analyzing code-switched text, where multiple languages could be mixed within a sentence. Existing models are restricted to code-switching between two specific languages and fail in real-world scenarios as text input rarely has a priori information on the languages used. We present a novel unsupervised word-level language detection technique for code-switched text for an arbitrarily large number of languages, which does not require any manually annotated training data. Our experiments with tweets in seven languages show a 74% relative error reduction in word-level labeling with respect to competitive baselines. We then use this system to conduct a large-scale quantitative analysis of code-switching patterns on Twitter, both global as well as region-specific, with 58M tweets.", "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment.\n\nType-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.", "Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task.\n\nUnderstanding common entities and their attributes is a primary requirement for any system that comprehends natural language. In order to enable learning about common entities, we introduce a novel machine comprehension task, GuessTwo: given a short paragraph comparing different aspects of two real-world semantically-similar entities, a system should guess what those entities are. Accomplishing this task requires deep language understanding which enables inference, connecting each comparison paragraph to different levels of knowledge about world entities and their attributes. So far we have crowdsourced a dataset of more than 14K comparison paragraphs comparing entities from a variety of categories such as fruits and animals. We have designed two schemes for evaluation: open-ended, and binary-choice prediction. For benchmarking further progress in the task, we have collected a set of paragraphs as the test set on which human can accomplish the task with an accuracy of 94.2\\% on open-ended prediction. We have implemented various models for tackling the task, ranging from semantic-driven to neural models. The semantic-driven approach outperforms the neural models, however, the results indicate that the task is very challenging across the models.", "Deep Keyphrase Generation.\n\nKeyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks.  We name it as \\textit{deep keyphrase generation} since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.", "Diversity driven attention model for query-based abstractive summarization.\n\nAbstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28\\% (absolute) in ROUGE-L scores.", "Naturalizing a Programming Language via Interactive Learning.\n\nOur goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to ``naturalize'' the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9\\% of the last 10K utterances.", "Abstract Syntax Networks for Code Generation and Semantic Parsing.\n\nTasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.", "Cross-lingual Name Tagging and Linking for 282 Languages.\n\nThe ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating ``silver-standard'' annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.", "Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations.\n\nCultural fit is widely believed to affect the success of individuals and the groups to which they belong. Yet it remains an elusive, poorly measured construct. Recent research draws on computational linguistics to measure cultural fit but overlooks asymmetries in cultural adaptation. By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person's word use on another's and distinguishes between two enculturation mechanisms: internalization and self-regulation. We use this measure to trace employees' enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals' downstream outcomes, especially involuntary exit. Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.", "Combating Human Trafficking with Multimodal Deep Models.\n\nHuman trafficking is a global epidemic affecting millions of people across the planet. Sex trafficking, the dominant form of human trafficking, has seen a significant rise mostly due to the abundance of escort websites, where human traffickers can openly advertise among at-will escort advertisements. In this paper, we take a major step in the automatic detection of advertisements suspected to pertain to human trafficking. We present a novel dataset called Trafficking-10k, with more than 10,000~advertisements annotated for this task. The dataset contains two sources of information per advertisement: text and images. For the accurate detection of trafficking advertisements, we designed and trained a deep multimodal model called the Human Trafficking Deep Network (HTDN).", "Reading Wikipedia to Answer Open-Domain Questions.\n\nThis paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.", "Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains.\n\nIn this paper, we study how to improve the domain adaptability of a deletion-based Long Short-Term Memory (LSTM) neural network model for sentence compression. We hypothesize that syntactic information helps in making such models more robust across domains. We propose two major changes to the model: using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming (ILP). Our evaluation shows that the proposed model works better than the original model as well as a traditional non-neural-network-based model in a cross-domain setting.", "MORSE: Semantic-ally Drive-n MORpheme SEgment-er.\n\nWe present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies  of  available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality.  We validate our algorithm across datasets and present state-of-the-art results.", "Learning a Neural Semantic Parser from User Feedback.\n\nWe present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.", "Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter.\n\nFraming is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.", "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings.\n\nWe consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.", "Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning.\n\nHow fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-of-the-art rumor detection models.", "Active Sentiment Domain Adaptation.\n\nDomain adaptation is an important technology to handle domain dependence problem in sentiment analysis field. Existing methods usually rely on sentiment classifiers trained in source domains. However, their performance may heavily decline if the distributions of sentiment features in source and target domains have significant difference. In this paper, we propose an active sentiment domain adaptation approach to handle this problem. Instead of the source domain sentiment classifiers, our approach adapts the general-purpose sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain. A unified model is proposed to fuse different types of sentiment information and train sentiment classifier for target domain. Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples.", "Probabilistic Typology: Deep Generative Models of Vowel Inventories.\n\nLinguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory?  We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches.  We provide a comprehensive suite of experiments on over 200 distinct languages.", "Visualizing and Understanding Neural Machine Translation.\n\nWhile neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoder-decoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.", "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.\n\nWe present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer  pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions,  (2)  has considerable syntactic and  lexical                                      variability  between questions and corresponding answer-evidence  sentences,  and  (3) requires more cross sentence reasoning to find answers.  We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.", "Deep Learning in Semantic Kernel Spaces.\n\nKernel methods enable the direct usage of structured representations of textual data during language learning and inference tasks. Expressive kernels, such as Tree Kernels, achieve excellent performance in NLP. On the other side, deep neural networks have been demonstrated effective in automatically learning feature representations during training. However, their input is tensor data, i.e., they can not manage rich structured information. In this paper, we show that expressive kernels and deep neural networks can be combined in a common framework in order to (i) explicitly model structured information and (ii) learn non-linear decision functions. We show that the input layer of a deep architecture can be pre-trained through the application of the Nystrom low-rank approximation of kernel spaces. The resulting ``kernelized'' neural network achieves state-of-the-art accuracy in three different tasks.", "Automatic Induction of Synsets from a Graph of Synonyms.\n\nThis paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Second, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.", "Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization.\n\nAlthough neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning processing of the neural translation model. Experiments on Chinese-English dataset show that our approach leads to significant improvements.", "Translating Neuralese.\n\nSeveral approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them.  Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener.  We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.", "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation.\n\nSequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.", "Joint Modeling of Content and Discourse Relations in Dialogues.\n\nWe present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members' understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.", "Learning to Skim Text.\n\nRecurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\\\\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.", "Get To The Point: Summarization with Pointer-Generator Networks.\n\nNeural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.", "Bayesian Modeling of Lexical Resources for Low-Resource Settings.\n\nLexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better. In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed generative model. The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings: part-of-speech induction and low-resource named-entity recognition.", "Search-based Neural Structured Learning for Sequential Question Answering.\n\nRecent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our model effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions.", "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings.\n\nWe study a \\emph{symmetric collaborative dialogue} setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.", "Don't understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures.\n\nAn interesting aspect of structured prediction is the evaluation of an output structure against the gold standard. Especially in the loss-augmented setting, the need of finding the max-violating constraint has severely limited the expressivity of effective loss functions. In this paper, we trade off exact computation for enabling the use and study of more complex loss functions for coreference resolution. Most interestingly, we show that such functions can be (i) automatically learned also from controversial but commonly accepted coreference measures, e.g., MELA, and (ii) successfully used in learning algorithms. The accurate model comparison on the standard CoNLL-2012 setting shows the benefit of more expressive loss functions.", "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood.\n\nOur goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by \\emph{spurious programs}: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.", "Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization.\n\nWe presents in this paper our approach for modeling inter-topic preferences of Twitter users: for example, ``those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade''. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, electoral prediction, electoral campaigns, and online debates. In order to extract users' preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., ``A is completely wrong''). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users' preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.", "A Teacher-Student Framework for Zero-Resource Neural Machine Translation.\n\nWhile end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, our method is able to train a source-to-target NMT model (``student'') without parallel corpora available guided by an existing pivot-to-target NMT model (``teacher'') on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.", "An Interpretable Knowledge Transfer Model for Knowledge Base Completion.\n\nKnowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.", "Domain Attention with an Ensemble of Experts.\n\nAn important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains. One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daum\u00b4e III (2009). However, it is desirable to adapt without having to re-estimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a solution based on attending an ensemble of domain experts. We assume K domain specific intent and slot models trained on respective domains. When given domain K + 1, our model uses a weighted combination of the K domain experts' feedback along with its own opinion to make predictions on the new domain. In experiments, the model significantly outperforms baselines that do not use domain adaptation and also performs better than the full retraining approach.", "Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix.\n\nDistant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervision in relation extraction. We show that the dynamic transition matrix can effectively characterize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios.", "Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing.\n\nWe present a simple encoding for unlabeled noncrossing graphs and show how its latent counterpart helps us to represent several families of directed and undirected graphs used in syntactic and semantic parsing of natural language as context-free languages.  The families are separated purely on the basis of forbidden patterns in latent encoding, eliminating the need to differentiate the families of non-crossing graphs in inference algorithms: one algorithm works for all when the search space can be controlled in parser input.", "A Minimal Span-Based Neural Constituency Parser.\n\nIn this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).", "TextFlow: A Text Similarity Measure based on Continuous Sequences.\n\nText similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include n-grams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on 8 different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.", "Joint Learning for Event Coreference Resolution.\n\nWhile joint models have been developed for many NLP tasks, the vast majority of event coreference resolvers, including the top-performing resolvers competing in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, are pipeline-based, where the propagation of errors from the trigger detection component to the event coreference component is a major performance limiting factor. To address this problem, we propose a model for jointly learning event coreference, trigger detection, and event anaphoricity. Our joint model is novel in its choice of tasks and its features for capturing cross-task interactions. To our knowledge, this is the first attempt to train a mention-ranking model and employ event anaphoricity for event coreference. Our model achieves the best results to date on the KBP 2016 English and Chinese datasets.", "Verb Physics: Relative Physical Knowledge of Actions and Objects.\n\nLearning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., ``My house is bigger than me.'' However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, ``Tyler entered his house'' implies that his house is bigger than Tyler. In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.", "Semantic Parsing of Pre-university Math Problems.\n\nWe have been developing an end-to-end math problem solving system that accepts natural language input. The current paper focuses on how we analyze the problem sentences to produce logical forms. We chose a hybrid approach combining a shallow syntactic analyzer and a manually-developed lexicalized grammar. A feature of the grammar is that it is extensively typed on the basis of a formal ontology for pre-university math. These types are helpful in semantic disambiguation inside and across sentences. Experimental results show that the hybrid system produces a well-formed logical form with 88\\% precision and 56\\% recall.", "An Algebra for Feature Extraction.\n\nThough feature extraction is a necessary first step in statistical NLP, it is often seen as a mere preprocessing step. Yet, it can dominate computation time, both during training, and especially at deployment. In this paper, we formalize feature extraction from an algebraic perspective. Our formalization allows us to define a message passing algorithm that can restructure feature templates to be more computationally efficient. We show via experiments on text chunking and relation extraction that this restructuring does indeed speed up feature extraction in practice by reducing redundant computation.", "A Syntactic Neural Model for General-Purpose Code Generation.\n\nWe consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.", "PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents.\n\nThe large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document's content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word's occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically, on several datasets of research papers, PositionRank achieves improvements as high as $29.09\\%$.", "A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing.\n\nRestricted non-monotonicity has been shown beneficial for the projective arc-eager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss. Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.", "Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation.\n\nSarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, ''Sarcasm is the giant chasm between what I say, and the person who doesn't get it.''. In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN's interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.", "Active Discriminative Text Representation Learning.\n\n", "Using millions of emoji occurrences to pretrain any-domain models for detecting emotion, sentiment and sarcasm.\n\nNLP tasks are often limited by the scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn a richer emotional representations. Through emoji prediction on a dataset of 634 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within emotion, sentiment and sarcasm detection using a single pretrained model. Our analysis shows that the diversity of our noisy labels is important for the performance of our model. We release our pretrained model.", "Evaluating Layers of Representation in Neural Machine Translation on Syntactic and Semantic Tagging.\n\nWhile neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and semantic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech (POS) and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers are better for POS tagging.", "Machine Comprehension by Text-to-Text Neural Question Generation.\n\nWe propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.", "Emergent Predication Structure in Hidden State Vectors of Neural Readers.\n\nA significant number of neural architectures for reading comprehension have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of ``predication structure'' in the hidden state vectors of these readers.  More specifically, we provide evidence that the hidden state vectors represent atomic formulas $\\Phi[c]$ where $\\Phi$ is a semantic property (predicate) and $c$ is a constant symbol entity identifier.", "Towards Harnessing Memory Networks for Coreference Resolution.\n\nCoreference resolution task demands comprehending a discourse, especially for anaphoric mentions which require semantic information for resolving antecedents. We investigate into how memory networks can be helpful for coreference resolution when posed as question answering problem. The comprehension capability of memory networks assists coreference resolution, particularly for the mentions that require semantic and context information. We experiment memory networks for coreference resolution, with 4 synthetic datasets generated for coreference resolu- tion with varying difficulty levels. Our system's performance is compared with a traditional coreference resolution system to show why memory network can be promising for coreference resolution.", "Combining Word-Level and Character-Level Representations for Relation Classification of Informal Text.\n\nWord representation models have achieved great success in natural language processing tasks, such as relation classification. However, it does not always work on informal text, and the morphemes of some misspelling  words may carry important short-distance semantic information. We propose a hybrid model, combining the merits of word-level and character-level representations to learn better representations on informal text. Experiments on two dataset of relation classification, SemEval-2010 Task8 and a large-scale one we compile from informal text, show that our model achieves a competitive result in the former and state-of-the-art with the other.", "Regularized Topic Models for Sparse Interpretable Word Embeddings.\n\nProbabilistic topic modeling is a tool for semantic analysis of texts widely used during the last decades. Word embeddings have gained their popularity more recently being inspired by achievements in neural networks. In this paper we consider both approaches from the perspective of learning hidden semantic representations of words via matrix factorization techniques. We show that a topic modeling performed over word co-occurrence data is capable of producing state-of-the-art results for word similarity task. Unlike SGNS, the components of obtained word embeddings are interpretable and highly sparse. We impose further requirements by additive regularization approach thus bridging the gap between word embeddings and extensions of topic models.", "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings.\n\nThe blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.", "Transfer Learning for Neural Semantic Parsing.\n\nThe goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence model and compare their performance with the independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to the target task with smaller labeled data. We see an absolute accuracy gain ranging from 1.0% to 4.4% in in our in-house data set and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.", "MUSE: Modularizing Unsupervised Sense Embeddings.\n\nThis paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work about learning multi-sense embeddings suffered from either ambiguity of different-level embeddings or inefficient sense selection. The proposed modular framework, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with linear-time sense selection. Further, joint training on the proposed modules is achieved. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.", "Modeling Large-Scale Structured Relationships with Shared Memory for Knowledge Base Completion.\n\nRecent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed triplets could be costly. Hence, a manually designed procedure is often used when training the models. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform multi-step inference implicitly through a controller and shared memory. Without a human-designed inference procedure, IRNs use training data to learn to perform multi-step inference in an embedding neural space through the shared memory and controller. While the inference procedure does not explicitly operate on top of observed triplets, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "Knowledge Base Completion: Baselines Strike Back.\n\nMany papers have been published on the knowledge base completion task in the past few years. Most of these introduce novel architectures for relation learning that are evaluated on standard datasets like FB15k and WN18. This paper shows that the accuracy of almost all models published on the FB15k can be outperformed by an appropriately tuned baseline --- our reimplementation of the DistMult model. Our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyper-parameter tuning or different training objectives. This should prompt future research to re-consider how the performance of models is evaluated and reported.", "Sequential Attention: A Context-Aware Alignment Function for Machine Reading.\n\nIn this paper we  propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (on the Who did What and CNN datasets) and show that it dramatically improves a strong baseline---the Stanford Reader---and is competitive with the state of the art.", "Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines.\n\nVector representations and vector space modeling (VSM) play a central role in modern machine learning. We propose a novel approach to \u2018vector similarity searching' over dense semantic representations of words and documents that can be deployed on top of traditional inverted-index-based fulltext engines, taking advantage of their robustness, stability, scalability and ubiquity. We show that this approach allows the indexing and querying of dense vectors in text domains. This opens up exciting avenues for major efficiency gains, along with simpler deployment, scaling and monitoring. The end result is a fast and scalable vector database with a tunable trade-off between vector search performance and quality, backed by a standard fulltext engine such as Elasticsearch. We empirically demonstrate its querying performance and quality by applying this solution to the task of semantic searching over a dense vector representation of the entire English Wikipedia.", "Multi-task Domain Adaptation for Sequence Tagging.\n\nMany domain adaptation approaches rely on learning cross domain shared representations to transfer the knowledge learned in one domain to other domains. Traditional domain adaptation only considers adapting for one task. In this paper, we explore multi-task representation learning under the domain adaptation scenario. We propose a neural network framework that supports domain adaptation for multiple tasks simultaneously, and learns shared representations that better generalize for domain adaptation. We apply the proposed framework to domain adaptation for sequence tagging problems considering two tasks: Chinese word segmentation and named entity recognition. Experiments show that multi-task domain adaptation works better than disjoint domain adaptation for each task, and achieves the state-of-the-art results for both tasks in the social media domain.", "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context.\n\nWord embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense wor d embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art monolingual model trained on five times more training data.", "DocTag2Vec: An Embedding Based Multi-label Learning Approach for Document Tagging.\n\nTagging news articles or blog posts with relevant tags from a collection of predefined ones is coined as document tagging in this work. Accurate tagging of articles can benefit several downstream applications such as recommendation and search. In this work, we propose a novel yet simple approach called DocTag2Vec to accomplish this task. We substantially extend Word2Vec and Doc2Vec -- two popular models for learning  distributed representation of words and documents. In DocTag2Vec, we simultaneously learn the representation of words, documents, and tags in a joint vector space during training, and employ the simple k-nearest neighbor search to predict tags for unseen documents. In contrast to previous multi-label learning methods, DocTag2Vec directly deals with raw text instead of provided feature vector, and in addition, enjoys advantages like the learning of tag representation, and the ability of handling newly created tags. To demonstrate the effectiveness of our approach, we conduct experiments on several datasets and show promising results against state-of-the-art methods.", "Binary Paragraph Vectors.\n\nRecently Le \\& Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.", "Representing Compositionality based on Multiple Timescales Gated Recurrent Neural Networks with Adaptive Temporal Hierarchy for Character-Level Language Models.\n\nA novel character-level neural language model is proposed in this paper. The proposed model incorporates a biologically inspired temporal hierarchy in the architecture for representing multiple compositions of language in order to handle longer sequences for the character-level language model. The temporal hierarchy is introduced in the language model by utilizing a Gated Recurrent Neural Network with multiple timescales. The proposed model incorporates a timescale adaptation mechanism for enhancing the performance of the language model. We evaluate our proposed model using the popular Penn Treebank and Text8 corpora. The experiments show that the use of multiple timescales in a Neural Language Model (NLM) enables improved performance despite having fewer parameters and with no additional computation requirements. Our experiments also demonstrate the ability of the adaptive temporal hierarchies to represent multiple compositonality without the help of complex hierarchical architectures and shows that better representation of the longer sequences lead to enhanced performance of the probabilistic language model.", "Learning Bilingual Projections of Embeddings for Vocabulary Expansion in Machine Translation.\n\nWe propose a simple log-bilinear softmax-based model to deal with vocabulary expansion in machine translation. Our model uses word embeddings trained on significantly large unlabelled monolingual corpora and learns over a fairly small, word-to-word bilingual dictionary. Given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language using the trained bilingual embeddings. We integrate these translation options into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the English--Spanish language pair. When tested over an out-of-domain testset, we get a significant improvement of 3.9 BLEU points.", "Learning to Compose Words into Sentences with Reinforcement Learning.\n\nWe use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models, in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.", "Prediction of Frame-to-Frame Relations in the FrameNet Hierarchy with Frame Embeddings.\n\nAutomatic completion of frame-to-frame (F2F) relations in the FrameNet (FN) hierarchy has received little attention, although they incorporate meta-level commonsense knowledge and are used in downstream approaches. We address the problem of sparsely annotated F2F relations. First, we examine whether the manually defined F2F relations emerge from text by learning text-based frame embeddings. Our analysis reveals insights about the difficulty of reconstructing F2F relations purely from text. Second, we present different systems for predicting F2F relations; our best-performing one uses the FN hierarchy to train on and to ground embeddings in. A comparison of systems and embeddings exposes the crucial influence of knowledge-based embeddings to a system's performance in predicting F2F relations.", "Learning Joint Multilingual Sentence Representations with Neural Machine Translation.\n\nIn this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics.  We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax.  These relations also hold when comparing sentences in different languages.", "Transfer Learning for Speech Recognition on a Budget.\n\nEnd-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.", "Gradual Learning of Matrix-Space Models of Language for Sentiment Analysis.\n\nLearning word representations to capture the semantics and compositionality of language has received much research interest in natural language processing. Beyond the popular vector space models, matrix representations for words have been proposed, since then, matrix multiplication can serve as natural composition operation. In this work, we investigate the problem of learning matrix representations of words. We present a learning approach for compositional matrix-space models for the task of sentiment analysis. We show that our approach, which learns the matrices gradually in two steps, outperforms other approaches and a gradient-descent baseline in terms of quality and computational cost.", "Improving Language Modeling using Densely Connected Recurrent Neural Networks.\n\nIn this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2- layer LSTM model trained with dropout (Zaremba et al., 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.", "NewsQA: A Machine Comprehension Dataset.\n\nWe present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text in the articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. Analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (13.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available online.", "Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in Twitter Streams.\n\nLanguage in social media is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging, disappearing, and changing their meaning. These changes can be estimated using word representations in context, over time and across locations. A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations. Previous work largely focused on qualitative evaluation, which we improve by proposing a set of visualizations that highlight changes in text representation over both space and time. We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the corpus of tweets collected from European countries over a two-week period centered around the terrorist attacks in Brussels in March 2016. In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task -- event type prediction. Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quantitative) evaluation of text representations for spatiotemporal trends.", "Rethinking Skip-thought: A Neighborhood based Approach.\n\nWe study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our model didn't aid our model to perform better, while it hurts the performance of the skip-thought model.", "A Frame Tracking Model for Memory-Enhanced Dialogue Systems.\n\nRecently, resources and tasks were proposed to go beyond state tracking in dialogue systems. An example is the frame tracking task, which requires recording multiple frames, one for each user goal set during the dialogue. This allows a user, for instance, to compare items corresponding to different goals. This paper proposes a model which takes as input the list of frames created so far during the dialogue, the current user utterance as well as the dialogue acts, slot types, and slot values associated with this utterance. The model then outputs the frame being referenced by each  triple of dialogue act, slot type, and slot value. We show that on the recently published Frames dataset, this model significantly outperforms a previously proposed rule-based baseline. In addition, we propose an extensive analysis of the frame tracking task by dividing it into sub-tasks and assessing their difficulty with respect to our model.", "Sense Contextualization in a Dependency-Based Compositional Distributional Model.\n\nLittle attention has been paid to distributional compositional methods which employ syntactically structured vector models. As word vectors belonging to different syntactic categories                                      have incompatible syntactic distributions, no trivial compositional operation can be applied to combine them into a new compositional vector. In this article, we generalize the method described by Erk and Pad\u00f3 (2009) by proposing a dependency-base framework that contextualize not only lemmas but also selectional preferences.  The main contribution of the article is to expand their model to a fully compositional framework in which syntactic dependencies are put at the core of semantic composition. We claim that semantic composition is mainly driven by syntactic dependencies. Each syntactic dependency generates two new compositional vectors representing the contextualized sense of the two related lemmas.  The sequential application of the compositional operations associated to the dependencies results in as many contextualized vectors as lemmas the composite expression contains. At the end of the semantic process, we do not obtain a single compositional vector representing the semantic denotation of the whole composite expression, but one contextualized vector for each lemma of the whole expression. Our method avoids the troublesome high-order tensor representations by defining lemmas and selectional restrictions as first-order tensors (i.e. standard vectors). A corpus-based experiment is performed to both evaluate the quality of the compositional vectors built with our strategy, and to compare them to other approaches on distributional compositional semantics. The experiments show that our dependency-based compositional method performs as  (or even better than) the state-of-the-art.", "Plan, Attend, Generate: Character-Level Neural Machine Translation with Planning.\n\nWe investigate the integration of a planning mechanism into an encoder-decoder architecture with attention. We develop a model that can plan ahead when it computes alignments between the source and target sequences not only for a single time-step but for the next k time-steps as well by constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by strategic attentive reader and writer (STRAW) model, a recent neural architecture for planning with hierarchical reinforcement learning that can also learn higher level temporal abstractions. Our proposed model is end-to-end trainable with differentiable operations. We show that our model outperforms strong baselines on character-level translation task from WMT'15 with fewer parameters and computes alignments that are qualitatively intuitive.", "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology-Based Representations.\n\nWe investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like tf-idf, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.", "Adversarial Generation of Natural Language.\n\nGenerative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language  with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.", "Deep Active Learning for Named Entity Recognition.\n\nDeep neural networks have advanced the state of the art in named entity recognition. However, under typical training procedures, advantages over classical methods emerge only with large datasets. As a result,  deep learning is employed only when large public datasets or a large budget for manually labeling data is available. In this work, we show otherwise: by combining deep learning with active learning, we can outperform classical methods even with a significantly smaller amount of training data.", "The Coadaptation Problem when Learning How and What to Compose.\n\nThis paper discusses a potential problem with tree-sequence models that induce syntax, in that there exists a failure mode when naively treating the relationship between composition and parsing. The first section presents a tree-sequence model that induces syntax. The second section proposes a strategy that attempts to prevent coadaptation between composition and parsing. The third section covers a method for randomly sampling binary trees in a transition-based setting, a simple and useful technique for transition-based parsing models.", "Learning to Embed Words in Context for Syntactic Tasks.\n\nWe present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data.  We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.", "Learning when to skim and when to read.\n\nMany recent advances in deep learning for natural language processing have come at increasing computational cost, but the power of these state-of-the-art models is not needed for every example in a dataset. We demonstrate two approaches to reducing unnecessary computation in cases where a fast but weak baseline classier and a stronger, slower model are both available. Applying an AUC-based metric to the task of sentiment classification, we find significant efficiency gains with both a probability-threshold method for reducing computational cost and one that uses a secondary decision network.", "Context encoders as a simple but powerful extension of word2vec.\n\nWith a strikingly simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings and, additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for words with multiple meanings can be created based on the words' local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.", "Communication with Robots using Multilayer Recurrent Networks.\n\nIn this paper, we describe an improvement on the task of giving instructions to robots in a simulated block world using unrestricted natural language commands.", "Grounding Symbols in Multi-Modal Instructions.\n\nAs robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability---for instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users' contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal input---i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations---to produce the segmentation of objects with a correspondent association to high-level concepts. To test our framework we present experiments in a table-top object manipulation scenario. Our results show our model learns the user's notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.", "Exploring Variation of Natural Human Commands to a Robot in a Collaborative Navigation Task.\n\nRobot-directed communication is variable, and may change based on human perception of robot capabilities. To collect training data for a dialogue system and to investigate possible communication changes over time, we developed a Wizard-of-Oz study that (a) simulates a robot's limited understanding, and (b) collects dialogues where human participants build a progressively better mental model of the robot's understanding. With ten participants, we collected ten hours of human-robot dialogue. We analyzed the structure of instructions that participants gave to a remote robot before it responded. Our findings show a general initial preference for including metric information (e.g., move forward 3 feet) over landmarks (e.g., move to the desk) in motion commands, but this decreased over time, suggesting changes in perception.", "A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented Instructions.\n\nRobots operating alongside humans in diverse, stochastic environments must be able to accurately interpret natural language commands. These instructions often fall into one of two categories: those that specify a goal condition or target state, and those that specify explicit actions, or how to perform a given task. Recent approaches have used reward functions as a semantic representation of goal-based commands, which allows for the use of a state-of-the-art planner to find a policy for the given task. However, these reward functions cannot be directly used to represent action-oriented commands. We introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding Network (DRAGGN), for task grounding and execution that handles natural language from either category as input, and generalizes to unseen environments. Our robot-simulation results demonstrate that a system successfully interpreting both goal-oriented and action-oriented task specifications brings us closer to robust natural language understanding for human-robot interaction.", "Grounding Language for Interactive Task Learning.\n\nThis paper describes how language is grounded by a comprehension system called Lucia within a robotic agent called Rosie that can manipulate objects and navigate indoors. The whole system is built within the Soar cognitive architecture and uses Embodied Construction Grammar (ECG) as a formalism for describing linguistic knowledge. Grounding is performed using knowledge from the grammar itself, from the linguistic context, from the agents perception, and from an ontology of long-term knowledge about object categories and properties and actions the agent can perform. The paper also describes a benchmark corpus of 200 sentences in this domain along with test versions of the world model and ontology and gold-standard meanings for each of the sentences. The benchmark is contained in the supplemental materials.", "Are Distributional Representations Ready for the Real World? Evaluating Word Vectors for Grounded Perceptual Meaning.\n\nDistributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.", "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction.\n\nRecognition of social signals, coming from human facial expressions or prosody of human speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users' impressions of the robot after a conversation. We find that happiness in the user's recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that the facial expression emotional features and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learning-based adaptive human-robot dialogue systems.", "Towards Problem Solving Agents that Communicate and Learn.\n\nAgents that communicate back and forth with humans to help them execute non-linguistic tasks are a long sought goal of AI. These agents need to translate between utterances and actionable meaning representations that can be interpreted by task-specific problem solvers in a context-dependent manner. They should also be able to learn such actionable interpretations for new predicates on the fly. We define an agent architecture for this scenario and present a series of experiments in the Blocks World domain that illustrate how our architecture supports language learning and problem solving in this domain.", "Learning how to Learn: An Adaptive Dialogue Agent for Incrementally Learning Visually Grounded Word Meanings.\n\nWe present an optimised multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human tutor, trained on real human-human tutoring data. Within a life-long interactive learning period, the agent, trained using Reinforcement Learning (RL), must be able to handle natural conversations with human users, and achieve  good learning performance (i.e. accuracy) while minimising human effort in the learning process. We train and evaluate this  system in interaction with a simulated human tutor, which is built on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual learning task. The results show that: 1) The learned policy can coherently interact with the simulated user to achieve the goal of the task (i.e. learning visual attributes of  objects, e.g. colour and shape); and 2) it finds a better trade-off between  classifier accuracy and tutoring costs than hand-crafted rule-based policies, including ones with dynamic policies.", "Guiding Interaction Behaviors for Multi-modal Grounded Language Learning.\n\nMulti-modal grounded language learning connects language predicates to physical properties of objects in the world. Sensing with multiple modalities, such as audio, haptics, and visual colors and shapes while performing interaction behaviors like lifting, dropping, and looking on objects enables a robot to ground non-visual predicates like ``empty'' as well as visual predicates like ``red''. Previous work has established that grounding in multi-modal space improves performance on object retrieval from human descriptions. In this work, we gather behavior annotations from humans and demonstrate that these improve language grounding performance by allowing a system to focus on relevant behaviors for words like ``white'' or ``half-full'' that can be understood by looking or lifting, respectively. We also explore adding modality annotations (whether to focus on audio or haptics when performing a behavior), which improves performance, and sharing information between linguistically related predicates (if ``green'' is a color, ``white'' is a color), which improves grounding recall but at the cost of precision.", "Structured Learning for Context-aware Spoken Language Understanding of Robotic Commands.\n\nService robots are expected to operate in specific environments, where the presence of humans plays a key role. A major feature of such robotics platforms is thus the ability to react to spoken commands. This requires the understanding of the user utterance with an accuracy able to trigger the robot reaction. Such correct interpretation of linguistic exchanges depends on physical, cognitive and language-dependent aspects related to the environment. In this work, we present the empirical evaluation of an adaptive Spoken Language Understanding chain for robotic commands, that explicitly depends on the operational environment during both the learning and recognition stages. The effectiveness of such a context-sensitive command interpretation is tested against an extension of an already existing corpus of commands, that introduced explicit perceptual knowledge: this enabled deeper measures proving that more accurate disambiguation capabilities can be actually obtained.", "Natural Language Grounding and Grammar Induction for Robotic Manipulation Commands.\n\nWe present a cognitively plausible system capable of acquiring knowledge in language and vision from pairs of short video clips and linguistic descriptions. The aim of this work is to teach a robot manipulator how to execute natural language commands by demonstration. This is achieved by first learning a set of visual `concepts' that abstract the visual feature spaces into concepts that have human-level meaning. Second, learning the mapping/grounding between words and the extracted visual concepts. Third, inducing grammar rules via a semantic representation known as Robot Control Language (RCL). We evaluate our approach against state-of-the-art supervised and unsupervised grounding and grammar induction systems, and show that a robot can learn to execute never seen-before commands from pairs of unlabelled linguistic and visual inputs.", "Duluth at SemEval-2017 Task 7 : Puns Upon a Midnight Dreary, Lexical Semantics for the Weak and Weary.\n\nThis paper describes the Duluth systems that participated in SemEval-2017 Task 7 : Detection and Interpretation of English Puns. The Duluth systems participated in all three subtasks, and relied on methods that included word sense disambiguation and measures of semantic relatedness.", "HCCL at SemEval-2017 Task 2: Combining Multilingual Word Embeddings and Transliteration Model for Semantic Similarity.\n\nIn this paper, we introduce an approach to combining word embeddings and machine translation for multilingual semantic word similarity, the task2 of SemEval-2017. Thanks to the unsupervised transliteration model, our cross-lingual word embeddings encounter decreased sums of OOVs. Our results are produced using only monolingual Wikipedia corpora and a limited amount of sentence-aligned data. Although relatively little resources are utilized, our system ranked 3rd in the monolingual subtask and can be the 6th in the cross-lingual subtask.", "TwiSe at SemEval-2017 Task 4: Five-point Twitter Sentiment Classification and Quantification.\n\nThe paper describes the participation of the team ``TwiSE'' in the SemEval-2017 challenge. Specifically, I participated at Task 4 entitled ``Sentiment Analysis in Twitter'' for which I implemented systems for five-point tweet classification (Subtask C) and five-point tweet quantification (Subtask E) for English tweets. In the feature extraction steps the systems rely on the vector space model, morpho-syntactic analysis of the tweets and several sentiment lexicons. The classification step of Subtask C uses a Logistic Regression trained with the one-versus-rest approach. Another instance of Logistic Regression combined with the classify-and-count approach is trained for the quantification task of Subtask E. In the official leaderboard the system is ranked \\textit{5/15} in Subtask C and \\textit{2/12} in Subtask E.", "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing.\n\nWe evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.", "RUFINO at SemEval-2017 Task 2: Cross-lingual lexical similarity by extending PMI and word embeddings systems with a Swadesh's-like list.\n\nThe RUFINO team proposed a non-supervised, conceptually-simple and low-cost approach for addressing the Multilingual and Cross-lingual Semantic Word Similarity challenge at SemEval 2017. The proposed systems were cross-lingual extensions of popular monolingual lexical similarity approaches such as PMI and word2vec. The extensions were possible by means of a small parallel list of concepts similar to the Swadesh's list, which we obtained in a semi-automatic way. In spite of its simplicity, our approach showed to be effective obtaining statistically-significant and consistent results in all datasets proposed for the task. Besides, we provide some research directions for improving this novel and affordable approach.", "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines.\n\nThis paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6\\% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.", "LIA at SemEval-2017 Task 4: An Ensemble of Neural Networks for Sentiment Classification.\n\nThis paper describes the system developed at LIA for the SemEval-2017 evaluation campaign. The goal of Task 4.A was to identify sentiment polarity in tweets. The system is an ensemble of Deep Neural Network (DNN) models: Convolutional Neural Network (CNN) and Recurrent Neural Network Long Short-Term Memory (RNN-LSTM). We initialize the input representation of DNN with different sets of embeddings trained on large datasets. The ensemble of DNNs are combined using a score-level fusion approach. The system ranked 2nd at SemEval-2017 and obtained an average recall of 67.6%.", "SwissAlps at SemEval-2017 Task 3: Attention-based Convolutional Neural Network for Community Question Answering.\n\nIn this paper we propose a system for reranking answers for a given question. Our method builds on a siamese CNN architecture which is extended by two attention mechanisms. The approach was evaluated on the datasets of the SemEval-2017 competition for Community Question Answering (cQA), where it achieved 7th place obtaining a MAP score of 86:24 points on the Question-Comment Similarity subtask.", "SEF@UHH at SemEval-2017 Task 1: Unsupervised Knowledge-Free Semantic Textual Similarity via Paragraph Vector.\n\nThis paper describes our unsupervised knowledge-free approach to the SemEval-2017 Task 1 Competition. The proposed method makes use of Paragraph Vector for assessing the semantic similarity between pairs of sentences. We experimented with various dimensions of the vector and three state-of-the-art similarity metrics. Given a cross-lingual task, we trained models corresponding to its two languages and combined the models by averaging the similarity scores. The results of our submitted runs are above the median scores for five out of seven test sets by means of Pearson Correlation. Moreover, one of our system runs performed best on the Spanish-English-WMT test set ranking first out of 53 runs submitted in total by all participants.", "TopicThunder at SemEval-2017 Task 4: Sentiment Classification Using a Convolutional Neural Network with Distant Supervision.\n\nIn this paper, we propose a classifier for predicting topic-specific sentiments of English Twitter messages. Our method is based on a 2-layer CNN.With a distant supervised phase we leverage a large amount of weakly-labelled training data. Our system was evaluated on the data provided by the SemEval-2017 competition in the Topic-Based Message Polarity Classification subtask, where it ranked 4th place.", "Know-Center at SemEval-2017 Task 10: Sequence Classification with the CODE Annotator.\n\nThis paper describes our participation in SemEval-2017 Task 10. We competed in Subtask 1 and 2 which consist respectively in identifying all the key phrases in scientific publications and label them with one of the three categories: Task, Process, and Material. These scientific publications are selected from Computer Science, Material Sciences, and Physics domains. We followed a supervised approach for both subtasks by using a sequential classifier (CRF - Conditional Random Fields). For generating our solution we used a web-based application implemented in the EU-funded research project, named CODE. Our system achieved an F1 score of 0.39 for the Subtask 1 and 0.28 for the Subtask 2.", "Hitachi at SemEval-2017 Task 12: System for temporal information extraction from clinical notes.\n\nThis paper describes the system developed for the task of temporal information extraction from clinical narratives in the context of the 2017 Clinical TempEval challenge. Clinical TempEval 2017 addressed the problem of temporal reasoning in the clinical domain by providing annotated clinical notes, pathology and radiology reports in line with Clinical TempEval challenges 2015/16, across two different evaluation phases focusing on cross domain adaptation. Our team focused on subtasks involving extractions of temporal spans and relations for which the developed systems showed average F-score of 0.45 and 0.47 across the two phases of evaluations.", "INGEOTEC at SemEval 2017 Task 4: A B4MSA Ensemble based on Genetic Programming for Twitter Sentiment Analysis.\n\nThis paper describes the system used in SemEval-2017 Task 4 (Subtask A): Message Polarity Classification for both English and Arabic languages. Our proposed system is an ensemble of two layers, the first one uses our generic framework for multilingual polarity classification (B4MSA) and the second layer combines all the decision function values predicted by B4MSA systems using a non-linear function evolved using a Genetic Programming system, EvoDAG. With this approach, the best performances reached by our system were macro-recall 0.68 (English) and 0.477 (Arabic) which set us in sixth and fourth positions in the results table, respectively.", "UWAV at SemEval-2017 Task 7: Automated feature-based system for locating puns.\n\nIn this paper we describe our system created for SemEval-2017 Task 7: Detection and Interpretation of English Puns. We tackle subtask 1, pun detection, by leveraging features selected from sentences to design a classifier that can disambiguate between the presence or absence of a pun. We address subtask 2, pun location, by utilizing a decision flow structure that uses presence or absence of certain features to decide the next action. The results obtained by our system are encouraging, considering the simplicity of the system. We consider this system as a precursor for deeper exploration on efficient feature selection for pun detection.", "Idiom Savant at Semeval-2017 Task 7: Detection and Interpretation of English Puns.\n\nThis paper describes our system, entitled Idiom Savant, for the 7th Task of the Se- meval 2017 workshop, ``Detection and in- terpretation of English Puns''. Our sys- tem consists of two probabilistic models for each type of puns using Google n- gram and Word2Vec. Our system achieved f-score of calculating, 0.663, and 0.07 in homographic puns and 0.8439, 0.6631, and 0.0806 in heterographic puns in task 1, task 2, and task 3 respectively.", "MERALI at SemEval-2017 Task 2 Subtask 1: a Cognitively Inspired approach.\n\nIn this paper we report on the participation of the MERALI system to the SemEval Task 2 Subtask 1. The MERALI system approaches conceptual similarity through a simple, cognitively inspired, heuristics; it builds on a linguistic resource, the TTCS-e, that relies on BabelNet, NASARI and ConceptNet. The linguistic resource in fact contains a novel mixture of common-sense and encyclopedic knowledge. The obtained results point out that there is ample room for improvement, so that they are used to elaborate on present limitations and on future steps.", "funSentiment at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs  Using Word  Vectors Built from StockTwits and Twitter.\n\nThis paper describes the approach we used for SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs. We use three types of word embeddings in our algorithm: word embeddings learned from 200 million tweets, sentiment-specific word embeddings learned from 10 million tweets using distance supervision, and word embeddings learned from 20 million StockTwits messages.  In our approach, we also take the left and right context of the target company into consideration when generating polarity prediction features. All the features generated from different word embeddings and contexts are integrated together to train our algorithm", "SentiHeros at SemEval-2017 Task 5: An application of Sentiment Analysis on Financial Tweets.\n\nSentiment analysis is the process of identifying the opinion expressed in text. Recently it has been used to study behavioral finance, and in particular the effect of opinions and emotions on economic or financial decisions. SemEval-2017 task 5 focuses on the financial market as the do- main for sentiment analysis of text; specifically, task 5, subtask 1 focuses on financial tweets about stock symbols. In this paper, we describe a machine learning classifier for binary classification of financial tweets. We used natural language processing techniques and the random forest algorithm to train our model, and tuned it for the training dataset of Task 5, subtask 1. Our system achieves the 7th rank on the leaderboard of the task.", "LIMSI-COT at SemEval-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives.\n\nIn this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks.", "DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison.\n\nIn this paper we present a deep-learning system that competed at SemEval-2017 Task 6 ``\\#HashtagWars: Learning a Sense of Humor''. We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-the-art results on \\#HashtagWars dataset.", "BUSEM at SemEval-2017 Task 4A Sentiment Analysis with Word Embedding and Long Short Term Memory RNN Approaches.\n\nThis paper describes our approach for SemEval-2017 Task 4: Sentiment Analysis in Twitter. We have participated in Subtask A: Message Polarity Classification subtask and  developed two systems. The first system  uses  word embeddings for feature representation and Support Vector Machine, Random Forest and Naive Bayes algorithms for classification of Twitter messages into negative, neutral and positive polarity. The second system is based on Long Short Term Memory Recurrent Neural Networks and uses word indexes as sequence of inputs for feature representation.", "deepSA at SemEval-2017 Task 4: Interpolated Deep Neural Networks for Sentiment Analysis in Twitter.\n\nIn this paper, we describe our system implementation for sentiment analysis in Twitter. This system combines two models based on deep neural networks, namely a convolutional neural network (CNN) and a long short-term memory (LSTM) recurrent neural network, through interpolation. Distributed representation of words as vectors are input to the system, and the output is a sentiment class. The neural network models are trained exclusively with the data sets provided by the organizers of SemEval-2017 Task 4 Subtask A. Overall, this system has achieved 0.618 for the average recall rate, 0.587 for the average F1 score, and 0.618 for accuracy.", "TakeLab at SemEval-2017 Task 4: Recent Deaths and the Power of Nostalgia in Sentiment Analysis in Twitter.\n\nThis paper describes the system we submitted to SemEval-2017 Task 4 (Sentiment Analysis in Twitter), specifically subtasks A, B, and D. Our main focus was topic-based message polarity classification on a two-point scale (subtask B). The system we submitted uses a Support Vector Machine classifier with rich set of features, ranging from standard to more creative, task-specific features, including a series of rating-based features as well as features that account for sentimental reminiscence of past topics and deceased famous people. Our system ranked 14th out of 39 submissions in subtask A, 5th out of 24 submissions in subtask B, and 3rd out of 16 submissions in subtask D.", "Sheffield at SemEval-2017 Task 9: Transition-based language generation from AMR..\n\nThis paper describes the submission by the University of Sheffield to the SemEval 2017 Abstract Meaning Representation Parsing and Generation task (SemEval 2017 Task 9, Subtask 2). We cast language generation from AMR as a sequence of actions (e.g., insert/remove/rename edges and nodes) that progressively transform the AMR  graph into a dependency parse tree. This transition-based approach relies on the fact that an AMR graph can be considered structurally similar to a dependency tree, with a focus on content rather than function words. An added benefit to this approach is the greater amount of data we can take advantage of to train the parse-to-text linearizer. Our submitted run on the test data achieved a BLEU score of 3.32 and a Trueskill score of -22.04 on automatic and human evaluation respectively.", "TakeLab at SemEval-2017 Task 6: \\#RankingHumorIn4Pages.\n\nThis paper describes our system for humor ranking in tweets within the SemEval 2017 Task 6: \\\\#HashtagWars (6A and 6B). For both subtasks, we use an off-the-shelf gradient boosting model built on a rich set of features, handcrafted to provide the model with the external knowledge needed to better predict the humor in the text. The features capture various cultural references and specific humor patterns. Our system ranked 2nd (officially 7th) among 10 submissions on the Subtask A and 2nd among 9 submissions on the Subtask B.", "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge.\n\nThis paper describes Luminoso's participation in SemEval 2017 Task 2, ``Multilingual and Cross-lingual Semantic Word Similarity'', with a system based on ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses on general knowledge that relates the meanings of words and phrases. Our submission to SemEval was an update of previous work that builds high-quality, multilingual word embeddings from a combination of ConceptNet and distributional semantics. Our system took first place in both subtasks. It ranked first in 4 out of 5 of the separate languages, and also ranked first in all 10 of the cross-lingual language pairs.", "NTNU-2 at SemEval-2017 Task 10: Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents.\n\nThis paper presents our relation extraction system for subtask C of SemEval-2017 Task 10: ScienceIE. Assuming that the keyphrases are already annotated in the input data, our work explores a  wide range of linguistic features, applies various feature selection techniques, optimizes the hyper parameters and class weights and experiments with different problem formulations (single classification model vs individual classifiers for each keyphrase type, single-step classifier vs pipeline classifier for hyponym relations). Performance of five popular classification algorithms are evaluated for each problem formulation along with feature selection. The best setting achieved an F1 score of 71.0% for synonym and 30.0% for hyponym relation on the test data.", "HHU at SemEval-2017 Task 2: Fast Hash-Based Embeddings for Semantic Word Similarity Assessment.\n\nThis paper describes the HHU system that participated in Task 2 of SemEval 2017, Multilingual and Cross-lingual Semantic Word Similarity. We introduce our unsupervised embedding learning technique and describe how it was employed and configured to address the problems of monolingual and multilingual word similarity measurement. This paper reports from empirical evaluations on the benchmark provided by the task's organizers.", "TakeLab-QA at SemEval-2017 Task 3: Classification Experiments for Answer Retrieval in Community QA.\n\nIn this paper we present the TakeLab-QA entry to SemEval 2017 task 3, which is a question-comment re-ranking problem. We present a classification based approach, including two supervised learning models --- Support Vector Machines (SVM) and Convolutional Neural Networks (CNN). We use features based on different semantic similarity models (e.g., Latent Dirichlet Allocation), as well as features based on several types of pre-trained word embeddings. Moreover, we also use some hand-crafted task-specific features. For training, our system uses no external labeled data apart from that provided by the organizers. Our primary submission achieves a MAP-score of 81.14 and F1-score of 66.99 --- ranking us 10th on the SemEval 2017 task 3, subtask A.", "STS-UHH at SemEval-2017 Task 1: Scoring Semantic Textual Similarity Using Supervised and Unsupervised Ensemble.\n\nThis paper reports the STS-UHH participation in the SemEval 2017 shared Task 1 of Semantic Textual Similarity (STS). Overall, we submitted 3 runs covering monolingual and cross-lingual STS tracks. Our participation involves two approaches: unsupervised approach, which estimates a word alignment-based similarity score, and supervised approach, which combines dependency graph similarity and coverage features with lexical similarity measures using regression methods. We also present a way on ensembling both models. Out of 84 submitted runs, our team best multi-lingual run has been ranked 12th in overall performance with correlation of 0.61, 7th among 31 participating teams.", "GUIR at SemEval-2017 Task 12: A Framework for Cross-Domain Clinical Temporal Information Extraction.\n\nClinical TempEval 2017 (SemEval 2017 Task 12) addresses the task of cross-domain temporal extraction from clinical text. We present a system for this task that uses supervised learning for the extraction of temporal expression and event spans with corresponding attributes and narrative container relations. Approaches include conditional random fields and decision tree ensembles, using lexical, syntactic, semantic, distributional, and rule-based features. Our system received best or second best scores in TIMEX3 span, EVENT span, and CONTAINS relation extraction.", "DUTH at SemEval-2017 Task 5: Sentiment Predictability in Financial Microblogging and News Articles.\n\nWe present the system developed by the team DUTH for the participation in Semeval-2017 task 5 - Fine-Grained Sentiment Analysis on Financial Microblogs and News, in subtasks A and B. Our approach to determine the sentiment of Microblog Messages and News Statements \\& Headlines is based on linguistic preprocessing, feature engineering, and supervised machine learning techniques. To train our model, we used Neural Network Regression, Linear Regression, Boosted Decision Tree Regression and Decision Forrest Regression classifiers to forecast sentiment scores. At the end, we present an error measure, so as to improve the performance about forecasting methods of the system.", "NNEMBs at SemEval-2017 Task 4: Neural Twitter Sentiment Classification: a Simple Ensemble Method with Different Embeddings.\n\nRecently, neural twitter sentiment classification has become one of state-of-thearts, which relies less feature engineering work compared with traditional methods. In this paper, we propose a simple and effective ensemble method to further boost the performances of neural models. We collect several word embedding sets which are publicly released (often are learned on different corpus) or constructed by running Skip-gram on released large-scale corpus. We make an assumption that different word embeddings cover different words and encode different semantic knowledge, thus using them together can improve the generalizations and performances of neural models. In the SemEval 2017, our method ranks 1st in Accuracy, 5th in AverageR. Meanwhile, the additional comparisons demonstrate the superiority of our model over these ones based on only one word embedding set. We release our code for the method duplicability.", "TakeLab at SemEval-2017 Task 5: Linear aggregation of word embeddings for fine-grained sentiment analysis of financial news.\n\nThis paper describes our system for fine-grained sentiment scoring of news headlines submitted to SemEval 2017 task 5--subtask 2. Our system uses a feature-light method that consists of a Support Vector Regression (SVR) with various kernels and word vectors as features. Our best-performing submission scored 3rd on the task out of 29 teams and 4th out of 45 submissions with a cosine score of 0.733.", "NileTMRG at SemEval-2017 Task 8: Determining Rumour and Veracity Support for Rumours on Twitter..\n\nFinal submission for NileTMRG on RumourEval 2017.", "LABDA at SemEval-2017 Task 10: Relation Classification between keyphrases via Convolutional Neural Network.\n\nIn this paper, we describe our participation at the subtask of extraction of relationships between two identified keyphrases. This task can be very helpful in improving search engines for scientific articles. Our approach is based on the use of a convolutional neural network (CNN) trained on the training dataset. This deep learning model has already achieved successful results for the extraction relationships between named entities. Thus, our hypothesis is that this model can be also applied to extract relations between keyphrases. The official results of the task show that our architecture obtained an F1-score of 0.38% for Keyphrases Relation Classification. This performance is lower than the expected due to the generic preprocessing phase and the basic configuration of the CNN model, more complex architectures are proposed as future work to increase the classification rate.", "NileTMRG at SemEval-2017 Task 4: Arabic Sentiment Analysis.\n\nThis paper describes two systems that were used by the NileTMRG for addressing Arabic Sentiment Analysis as part of SemEval-2017, task 4. NileTMRG participated in three Arabic related subtasks which are: Subtask A (Message Polarity Classification), Subtask B (Topic-Based Message Polarity classification) and Subtask D  (Tweet quantification). For subtask A, we made use of NU's sentiment analyzer which we augmented with a scored lexicon. For subtasks B and D, we used an ensemble of  three different classifiers. The first classifier was a convolutional neural network that used trained (word2vec) word embeddings. The second classifier consisted of a  MultiLayer Perceptron  while the third classifier was a Logistic regression model that takes the same input as the second classifier. Voting between the three classifiers was used to determine the final outcome.  In all three Arabic related tasks in which NileTMRG participated, the team ranked at number one.", "UMDeep at SemEval-2017 Task 1: End-to-End Shared Weight LSTM Model for Semantic Textual Similarity.\n\nWe describe a modified shared-LSTM network for the Semantic Textual Similarity (STS) task at SemEval-2017. The network builds on previously explored Siamese network architectures. We treat max sentence length as an additional hyperparameter to be tuned (beyond learning rate, regularization, and dropout). Our results demonstrate that hand-tuning max sentence training length significantly improves final accuracy. After optimizing hyperparameters, we train the network on the multilingual semantic similarity task using pre-translated sentences. We achieved a correlation of 0.4792 for all the subtasks.  We achieved the fourth highest team correlation for Task 4b, which was our best relative placement.", "WING-NUS at SemEval-2017 Task 10: Keyphrase Extraction and Classification as Joint Sequence Labeling.\n\nWe describe an end-to-end pipeline processing approach for SemEval 2017's Task 10 to extract keyphrases and their relations from scientific publications.  We jointly identify and classify keyphrases by modeling the subtasks as sequential labeling.  Our system utilizes standard, surface-level features along with the adjacent word features, and performs conditional decoding on whole text to extract keyphrases. We focus only on the identification and typing of keyphrases (Subtasks A and B, together referred as extraction), but provide an end-to-end system inclusive of keyphrase relation identification (Subtask C) for completeness.  Our top performing configuration achieves an $F\\_1$ of 0.27 for the end-to-end keyphrase extraction and relation identification scenario on the final test data, and compares on par to other top ranked systems for keyphrase extraction.  Our system outperforms other techniques that do not employ global decoding and hence do not account for dependencies between keyphrases. We believe this is crucial for keyphrase classification in the given context of scientific document mining.", "UW-FinSent at SemEval-2017 Task 5: Sentiment Analysis on Financial News Headlines using Training Dataset Augmentation.\n\nThis paper discusses the approach taken by the UWaterloo team to arrive at a solution for the Fine-Grained Sentiment Analysis problem posed by Task 5 of SemEval 2017. The paper describes the document vectorization and sentiment score prediction techniques used, as well as the design and implementation decisions taken while building the system for this task. The system uses text vectorization models, such as N-gram, TF-IDF and paragraph embeddings, coupled with regression model variants to predict the sentiment scores. Amongst the methods examined, unigrams and bigrams coupled with simple linear regression obtained the best baseline accuracy. The paper also explores data augmentation methods to supplement the training dataset. This system was designed for Subtask 2 (News Statements and Headlines).", "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM.\n\nThis paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A.", "PKU\\_ICL at SemEval-2017 Task 10: Keyphrase Extraction with Model Ensemble and External Knowledge.\n\nThis paper presents a system that participated in SemEval 2017 Task 10 (subtask A and subtask B): Extracting Keyphrases and Relations from Scientific Publications (Augenstein et al., 2017). Our proposed approach utilizes external knowledge to enrich feature representation of candidate keyphrase, including Wikipedia, IEEE taxonomy and pre-trained word embeddings etc. Ensemble of unsupervised models, random forest and linear models are used for candidate keyphrase ranking and keyphrase type classification. Our system achieves the 3rd place in subtask A and 4th place in subtask B.", "Mama Edha at SemEval-2017 Task 8: Stance Classification with CNN and Rules.\n\nFor the competition SemEval-2017 we investigated the possibility of performing stance classification (support, deny, query or comment) for messages in Twitter conversation threads related to rumours. Stance classification is interesting since it can provide a basis for rumour veracity assessment. Our ensemble classification approach of combining convolutional neural networks with both automatic rule mining and manually written rules achieved a final accuracy of 74.9% on the competition's test data set for Task 8A. To improve classification we also experimented with data relabeling and using the grammatical structure of the tweet contents for classification.", "YNU-HPCC at SemEval 2017 Task 4: Using A Multi-Channel CNN-LSTM Model for Sentiment Classification.\n\nIn this paper, we propose a multi-channel convolutional neural network-long short-term memory (CNN-LSTM) model that consists of two parts: multi-channel CNN and LSTM to analyze the sentiments of short English messages from Twitter. Un-like a conventional CNN, the proposed model applies a multi-channel strategy that uses several filters of different length to extract active local n-gram features in different scales. This information is then sequentially composed using LSTM. By combining both CNN and LSTM, we can consider both local information within tweets and long-distance dependency across tweets in the classification process. Officially released results show that our system outperforms the baseline algo-rithm.", "MITRE at SemEval-2017 Task 1: Simple Semantic Similarity.\n\nThis paper describes MITRE's participation in the Semantic Textual Similarity task (SemEval-2017 Task 1), which evaluated machine learning approaches to the identification of similar meaning among text snippets in English, Arabic, Spanish, and Turkish. We detail the techniques we explored ranging from simple bag-of-ngrams classifiers to neural architectures with varied attention and alignment mechanisms. Linear regression is used to tie the systems together into an ensemble submitted for evaluation. The resulting system is capable of matching human similarity ratings of image captions with correlations of 0.73 to 0.83 in monolingual settings and 0.68 to 0.78 in cross-lingual conditions, demonstrating the power of relatively simple approaches.", "RiTUAL-UH at SemEval-2017 Task 5: Sentiment Analysis on Financial Data Using Neural Networks.\n\nIn this paper, we present our systems for the ``SemEval-2017 Task-5 on Fine- Grained Sentiment Analysis on Financial Microblogs and News''. In our system, we combined hand-engineered lexical, sentiment and metadata features, the representations learned from Convolutional Neural Networks (CNN) and Bidirectional Gated Recurrent Unit (Bi-GRU) with Attention model applied on top. With this architecture we obtained weighted cosine similarity scores of 0.72 and 0.74 for subtask-1 and subtask-2, respectively. Using the official scoring system, our system ranked the second place for subtask-2 and eighth place for the subtask-1. It ranked first for both of the subtasks by the scores achieved by an alternate scoring system.", "GW\\_QA at SemEval-2017 Task 3: Question Answer Re-ranking on Arabic Fora.\n\nThis paper describes our submission to SemEval-2017 Task 3 Subtask D, ``Question Answer Ranking in Arabic Community Question Answering''. In this work, we applied a supervised machine learning approach to automatically re-rank a set of QA pairs according to their relevance to a given question. We employ features based on latent semantic models, namely WTMF, as well as a set of lexical features based on string lengths and surface level matching. The proposed system ranked first out of 3 submissions, with a MAP score of 61.16\\%.", "NLM\\_NIH at SemEval-2017 Task 3: from Question Entailment to Question Similarity for Community Question Answering.\n\nThis paper describes our participation in SemEval-2017 Task 3 on Community Question  Answering (cQA). The Question Similarity subtask (B) aims to rank  a set of related questions retrieved by a search engine according to their similarity to the original question. We adapted our feature-based system for Recognizing Question Entailment (RQE) to the question similarity task. Tested on cQA-B-2016 test data, our RQE system outperformed the best system of the 2016 challenge in all measures with 77.47 MAP and 80.57 Accuracy. On cQA-B-2017 test data, performances of all systems dropped by around 30 points. Our primary system obtained 44.62 MAP, 67.27 Accuracy and 47.25 F1 score. The cQA-B-2017 best system achieved 47.22 MAP and 42.37 F1 score. Our system is ranked sixth in terms of MAP and third in terms of F1 out of 13 participating teams.", "TSA-INF at SemEval-2017 Task 4: An Ensemble of Deep Learning Architectures Including Lexicon Features for Twitter Sentiment Analysis.\n\nThis paper describes the submission of team TSA-INF to SemEval-2017 Task 4 Subtask A. The submitted system is an ensemble of three varying deep learning architectures for sentiment analysis. The core of the architecture is a convolutional neural network that performs well on text classification as is. The second subsystem is a gated recurrent neural network implementation. Additionally, the third system integrates opinion lexicons directly into a convolution neural network architecture. The resulting ensemble of the three architectures achieved a top ten ranking with a macro-averaged recall of 64.3%. Additional results comparing variations of the submitted system are not conclusive enough to determine a best architecture, but serve as a benchmark for further implementations.", "DFKI-DKT at SemEval-2017 Task 8: Rumour Detection and Classification using Cascading Heuristics.\n\nWe describe our submissions for SemEval-2017 Task~8, Determining Rumour Veracity and Support for Rumours. The Digital Curation Technologies (DKT) team at the German Research Center for Artificial Intelligence (DFKI) participated in two subtasks: Subtask A (determining the stance of a message) and Subtask B (determining veracity of a message, closed variant). In both cases, our implementation consisted of a Multivariate Logistic Regression (Maximum Entropy) classifier coupled with hand-written patterns and rules (heuristics) applied in a post-process cascading fashion. We provide a detailed analysis of the system performance and report on variants of our systems that were not part of the official submission.", "SRHR at SemEval-2017 Task 6: Word Associations for Humour Recognition.\n\nThis paper explores the role of semantic relatedness features, such as word associations, in humour recognition. Specifically, we examine the task of inferring pairwise humour judgments in Twitter hashtag wars. We examine a variety of word association features derived from University of Southern Florida Free Association Norms (USF) and the Edinburgh Associative Thesaurus (EAT) and find that word association-based features outperform Word2Vec similarity, a popular semantic relatedness measure. Our system achieves an accuracy of 56.42% using a combination of unigram perplexity, bigram perplexity, EAT difference (tweet-avg), USF forward (max), EAT difference (word-avg), USF difference (word-avg), EAT forward (min), USF difference (tweet-max), and EAT backward (min).", "CrystalNest at SemEval-2017 Task 4: Using Sarcasm Detection for Enhancing Sentiment Classification and Quantification.\n\nThis paper describes a system developed for a shared sentiment analysis task and its subtasks organized by SemEval-2017. A key feature of our system is the embedded ability to detect sarcasm in order to enhance the performance of sentiment classification. We first constructed an affect-cognition-sociolinguistics sarcasm features model and trained a SVM-based classifier for detecting sarcastic expressions from general tweets. For sentiment prediction, we developed CrystalNest-- a two-level cascade classification system using features combining sarcasm score derived from our sarcasm classifier, sentiment scores from Alchemy, NRC lexicon, n-grams, word embedding vectors, and part-of-speech features. We found that the sarcasm detection derived features consistently benefited key sentiment analysis evaluation metrics, in different degrees, across four subtasks A-D.", "MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional Neural Networks.\n\nOver 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).", "ECNU at SemEval-2017 Task 1: Leverage Kernel-based Traditional NLP features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity.\n\nTo address semantic similarity on multilingual and cross-lingual sentences, we firstly translate other foreign languages into English, and then feed our monolingual English system with various interactive features. Our system is further supported by combining with deep learning semantic similarity and our best run achieves the mean Pearson correlation 73.16\\% in primary track.", "HumorHawk at SemEval-2017 Task 6: Mixing Meaning and Sound for Humor Recognition.\n\nThis paper describes the winning system for SemEval-2017 Task 6: \\#HashtagWars: Learning a Sense of Humor. Humor detection has up until now been predominantly addressed using feature-based approaches. Our system utilizes recurrent deep learning methods with dense embeddings to predict humorous tweets from the @midnight show \\#HashtagWars. In order to include both meaning and sound in the analysis, GloVe embeddings are combined with a novel phonetic representation to serve as input to an LSTM component. The output is combined with a character-based CNN model, and an XGBoost component in an ensemble model which achieves 0.675 accuracy on the evaluation data.", "TTI-COIN at SemEval-2017 Task 10: Investigating Embeddings for End-to-End Relation Extraction from Scientific Papers.\n\nThis paper describes our TTI-COIN system that participated in SemEval-2017 Task 10. We investigated appropriate embeddings to adapt a neural end-to-end entity and relation extraction system LSTM-ER to this task. We participated in the full task setting of the entity segmentation, entity classification and relation classification (scenario 1) and the setting of relation classification only (scenario 3). The system was directly applied to the scenario 1 without modifying the codes thanks to its generality and flexibility. Our evaluation results show that the choice of appropriate pre-trained embeddings affected the performance significantly. With the best embeddings, our system was ranked third in the scenario 1 with the micro F1 score of 0.38. We also confirm that our system can produce the micro F1 score of 0.48 for the scenario 3 on the test data, and this score is close to the score of the 3rd ranked system in the task.", "UCSC-NLP at SemEval-2017 Task 4: Sense n-grams for Sentiment Analysis in Twitter.\n\nThis paper describes the system submitted to SemEval-2017 Task 4-A Sentiment Analysis in Twitter developed by the UCSC-NLP team. We studied how relationships between sense n-grams and sentiment polarities can contribute to this task, i.e. co-occurrences of WordNet senses in the tweet, and the polarity. Furthermore, we evaluated the effect of discarding a large set of features based on char-grams reported in preceding works. Based on these elements, we developed a SVM system, which exploring SentiWordNet as a polarity lexicon. It achieves an $F\\_1=0.624$ of average. Among $39$ submissions to this task, we ranked $10th$.", "COMMIT at SemEval-2017 Task 5: Ontology-based Method for Sentiment Analysis of Financial Headlines.\n\nThis paper describes our submission to Task 5 of SemEval 2017, Fine-Grained Sentiment Analysis on Financial Microblogs and News, where we limit ourselves to performing sentiment analysis on news headlines only (track 2). The approach presented in this paper uses a Support Vector Machine to do the required regression, and besides unigrams and a sentiment tool, we use various ontology-based features. To this end we created a domain ontology that models various concepts from the financial domain. This allows us to model the sentiment of actions depending on which entity they are affecting (e.g., 'decreasing debt' is positive, but 'decreasing profit' is negative). The presented approach yielded a cosine distance of 0.6810 on the official test data, resulting in the 12th position.", "KULeuven-LIIR at SemEval-2017 Task 12: Cross-Domain Temporal Information Extraction from Clinical Records.\n\nIn this paper, we describe the system of the KULeuven-LIIR submission for Clinical TempEval 2017. We participated in all six subtasks, using a combination of Support Vector Machines (SVM) for event and temporal expression detection, and a structured perceptron for extracting temporal relations. Moreover, we present and analyze the results from our submissions, and verify the effectiveness of several system components. Our system performed above average for all subtasks in both phases.", "Mahtab at SemEval-2017 Task 2: Combination of Corpus-based and Knowledge-based Methods to Measure Semantic Word Similarity.\n\nIn this paper, we describe our proposed method for measuring semantic similarity for a given pair of words at SemEval-2017 monolingual semantic word similarity task. We use a combination of knowledge-based and corpus-based techniques. We use FarsNet, the Persian Word Net, besides deep learning techniques to extract the similarity of words. We evaluated our proposed approach on Persian (Farsi) test data at SemEval-2017. It outperformed the other participants and ranked the first in the challenge.", "PurdueNLP at SemEval-2017 Task 1: Predicting Semantic Textual Similarity with Paraphrase and Event Embeddings.\n\nThis paper describes our proposed solution for SemEval 2017 Task 1: Semantic Textual Similarity \\cite{semeval2017}. The task aims at measuring the degree of equivalence between sentences given in English. Performance is evaluated by computing Pearson Correlation scores between the predicted scores and human judgements. Our proposed system consists of two subsystems and one regression model for predicting STS scores. The two subsystems are designed to learn Paraphrase and Event Embeddings that can take the consideration of paraphrasing characteristics and sentence structures into our system. The regression model associates these embeddings to make the final predictions. The experimental result shows that our system acquires 0.8 of Pearson Correlation Scores in this task.", "bunji at SemEval-2017 Task 3: Combination of Neural Similarity Features and Comment Plausibility Features.\n\nThis paper describes a text-ranking system developed by bunji team in SemEval-2017 Task 3: Community Question Answering, Subtask A and C. The goal of the task is to re-rank the comments in a question-and-answer forum such that useful comments for answering the question are ranked high. We proposed a method that combines neural similarity features and hand-crafted comment plausibility features, and we modeled inter-comments relationship using conditional random field. Our approach obtained the fifth place in the Subtask A and the second place in the Subtask C.", "Beihang-MSRA at SemEval-2017 Task 3: A Ranking System with Neural Matching Features for Community Question Answering.\n\nThis paper presents the system in SemEval-2017 Task 3, Community Question Answering (CQA). We develop a ranking system that is capable of capturing semantic relations between text pairs with little word overlap.  In addition to traditional NLP features, we introduce several neural network based matching features which enable our system to measure text similarity beyond lexicons. Our system significantly outperforms baseline methods and holds the second place in Subtask A and the fifth place in Subtask B, which demonstrates its efficacy on answer selection and question retrieval.", "\\#WarTeam at SemEval-2017 Task 6: Using Neural Networks for Discovering Humorous Tweets.\n\nThis paper presents the participation of \\#WarTeam in Task 6 of SemEval2017 with a system classifying humor by comparing and ranking tweets. The training data consists of annotated tweets from the @midnight TV show. \\#WarTeam's system uses a neural network (TensorFlow) having inputs from a Na\u00efve Bayes humor classifier and a sentiment analyzer.", "Sew-Embed at SemEval-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched Wikipedia.\n\nThis paper describes Sew-Embed, our language-independent approach to multilingual and cross-lingual semantic word similarity as part of the SemEval-2017 Task 2. We leverage the Wikipedia-based concept representations developed by Raganato et al. (2016), and propose an embedded augmentation of their explicit high-dimensional vectors, which we obtain by plugging in an arbitrary word (or sense) embedding representation, and computing a weighted average in the continuous vector space. We evaluate Sew-Embed with two different off-the-shelf embedding representations, and report their performances across all monolingual and cross-lingual benchmarks available for the task. Despite its simplicity, especially compared with supervised or overly tuned approaches, Sew-Embed achieves competitive results in the cross-lingual setting (3rd best result in the global ranking of subtask 2, score 0.56).", "QU-BIGIR at SemEval 2017 Task 3: Using Similarity Features for Arabic Community Question Answering Forums.\n\nIn this paper we describe our QU-BIGIR system for the Arabic subtask D of the SemEval 2017 Task 3. Our approach builds on our participation in the past version of the same subtask. This year, our system uses different similarity measures that encodes lexical and semantic pairwise similarity of text pairs. In addition to well known similarity measures such as cosine similarity, we use other measures based on the summary statistics of word embedding representation for a given text. To rank a list of candidate question answer pairs for a given question, we learn a linear SVM classifier over our similarity features. Our best resulting run came second in subtask D with a very competitive performance to the first-ranking system.", "ECNU at SemEval-2017 Task 3: Using Traditional and Deep Learning Methods to Address Community Question Answering Task.\n\nThis paper describes the systems we submitted to the task 3 (Community Ques- tion Answering) in SemEval 2017 which contains three subtasks on English corpora, i.e., subtask A: Question-Comment Similarity, subtask B: Question-Question Similarity, and subtask C: Question-External Comment Similarity. For subtask A, we combined two different methods to represent question-comment pair, i.e., supervised model using traditional features and Convolutional Neural Network. For subtask B, we utilized the information of snippets returned from Search Engine with question subject as query. For subtask C, we ranked the comments by multiplying the probability of the pair related question comment being Good by the reciprocal rank of the related question.", "ECNU at SemEval-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models.\n\nThis paper describes our submissions to task 8 in SemEval 2017, i.e., Determining rumour veracity and support for rumours. Given a rumoured tweet and a lot of reply tweets, the subtask A is to label whether these tweets are support, deny, query or comment, and the subtask B aims to predict the veracity (i.e., true, false, and unverified) with a confidence (in range of 0-1) of the given rumoured tweet. For both subtasks, we adopted supervised machine learning methods, incorporating rich features. Since training data is imbalanced, we specifically designed a two-step classifier to address subtask A .", "ECNU at SemEval-2017 Task 5: An Ensemble of Regression Algorithms with Effective Features for Fine-Grained Sentiment Analysis in Financial Domain.\n\nThis paper describes our systems submitted to the Fine-Grained Sentiment Analysis on Financial Microblogs and News task (i.e., Task 5) in SemEval-2017. This task includes two subtasks in microblogs and news headline domain respectively. To settle this problem, we extract four types of effective features, including linguistic features, sentiment lexicon features, domain-specific features and word embedding features. Then we employ these features to construct models by using ensemble regression algorithms. Our submissions rank 1st and rank 5th in subtask 1 and subtask 2 respectively.", "ECNU at SemEval-2017 Task 4: Evaluating Effective Features on Machine Learning Methods for Twitter Message Polarity Classification.\n\nThis paper reports our submission to subtask A of task 4 (Sentiment Analysis in Twitter, SAT) in SemEval 2017, i.e., Message Polarity Classification. We investigated several traditional Natural Language Processing (NLP) features, domain specific features and word embedding features together with supervised machine learning methods to address this task. Officially released results showed that our system ranked above average.", "Wild Devs' at SemEval-2017 Task 2: Using Neural Networks to Discover Word Similarity.\n\nThis paper presents Wild Devs' participation in the SemEval-2017 Task 2 ``Multi-lingual and Cross-lingual Semantic Word Similarity'', which tries to automatically measure the semantic similarity between two words. The system was build using neural networks, having as input a collection of word pairs, whereas the output consists of a list of scores, from 0 to 4, corresponding to the degree of similarity between the word pairs.", "ECNU at SemEval-2017 Task 7: Using Supervised and Unsupervised Methods to Detect and Locate English Puns.\n\nThis paper describes our submissions to task 7 in SemEval 2017, i.e., Detection and Interpretation of English Puns. We participated in the first two subtasks, which are to detect and locate English puns respectively. For subtask 1, we presented a supervised system to determine whether or not a sentence contains a pun using similarity features calculated on sense vectors or cluster center vectors. For subtask 2, we established an unsupervised system to locate the pun by scoring each word in the sentence and we assumed that the word with the smallest score is the pun.", "Fermi at SemEval-2017 Task 7: Detection and Interpretation of Homographic puns in English Language.\n\nThis paper describes our system for detection and interpretation of English puns. We participated in 2 subtasks related to homographic puns achieve comparable results for these tasks. Through the paper we provide detailed description of the approach, as well as the results obtained in the task. Our models achieved a F1-score of 77.65\\% for Subtask 1 and 52.15\\% for Subtask 2.", "Citius at SemEval-2017 Task 2: Cross-Lingual Similarity from Comparable Corpora and Dependency-Based Contexts.\n\nThis article describes the distributional strategy submitted by the Citius team to the SemEval 2017 Task 2. Even though the team participated in two subtasks, namely monolingual and cross-lingual word similarity, the article is mainly focused on the cross-lingual subtask. Our method uses comparable corpora and syntactic dependencies to extract count-based and transparent bilingual distributional contexts. The evaluation of the results show that our method is competitive with other cross-lingual strategies, even those using aligned and parallel texts.", "UINSUSKA-TiTech at SemEval-2017 Task 3: Exploiting Word Importance Levels for Similarity Features for CQA.\n\nThe majority of core techniques to solve many problems in Community Question Answering (CQA) task rely on similarity computation. This work focuses on similarity between two sentences (or questions in subtask B) based on word embeddings. We exploit words importance levels in sentences or questions for similarity features, for classification and ranking with machine learning. Using only 2 types of similarity metric, our proposed method has shown comparable results with other complex systems. This method on subtask B 2017 dataset is ranked on position 7 out of 13 participants. Evaluation on 2016 dataset is on position 8 of 12, outperforms some complex systems. Further, this finding is explorable and potential to be used as baseline and extensible for many tasks in CQA and other textual similarity based system.", "IITP at SemEval-2017 Task 8 : A Supervised Approach for Rumour Evaluation.\n\nThis paper describes our system participation in the SemEval-2017 Task 8 'RumourEval: Determining rumour veracity and support for rumours'. The objective of this task was to predict the stance and veracity of the underlying rumour. We propose a supervised classification approach employing several lexical, content and twitter specific features for learning. Evaluation shows promising results for both the problems.", "RTM at SemEval-2017 Task 1: Referential Translation Machines for Predicting Semantic Similarity.\n\nWe use referential translation machines for predicting the semantic similarity of text in all STS tasks which contain Arabic, English, Spanish, and Turkish this year. RTMs pioneer a language independent approach to semantic similarity and remove the need to access any task or domain specific information or resource. RTMs become 6th out of 52 submissions in Spanish to English STS. We average prediction scores using weights based on the training performance to improve the overall performance.", "SZTE-NLP at SemEval-2017 Task 10: A High Precision Sequence Model for Keyphrase Extraction Utilizing Sparse Coding for Feature Generation.\n\nIn this paper we introduce our system participating at the 2017 SemEval shared task on keyphrase extraction from scientific documents. We aimed at the creation of a keyphrase extraction approach which relies on as little external resources as possible. Without applying any hand-crafted external resources, and only utilizing a transformed version of word embeddings trained at Wikipedia, our proposed system manages to perform among the best participating systems in terms of precision.", "LIPN-IIMAS at SemEval-2017 Task 1: Subword Embeddings, Attention Recurrent Neural Networks and Cross Word Alignment for Semantic Textual Similarity.\n\nIn this paper we report our attempt to use, on the one hand, state-of-the-art neural approaches that are proposed to measure Semantic Textual Similarity (STS). On the other hand, we propose an unsupervised cross-word alignment approach, which is linguistically motivated. The neural approaches proposed herein are divided into two main stages. The first stage deals with constructing neural word embeddings, the components of sentence embeddings. The second stage deals with constructing a semantic similarity function relating pairs of sentence embeddings. Unfortunately our competition results were poor in all tracks, therefore we concentrated our research to improve them for Track 5 (EN-EN).", "IITPB at SemEval-2017 Task 5: Sentiment Prediction in Financial Text.\n\nThis paper reports team IITPB's participation in the SemEval 2017 Task 5 on `Fine-grained sentiment analysis on financial microblogs and news'. We developed 2 systems for the two tracks. One system was based on an ensemble of Support Vector Classifier and Logistic Regression. This system relied on Distributional Thesaurus (DT), word embeddings and lexicon features to predict a floating sentiment value between -1 and +1. The other system was based on Support Vector Regression using word embeddings, lexicon features, and PMI scores as features. The system was ranked 5th in track 1 and 8th in track 2.", "IITP at SemEval-2017 Task 5: An Ensemble of Deep Learning and Feature Based Models for Financial Sentiment Analysis.\n\nIn this  paper we propose an ensemble based model which combines state of the art deep learning sentiment analysis algorithms like Convolution Neural Network (CNN) and Long Short Term Memory (LSTM) along with feature based models to identify optimistic or pessimistic sentiments associated with companies and stocks in financial texts. We build our system to participate in a competition organized by Semantic Evaluation 2017 International Workshop. We combined predictions from various models using an artificial neural network to determine the opinion towards an entity in (a) Microblog Messages and (b) News Headlines data. Our models achieved a cosine similarity score of 0.751 and 0.697 for the above two tracks giving us the rank of 2nd and 7th best team respectively.", "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings.\n\nThis paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-of-embeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S\\\\&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.", "Talla at SemEval-2017 Task 3: Identifying Similar Questions Through Paraphrase Detection.\n\nThis paper describes our approach to the SemEval-2017 shared task of determining question-question similarity in a community question-answering setting (Task 3B). We extracted both syntactic and semantic similarity features between candidate questions, performed pairwise-preference learning to optimize for ranking order, and then trained a random forest classifier to predict whether the candidate questions are paraphrases of each other. This approach achieved a MAP of 45.7% out of max achievable 67.0% on the test set.", "SVNIT @ SemEval 2017 Task-6: Learning a Sense of Humor Using Supervised Approach.\n\nThis paper describes the system devel-oped for SemEval 2017 task 6: \\#HashTagWars -Learning a Sense of Hu-mor. Learning to recognize sense of hu-mor is the important task for language understanding applications. Different set of features based on frequency of words, structure of tweets and semantics are used in this system to identify the presence of humor in tweets. Supervised machine learning approaches, Multilayer percep-tron and Na\u00efve Bayes are used to classify the tweets in to three level of sense of humor. For given Hashtag, the system finds the funniest tweet and predicts the amount of funniness of all the other tweets. In official submitted runs, we have achieved 0.506 accuracy using mul-tilayer perceptron in subtask-A and 0.938 distance in subtask-B. Using Na\u00efve bayes in subtask-B, the system achieved 0.949 distance. Apart from official runs, this system have scored 0.751 accuracy in subtask-A using SVM. But still there is a wide room for improvement in system.", "SINAI at SemEval-2017 Task 4: User based classification.\n\nThis document describes our participation in SemEval-2017 Task 4: Sentiment Analysis in Twitter. We have only reported results for subtask B - English, determining the polarity towards a topic on a two point scale (positive or negative sentiment). Our main contribution is the integration of user information in the classification process. A SVM model is trained with Word2Vec vectors from user's tweets extracted from his timeline. The obtained results show that user-specific classifiers trained on tweets from user timeline can introduce noise as they are error prone because they are classified by an imperfect system. This encourages us to explore further integration of user information for author-based Sentiment Analysis.", "EUDAMU at SemEval-2017 Task 11: Action Ranking and Type Matching for End-User Development.\n\nThe paper describes a system for end-user development using natural language. Our approach uses a ranking model to identify the actions to be executed followed by reference and parameter matching models to select parameter values that should be set for the given commands. We discuss the results of evaluation and possible improvements for future work.", "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity.\n\nThis paper introduces a new task on Multilingual and Cross-lingual SemanticThis paper introduces a new task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish. High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark). These were used for semi-automatic construction of ten cross-lingual datasets. 17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: https://alt.qcri.org/semeval2017/task2/", "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours.\n\nMedia is full of false claims. Even Oxford Dictionaries named ``post-truth'' as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics --- each having their own families of claims and replies --- and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.", "SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications.\n\nWe describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.", "SemEval-2017 Task 7: Detection and Interpretation of English Puns.\n\nA pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect.  Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-per-context assumption.  This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns. We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set.  Finally, we present an overview and discussion of the participating systems' methodologies, resources, and results.", "SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News.\n\nThis paper discusses the ``Fine-Grained Sentiment Analysis on Financial Microblogs and News'' task as part of SemEval-2017, specifically under the ``Detecting sentiment, humour, and truth'' theme. This task contains two tracks, where the first one concerns Microblog messages and the second one covers News Statements and Headlines. The main goal behind both tracks was to predict the sentiment score for each of the mentioned companies/stocks. The sentiment scores for each text instance adopted floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 designating neutral sentiment. This task attracted a total of 32 participants, with 25 participating in Track 1 and 29 in Track 2.", "SemEval-2017 Task 9: Abstract Meaning Representation Parsing and Generation.\n\nIn this report we summarize the results of the 2017 AMR SemEval shared task. The task consisted of two separate yet related subtasks. In the parsing subtask, participants were asked to produce Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the biomedical domain. In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/forum domain. A total of five sites participated in the parsing subtask, and four participated in the generation subtask. Along with a description of the task and the participants' systems, we show various score ablations and some sample outputs.", "SemEval-2017 Task 4: Sentiment Analysis in Twitter.\n\nThis paper describes the fifth year of the Sentiment Analysis in Twitter task. SemEval-2017 Task 4 continues with a rerun of the subtasks of SemEval-2016 Task 4, which include identifying the overall sentiment of the tweet, sentiment towards a topic with classification on a two-point and on a five-point ordinal scale, and quantification of the distribution of sentiment towards a topic across a number of tweets: again on a two-point and on a five-point ordinal scale. Compared to 2016, we made two changes: (i) we introduced a new language, Arabic, for all subtasks, and (ii) we made available information from the profiles of the Twitter users who posted the target tweets. The task continues to be very popular, with a total of 48 teams participating this year.", "SemEval-2017 Task 12: Clinical TempEval.\n\nClinical TempEval 2017 aimed to answer the question: how well do systems trained on annotated timelines for one medical condition (colon cancer) perform in predicting timelines on another medical condition (brain cancer)? Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were evaluated on clinical and pathology notes from Mayo Clinic cancer patients, annotated with an extension of TimeML for the clinical domain. 11 teams participated in the tasks, with the best systems achieving F1 scores above 0.55 for time expressions, above 0.70 for event expressions, and above 0.40 for temporal relations. Most tasks observed about a 20 point drop over Clinical TempEval 2016, where systems were trained and evaluated on the same domain (colon cancer).", "SemEval-2017 Task 6: \\#HashtagWars: Learning a Sense of Humor.\n\nThis paper describes a new shared task for humor understanding that attempts to eschew the ubiquitous binary approach to humor detection and focus on comparative humor ranking instead. The task is based on a new dataset of funny tweets posted in response to shared hashtags, collected from the `Hashtag Wars' segment of the TV show @midnight. The results are evaluated in two subtasks that require the participants to generate either the correct pairwise comparisons of tweets (subtask A), or the correct ranking of the tweets (subtask B) in terms of how funny they are. 7 teams participated in subtask A, and 5 teams participated in subtask B. The best accuracy in subtask A was 0.675. The best (lowest) rank edit distance for subtask B was 0.872.", "Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines.\n\nIn this paper, we describe a methodology to infer Bullish or Bearish sentiment towards companies/brands. More specifically, our approach leverages affective lexica and word embeddings in combination with convolutional neural networks to infer the sentiment of financial news headlines towards a target company. Such architecture was used and evaluated in the context of the SemEval 2017 challenge (task 5, subtask 2), in which it obtained the best performance.", "LIPN at SemEval-2017 Task 10:  Filtering Candidate Keyphrases from Scientific Publications with Part-of-Speech Tag Sequences to Train a Sequence Labeling Model.\n\nThis paper describes the system used by the team LIPN in SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications. The team participated in Scenario 1, that includes three subtasks, Identification of keyphrases (Subtask A), Classification of identified keyphrases (Subtask B) and Extraction of relationships between two identified keyphrases (Subtask C). The presented system was mainly focused on the use of part-of-speech tag sequences to filter candidate keyphrases for Subtask A. Subtasks A and B were addressed as a sequence labeling problem using Conditional Random Fields (CRFs) and even though Subtask C was out of the scope of this approach, one rule was included to identify synonyms.", "SemEval-2017 Task 11: End-User Development using Natural Language.\n\nThis task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language. The task aims for supporting the evaluation and evolution of the discussions surrounding the natural language processing approaches within the context of end-user natural language programming, under scenarios of high semantic heterogeneity/gap.", "SemEval-2017 Task 3: Community Question Answering.\n\nWe describe SemEval---2017 Task 3 on Community Question Answering. This year, we reran the four subtasks from SemEval-2016: (A) Question--Comment Similarity, (B) Question--Question Similarity, (C) Question--External Comment Similarity, and (D) Rerank the correct answers for a new question in Arabic, providing all the data from 2015 and 2016 for training, and fresh data for testing. Additionally, we added a new subtask E in order to enable experimentation with Multi-domain Question Duplicate Detection in a larger-scale scenario, using StackExchange subforums. A total of 23 teams participated in the task, and submitted a total of 85 runs (36 primary and 49 contrastive) for subtasks A--D. Unfortunately, no teams participated in subtask E. A variety of approaches and features were used by the participating systems to address the different subtasks. The best systems achieved an official score (MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D, respectively.  These scores are better than the baselines, especially for subtasks A--C.", "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation.\n\nSemantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \\emph{all  language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the {\\em STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "TrentoTeam at SemEval-2017 Task 3: An application of Grice Maxims in Ranking Community Question Answers.\n\nIn   this   paper   we                                 present               the Tren-toTeam  system  which participated  to  thetask  3  at  SemEval-2017                                (Nakov et al.,2017).We concentrated  our  work  onapplying   Grice   Maxims(used   in manystate-of-the-art Machine learning applica-tions(Vogel  et  al.,  2013; Kheirabadiand  Aghagolzadeh,  2012;  Dale  and Re-iter, 1995;  Franke, 2011))                                to              ranking  an-swers of a question by answers relevancy.Particularly, we  created  a                                ranker                    systembased on relevancy scores, assigned by 3main components:  Named entity recogni-tion,  similarity score,  sentiment analysis.Our system obtained a comparable resultsto Machine learning systems.", "HLP@UPenn at SemEval-2017 Task 4A: A simple, self-optimizing text classification system combining dense and sparse vectors.\n\nWe present a simple supervised text classification system that combines sparse and dense vector representations of words, and generalized representations of words via clusters. The sparse vectors are generated from word n-gram sequences (1-3). The dense vector representations of words (embeddings) are learned by training a neural network to predict neighboring words in a large unlabeled dataset. To classify a text segment, the different representations of it are concatenated, and the classification is performed using Support Vector Machines (SVM). Our system is particularly intended for use by non-experts of natural language processing and machine learning, and, therefore, the system does not require any manual tuning of parameters or weights. Given a training set, the system automatically generates the training vectors, optimizes the relevant hyper-parameters for the SVM classifier, and trains the classification model. We evaluated this system on the SemEval-2017 English sentiment analysis task. In terms of average F1-score, our system obtained 8th position out of 39 submissions (F1-score: 0.632, average recall: 0.637, accuracy: 0.646).", "MoRS at SemEval-2017 Task 3: Easy to use SVM in Ranking Tasks.\n\nThis paper describes our system, dubbed MoRS (Modular Ranking System), pronounced 'Morse', which participated in Task 3 of SemEval-2017. We used MoRS to perform the Community Question Answering Task 3, which consisted on reordering a set of comments according to their usefulness in answering the question in the thread. This was made for a large collection of questions created by a user community. As for this challenge we wanted to go back to simple, easy-to-use, and somewhat forgotten technologies that we think, in the hands of non-expert people, could be reused in their own data sets. Some of our techniques included the annotation of text, the retrieval of meta-data for each comment, POS tagging and Named Entity Recognition, among others. These gave place to syntactical analysis and semantic measurements. Finally we show and discuss our results and the context of our approach, which is part of a more comprehensive system in development, named MoQA.", "UWaterloo at SemEval-2017 Task 7: Locating the Pun Using Syntactic Characteristics and Corpus-based Metrics.\n\nThe paper presents a system for locating a pun word. The developed method calculates a score for each word in a pun, using a number of components, including its Inverse Document Frequency (IDF), Normalized Pointwise Mutual Information (NPMI) with other words in the pun text, its position in the text, part-of-speech and some syntactic features. The method achieved the best performance in the Heterographic category and the second best in the Homographic. Further analysis showed that IDF is the most useful characteristic, whereas the count of words with which the given word has high NPMI has a negative effect on performance.", "UWaterloo at SemEval-2017 Task 8: Detecting Stance towards Rumours with Topic Independent Features.\n\nThis paper describes our system for subtask-A: SDQC for RumourEval, task-8 of SemEval 2017. Identifying rumours, especially for breaking news events as they unfold, is a challenging task due to the absence of sufficient information about the exact rumour stories circulating on social media. Determining the stance of Twitter users towards rumourous messages could provide an indirect way of identifying potential rumours. The proposed approach makes use of topic independent features from two categories, namely cue features and message specific features to fit a gradient boosting classifier. With an accuracy of 0.78, our system achieved the second best performance on subtask-A of RumourEval.", "PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in Automatic Pun Recognition and Interpretation.\n\nThe article describes a model of automatic interpretation of English puns, based on Roget{'}s Thesaurus, and its implementation, PunFields. In a pun, the algorithm discovers two groups of words that belong to two main semantic fields. The fields become a semantic vector based on which an SVM classifier learns to recognize puns. A rule-based model is then applied for recognition of intentionally ambiguous (target) words and their definitions. In SemEval Task 7 PunFields shows a considerably good result in pun classification, but requires improvement in searching for the target word and its definition.", "SSN\\_MLRG1 at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis Using Multiple Kernel Gaussian Process Regression Model.\n\nThe system developed by the SSN\\_MLRG1 team for Semeval-2017 task 5 on fine-grained sentiment analysis uses Multiple Kernel Gaussian Process for identifying the optimistic and pessimistic sentiments associated with companies and stocks. Since the comments made at different times about the same companies and stocks may display different emotions, their properties such as smoothness and periodicity may vary. Our experiments show that while single kernel Gaussian Process can learn certain properties well, Multiple Kernel Gaussian Process are effective in learning the presence of different properties simultaneously.", "ej-sa-2017 at SemEval-2017 Task 4: Experiments for Target oriented Sentiment Analysis in Twitter.\n\nThis paper describes the system we have used for participating in Subtasks A (Message Polarity Classification) and B (Topic-Based Message Polarity Classification according to a two-point scale) of SemEval-2017 Task 4 Sentiment Analysis in Twitter. We used several features with a sentiment lexicon and NLP techniques, Maximum Entropy as a classifier for our system.", "NTU-1 at SemEval-2017 Task 12: Detection and classification of temporal events in clinical data with domain adaptation.\n\nThis study proposes a system to participate in the Clinical TempEval 2017 shared task, a part of the SemEval 2017 Tasks. Domain adaptation was the main challenge this year. We took part in the supervised domain adaption where data of 591 records of colon cancer patients and 30 records of brain cancer patients from Mayo clinic were given and we are asked to analyze the records from brain cancer patients. Based on the THYME corpus released by the organizer of Clinical TempEval, we propose a framework that automatically analyzes clinical temporal events in a fine-grained level. Support vector machine (SVM) and conditional random field (CRF) were implemented in our system for different subtasks, including detecting clinical relevant events and time expression, determining their attributes, and identifying their relations with each other within the document. The results showed the capability of domain adaptation of our system.", "IBA-Sys at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News.\n\nThis paper presents the details of our system IBA-Sys that participated in SemEval Task: Fine-grained sentiment analysis on Financial Microblogs and News. Our system participated in both tracks. For microblogs track, a supervised learning approach was adopted and the regressor was trained using XgBoost regression algorithm on lexicon features. For news headlines track, an ensemble of regressors was used to predict sentiment score. One regressor was trained using TF-IDF features and another was trained using the n-gram features. The source code is available at Github 1.", "BB\\_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs.\n\nIn this paper we describe our attempt at producing a state-of-the-art Twitter sentiment classifier using Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) networks.  Our system leverages a large amount of unlabeled data to pre-train word embeddings.  We then use a subset of the unlabeled data to fine tune the embeddings using distant supervision.                          The final CNNs and LSTMs are trained on the SemEval-2017 Twitter dataset where the embeddings are fined tuned again.  To boost performances we ensemble several CNNs and LSTMs together. Our approach achieved first rank on all of the five English subtasks amongst 40 teams.", "SentiME++ at SemEval-2017 Task 4: Stacking State-of-the-Art Classifiers to Enhance Sentiment Classification.\n\nIn this paper, we describe the participation of the SentiME++ system to the SemEval 2017 Task 4A ``Sentiment Analysis in Twitter'' that aims to classify whether English tweets are of positive, neutral or negative sentiment. SentiME++ is an ensemble approach to sentiment analysis that leverages stacked generalization to automatically combine the predictions of five state-of-the-art sentiment classifiers. SentiME++ achieved officially 61.30% F1-score, ranking 12th out of 38 participants.", "Duluth at SemEval-2017 Task 6: Language Models in Humor Detection.\n\nThis paper describes the Duluth system that participated in SemEval-2017 Task 6 \\#HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.", "EICA Team at SemEval-2017 Task 3: Semantic and Metadata-based Features for Community Question Answering.\n\nWe describe our system for participating in SemEval-2017 Task 3 on Community Question Answering. Our approach relies on combining a rich set of various types of features: semantic and metadata. The most important group turned out to be the metadata feature and the semantic vectors trained on QatarLiving data. In the main Subtask C, our primary submission was ranked fourth, with a MAP of 13.48 and accuracy of 97.08. In Subtask A, our primary submission get into the top 50%.", "JU CSE NLP @ SemEval 2017 Task 7: Employing Rules to Detect and Interpret English Puns.\n\nSystem description. Implementation of HMM and Cyclic Dependency Network.", "UIT-DANGNT-CLNLP at SemEval-2017 Task 9: Building Scientific Concept Fixing Patterns for Improving CAMR.\n\nThis paper describes the improvements that we have applied on CAMR baseline parser (Wang et al., 2016) at Task 8 of SemEval-2016. Our objective is to increase the performance of CAMR when parsing sentences from scientific articles, especially articles of biology domain more accurately. To achieve this goal, we built two wrapper layers for CAMR. The first layer, which covers the input data, will normalize, add necessary information to the input sentences to make the input dependency parser and the aligner better handle reference citations, scientific figures, formulas, etc. The second layer, which covers the output data, will modify and standardize output data based on a list of scientific concept fixing patterns. This will help CAMR better handle biological concepts which are not in the training dataset. Finally, after applying our approach, CAMR has scored 0.65 F-score on the test set of Biomedical training data and 0.61 F-score on the official blind test dataset.", "Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter.\n\nThis paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on a Twitter sentiment treebank, and the use of feedforward NN, Naive Bayes and logistic regression classifiers to produce predictions for the different sub-tasks. The algorithm reached the 3rd place on the 5-label classification task (sub-task C).", "XJNLP at SemEval-2017 Task 12: Clinical temporal information ex-traction with a Hybrid Model.\n\nTemporality is crucial in understanding the course of clinical events from a patient's electronic health recordsand temporal processing is becoming more and more important for improving access to content.SemEval 2017 Task 12 (Clinical TempEval) addressed this challenge using the THYME corpus, a corpus of clinical narratives annotated with a schema based on TimeML2 guidelines. We developed and evaluated approaches for: extraction of temporal expressions (TIMEX3) and EVENTs; EVENT attributes; document-time relations. Our approach is a hybrid model which is based on rule based methods, semi-supervised learning, and semantic features with addition of manually crafted rules.", "TWINA at SemEval-2017 Task 4: Twitter Sentiment Analysis with Ensemble Gradient Boost Tree Classifier.\n\nThis paper describes the TWINA system, with which we participated in SemEval-2017 Task 4B (Topic Based Message Polarity Classification --- Two point scale) and 4D (two-point scale Tweet quantification). We implemented ensemble based Gradient Boost Trees classification method for both the tasks. Our system could perform well for the task 4D and ranked 13th among 15 teams, for the task 4B our model ranked 23rd position.", "FA3L at SemEval-2017 Task 3: A ThRee Embeddings Recurrent Neural Network for Question Answering.\n\nIn this paper we present ThReeNN, a model for Community Question Answering, Task 3, of SemEval-2017. The proposed model exploits both syntactic and semantic information to build a single and meaningful embedding space. Using a dependency parser in combination with word embeddings, the model creates sequences of inputs for a Recurrent Neural Network, which are then used for the ranking purposes of the Task. The score obtained on the official test data shows promising results.", "Tw-StAR at SemEval-2017 Task 4: Sentiment Classification of Arabic Tweets.\n\nIn this paper, we present our contribution in SemEval 2017 international workshop. We have tackled task 4 entitled ``Sentiment analysis in Twitter'', specifically subtask 4A-Arabic. We propose two Arabic sentiment classification models implemented using supervised and unsupervised learning strategies. In both models, Arabic tweets were preprocessed first then various schemes of bag-of-N-grams were extracted to be used as features. The final submission was selected upon the best performance achieved by the supervised learning-based model. However, the results obtained by the unsupervised learning-based model are considered promising and evolvable if more rich lexica are adopted in further work.", "QUB at SemEval-2017 Task 6: Cascaded Imbalanced Classification for Humor Analysis in Twitter.\n\nThis paper presents our submission to SemEval-2017 Task 6: \\#HashtagWars: Learning a Sense of Humor. There are two subtasks: A. Pairwise Comparison, and B. Semi-Ranking. Our assumption is that the distribution of humorous and non-humorous texts in real life language is naturally imbalanced. Using Na\u00efve Bayes Multinomial with standard text-representation features, we approached Subtask B as a sequence of imbalanced classification problems, and optimized our system per the macro-average recall. Subtask A was then solved via the Semi-Ranking results. On the final test, our system was ranked 10th for Subtask A, and 3rd for Subtask B.", "OMAM at SemEval-2017 Task 4: English Sentiment Analysis with Conditional Random Fields.\n\nWe describe a supervised system that uses optimized Condition Random Fields and lexical features to predict the sentiment of a tweet. The system was submitted to the English version of all subtasks in SemEval-2017 Task 4.", "Tweester at SemEval-2017 Task 4: Fusion of Semantic-Affective and pairwise classification models for sentiment analysis in Twitter.\n\nIn this paper, we describe our submission to SemEval2017 Task 4: Sentiment Analysis in Twitter. Specifically the proposed system participated both to tweet polarity classification (two-, three- and five class) and tweet quantification (two and five-class) tasks.", "NRU-HSE at SemEval-2017 Task 4: Tweet Quantification Using Deep Learning Architecture.\n\nIn many areas, such as social science, politics or market research, people need to deal with dataset shifting over time. Distribution drift phenomenon usually appears in the field of sentiment analysis, when proportions of instances are changing over time. In this case, the task is to correctly estimate proportions of each sentiment expressed in the set of documents (quantification task). Basically, our study was aimed to analyze the effectiveness of a mixture of quantification technique with one of deep learning architecture. All the techniques are evaluated using the SemEval-2017 Task4 dataset and source code, mentioned in this paper and available online in the Python programming language. The results of an application of the quantification techniques are discussed.", "Oxford at SemEval-2017 Task 9: Neural AMR Parsing with Pointer-Augmented Attention.\n\nWe present a neural encoder-decoder AMR parser that extends an attention-based model by predicting the alignment between graph nodes and sentence tokens explicitly with a pointer mechanism. Candidate lemmas are predicted as a pre-processing step so that the lemmas of lexical concepts, as well as constant strings, are factored out of the graph linearization and recovered through the predicted alignments. The approach does not rely on syntactic parses or extensive external resources. Our parser obtained 59% Smatch on the SemEval test set.", "Jmp8 at SemEval-2017 Task 2: A simple and general distributional approach to estimate word similarity.\n\nWe have built a simple corpus-based system to estimate words similarity in multiple languages with a count-based approach. After training on Wikipedia corpora, our system was evaluated on the multilingual subtask of SemEval-2017 Task 2 and achieved a good level of performance, despite its great simplicity. Our results tend to demonstrate the power of the distributional approach in semantic similarity tasks, even without knowledge of the underlying language. We also show that dimensionality reduction has a considerable impact on the results.", "The AI2 system at SemEval-2017 Task 10 (ScienceIE): semi-supervised end-to-end entity and relation extraction.\n\nThis paper describes our submission for the ScienceIE shared task (SemEval- 2017 Task 10) on entity and relation extraction from scientific papers. Our model is based on the end-to-end relation extraction model of Miwa and Bansal (2016) with several enhancements such as semi-supervised learning via neural language models, character-level encoding, gazetteers extracted from existing knowledge bases, and model ensembles. Our of- ficial submission ranked first in end-to-end entity and relation extraction (scenario 1), and second in the relation-only extraction (scenario 3).", "OMAM at SemEval-2017 Task 4: Evaluation of English State-of-the-Art Sentiment Analysis Models for Arabic and a New Topic-based Model.\n\nWhile sentiment analysis in English has achieved significant progress, it remains a challenging task in Arabic given the rich morphology of the language. It becomes more challenging when applied to Twitter data that comes with additional sources of noise including dialects, misspellings, grammatical mistakes, code switching and the use of non-textual objects to express sentiments. This paper describes the ``OMAM'' systems that we developed as part of SemEval-2017 task 4. We evaluate English state-of-the-art methods on Arabic tweets for subtask A. As for the remaining subtasks, we introduce a topic-based approach that accounts for topic specificities by predicting topics or domains of upcoming tweets, and then using this information to predict their sentiment. Results indicate that applying the English state-of-the-art method to Arabic has achieved solid results without significant enhancements. Furthermore, the topic-based method ranked 1st in subtasks C and E, and 2nd in subtask D.", "DT\\_Team at SemEval-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and Gaussian Mixture Model Output.\n\nWe describe our system (DT Team) submitted at SemEval-2017 Task 1, Semantic Textual Similarity (STS) challenge for English (Track 5). We developed three different models with various features including similarity scores calculated using word and chunk alignments, word/sentence embeddings, and Gaussian Mixture Model(GMM). The correlation between our system's output and the human judgments were up to 0.8536, which is more than 10% above baseline, and almost as good as the best performing system which was at 0.8547 correlation (the difference is just about 0.1%). Also, our system produced leading results when evaluated with a separate STS benchmark dataset. The word alignment and sentence embeddings based features were found to be very effective.", "SCIR-QA at SemEval-2017 Task 3: CNN Model Based on Similar and Dissimilar Information between Keywords for Question Similarity.\n\nWe describe a method of calculating the similarity of questions in community QA. Question in cQA are usually very long and there are a lot of useless information about calculating the similarity of questions. Therefore,we implement a CNN model based on similar and dissimilar information between question's keywords. We extract the keywords of questions, and then model the similar and dissimilar information between the keywords, and use the CNN model to calculate the similarity.", "LearningToQuestion at SemEval 2017 Task 3: Ranking Similar Questions by Learning to Rank Using Rich Features.\n\nThis paper describes our official entry LearningToQuestion for SemEval 2017 task 3 community question answer, subtask B. The objective is to rerank questions obtained in web forum as per their similarity to original question. Our system uses pairwise learning to rank methods on rich set of hand designed and representation learning features. We use various semantic features that help our system to achieve promising results on the task. The system achieved second highest results on official metrics MAP and good results on other search metrics.", "IKM at SemEval-2017 Task 8: Convolutional Neural Networks for stance detection and rumor verification.\n\nThis paper describes our approach for SemEval-2017 Task 8. We aim at detecting the stance of tweets and determining the veracity of the given rumor. We utilize a convolutional neural network for short text categorization using multiple filter sizes. Our approach beats the baseline classifiers on different event data with good F1 scores. The best of our submitted runs achieves rank 1st among all scores on subtask B.", "L2F/INESC-ID at SemEval-2017 Tasks 1 and 2: Lexical and semantic features in word and textual similarity.\n\nThis paper describes our approach to the SemEval-2017 ``Semantic Textual Similarity'' and ``Multilingual Word Similarity'' tasks. In the former, we test our approach in both English and Spanish, and use a linguistically-rich set of features. These move from lexical to semantic features. In particular, we try to take advantage of the recent Abstract Meaning Representation and SMATCH measure. Although without state of the art results, we introduce semantic structures in textual similarity and analyze their impact. Regarding word similarity, we target the English language and combine WordNet information with Word Embeddings. Without matching the best systems, our approach proved to be simple and effective.", "N-Hance at SemEval-2017 Task 7: A Computational Approach using Word Association for Puns.\n\nThis paper presents a system developed for SemEval-2017 Task 7, Detection and Interpretation of English Puns consisting of three subtasks; pun detection, pun location, and pun interpretation, respectively. The system stands on recognizing a distinctive word which has a high association with the pun in the given sentence. The intended humorous meaning of pun is identified through the use of this word. Our official results confirm the potential of this approach.", "MI\\\\&T Lab at SemEval-2017 task 4: An Integrated Training Method of Word Vector for Sentiment Classification.\n\nA CNN method for sentiment classification task in Task 4A of SemEval 2017 is presented. To solve the problem of word2vec training word vector slowly, a method of training word vector by integrating word2vec and Convolutional Neural Network (CNN) is proposed. This training method not only improves the training speed of word2vec, but also makes the word vector more effective for the target task. Furthermore, the word2vec adopts a full connection between the input layer and the projection layer of the Continuous Bag-of-Words (CBOW) for acquiring the semantic information of the original sentence.", "SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity between Questions for Community Question Answering.\n\nThis paper describes the SimBow system submitted at SemEval2017-Task3, for the question-question similarity subtask B. The proposed approach is a supervised combination of different unsupervised textual similarities. These textual similarities rely on the introduction of a relation matrix in the classical cosine similarity between bag-of-words, so as to get a soft-cosine that takes into account relations between words. According to the type of relation matrix embedded in the soft-cosine, semantic or lexical relations can be considered. Our system ranked first among the official submissions of subtask B.", "FCICU at SemEval-2017 Task 1: Sense-Based Language Independent Semantic Textual Similarity Approach.\n\nThis paper describes FCICU team systems that participated in SemEval-2017 Semantic Textual Similarity task (Task1) for monolingual and cross-lingual sentence pairs. A sense-based language independent textual similarity approach is presented, in which a proposed alignment similarity method coupled with new usage of a semantic network (BabelNet) is used. Additionally, a previously proposed integration between sense-based and sur-face-based semantic textual similarity approach is applied together with our proposed approach. For all the tracks in Task1, Run1 is a string kernel with alignments metric and Run2 is a sense-based alignment similarity method. The first run is ranked 10th, and the second is ranked 12th in the primary track, with correlation 0.619 and 0.617 respectively", "HCTI at SemEval-2017 Task 1: Use convolutional neural network to evaluate Semantic Textual Similarity.\n\nThis paper describes our convolutional neural network (CNN) system for Semantic Textual Similarity (STS) task. We calculated semantic similarity score between two sentences by comparing their semantic vectors. We generated semantic vector of every sentence by max pooling every dimension of their word vectors. There are mainly two trick points in our system. One is that we trained a CNN to transfer GloVe word vectors to a more proper form for STS task before pooling. Another is that we trained a fully-connected neural network (FCNN) to transfer difference of two semantic vectors to probability of every similarity score. We decided all hyper parameters empirically. In spite of the simplicity of our neural network system, we achieved a good accuracy and ranked 3rd in primary track of SemEval 2017.", "NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis.\n\nThis paper describes our multi-view ensemble approach to SemEval-2017 Task 4 on Sentiment Analysis in Twitter, specifically, the Message Polarity Classification subtask for English (subtask A). Our system is a voting ensemble, where each base classifier is trained in a different feature space. The first space is a bag-of-words model and has a Linear SVM as base classifier. The second and third spaces are two different strategies of combining word embeddings to represent sentences and use a Linear SVM and a Logistic Regressor as base classifiers. The proposed system was ranked 18th out of 38 systems considering F1 score and 20th considering recall.", "NTNU-1@ScienceIE at SemEval-2017 Task 10: Identifying and Labelling Keyphrases with Conditional Random Fields.\n\nWe present NTNU's systems for Task A (prediction of keyphrases) and Task B (labelling as Material, Process or Task) at SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications \\cite{augenstein2017scienceie}. Our approach relies on supervised machine learning using Conditional Random Fields. Our system yields a micro F-score of 0.34 for Tasks A and B combined on the test data. For Task C (relation extraction), we relied on an independently developed system described in \\cite{Barik:2017}. For the full Scenario 1 (including relations), our approach reaches a micro F-score of 0.33 (5th place). Here we describe our systems, report results and discuss errors.", "SiTAKA at SemEval-2017 Task 4: Sentiment Analysis in Twitter Based on a Rich Set of Features.\n\nThis paper describes SiTAKA, our system that has been used in task 4A, English and Arabic languages, Sentiment Analysis in Twitter of SemEval2017. The system proposes the representation of tweets using a novel set of features, which include a bag of negated words and the information provided by some lexicons. The polarity of tweets is determined by a classifier based on a Support Vector Machine. Our system ranks 2nd among 8 systems in the Arabic language tweets and ranks 8th among 38 systems in the English-language tweets.", "Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification.\n\nThis paper presents Senti17 system which uses ten convolutional neural networks (Con- vNet) to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Soft- max on top. Ten instances of this network are initialized with the same word embeddings  as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4\\% average recall.", "LIM-LIG at SemEval-2017 Task1: Enhancing the Semantic Similarity for Arabic Sentences with Vectors Weighting.\n\nThis article describes our proposed system  named LIM-LIG. This system is designed for SemEval 2017 Task1: Semantic  Textual  Similarity (Track1). LIM-LIG  proposes  an  innovative  enhancement to word embedding-based model devoted  to    measure                                            the  semantic similarity in Arabic sentences. The  main  idea  is  to  exploit    the  word representations  as  vectors  in a  multidimensional  space    to    capture  the  semantic  and  syntactic properties  of                                              words. IDF weighting and Part-of-Speech                                tagging are applied  on  the  examined  sentences  to  support    the  identification  of words  that  are  highly  descriptive  in  each  sentence.  LIM-LIG system achieves a Pearson\\'s correlation of 0.74633, ranking 2nd among all participants in the   Arabic monolingual pairs STS task organized within the SemEval 2017 evaluation campaign", "HHU at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Data using Machine Learning Methods.\n\nIn this Paper a system for solving SemEval-2017 Task 5 is presented. This task is divided into two tracks where the sentiment of microblog messages and news headlines has to be predicted. Since two submissions were allowed, two different machine learning methods were developed to solve this task, a support vector machine approach and a recurrent neural network approach. To feed in data for these approaches, different feature extraction methods are used, mainly word representations and lexica. The best submissions for both tracks are provided by the recurrent neural network which achieves a F1-score of 0.729 in track 1 and 0.702 in track 2.", "OPI-JSA at SemEval-2017 Task 1: Application of Ensemble learning for computing semantic textual similarity.\n\nSemantic Textual Similarity (STS) evaluation assesses the degree to which two parts of texts are similar, based on their semantic evaluation. In this paper, we describe three models submitted to STS SemEval 2017. Given two English parts of a text, each of proposed methods outputs the assessment of their semantic similarity. We propose an approach for computing monolingual semantic textual similarity based on an ensemble of three distinct methods. Our model consists of recursive neural network (RNN) text auto-encoders ensemble with supervised a model of vectorized sentences using reduced part of speech (PoS) weighted word embeddings as well as unsupervised a method based on word coverage (TakeLab). Additionally, we enrich our model with additional features that allow disambiguation of ensemble methods based on their efficiency. We have used Multi-Layer Perceptron as an ensemble classifier basing on estimations of trained Gradient Boosting Regressors. Results of our research proves that using such ensemble leads to a higher accuracy due to a fact that each member-algorithm tends to specialize in particular type of sentences. Simple model based on PoS weighted Word2Vec word embeddings seem to improve performance of more complex RNN based auto-encoders in the ensemble. In the monolingual English-English STS subtask our Ensemble based model achieved mean Pearson correlation of .785 compared with human annotators.", "DUTH at SemEval-2017 Task 4: A Voting Classification Approach for Twitter Sentiment Analysis.\n\nThis report describes our participation to SemEval-2017 Task 4: Sentiment Analysis in Twitter, specifically in subtasks A, B, and C. The approach for text sentiment classification is based on a Majority Vote scheme and combined supervised machine learning methods with classical linguistic resources, including bag-of-words and sentiment lexicon features.", "SSN\\_MLRG1 at SemEval-2017 Task 4: Sentiment Analysis in Twitter Using Multi-Kernel Gaussian Process Classifier.\n\nThe SSN MLRG1 team for Semeval-2017 task 4 has applied Gaussian Process, with bag of words feature vectors and fixed rule multi-kernel learning, for sentiment analysis of tweets. Since tweets on the same topic, made at different times, may exhibit different emotions, their properties such as smoothness and periodicity also vary with time. Our experiments show that, compared to single kernel, multiple kernels are effective in learning the simultaneous presence of multiple properties.", "INF-UFRGS at SemEval-2017 Task 5: A Supervised Identification of Sentiment Score in Tweets and Headlines.\n\nThis paper describes a supervised solution for detecting the polarity scores of tweets or headline news in the financial domain, submitted to the SemEval 2017 Fine-Grained Sentiment Analysis on Financial Microblogs and News Task. The premise is that it is possible to understand market reaction over a company stock by measuring the positive/negative sentiment contained in the financial tweets and news headlines, where polarity is measured in a continuous scale ranging from -1.0 (very bearish) to 1.0 (very bullish). Our system receives as input the textual content of tweets or news headlines, together with their ids, stock cashtag or name of target company, and the polarity score gold standard for the training dataset. Our solution retrieves features from these text instances using n-gram, hashtags, sentiment score calculated by a external APIs and others features to train a regression model capable to detect continuous score of these sentiments with precision.", "CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity.\n\nWe present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02% with human annotations.", "IIT-UHH at SemEval-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification.\n\nIn this paper we present the system for Answer Selection and Ranking in Community Question Answering, which we build as part of our participation in SemEval-2017 Task 3. We develop a Support Vector Machine (SVM) based system that makes use of textual, domain-specific, word-embedding and topic-modeling features. In addition, we propose a novel method for dialogue chain identification in comment threads. Our primary submission won subtask C, outperforming other systems in all the primary evaluation metrics. We performed well in other English subtasks, ranking third in subtask A and eighth in subtask B. We also developed open source toolkits for all the three English subtasks by the name cQARank [https://github.com/TitasNandi/cQARank].", "Lump at SemEval-2017 Task 1: Towards an Interlingua Semantic Similarity.\n\nThis is the Lump team participation at SemEval 2017 Task 1 on Semantic Textual Similarity. Our supervised model relies on features which are multilingual or interlingual in nature. We include lexical similarities, cross-language explicit semantic analysis, internal representations of multilingual neural networks and interlingual word embeddings. Our representations allow to use large datasets in language pairs with many instances to better classify instances in smaller language pairs avoiding the necessity of translating into a single language. Hence we can deal with all the languages in the task: Arabic, English, Spanish, and Turkish.", "QLUT at SemEval-2017 Task 1: Semantic Textual Similarity Based on Word Embeddings.\n\nThis paper reports the details of our submissions in the task 1 of SemEval 2017. This task aims at assessing the semantic textual similarity of two sentences or texts. We submit three unsupervised systems based on word embeddings. The differences between these runs are the various preprocessing on evaluation data. The best performance of these systems on the evaluation of Pearson correlation is 0.6887. Unsurprisingly, results of our runs demonstrate that data preprocessing, such as tokenization, lemmatization, extraction of content words and removing stop words, is helpful and plays a significant role in improving the performance of models.", "ELiRF-UPV at SemEval-2017 Task 7: Pun Detection and Interpretation.\n\nThis paper describes the participation of ELiRF-UPV team at task 7 (subtask 2: homographic pun detection and subtask 3: homographic pun interpretation) of SemEval2017. Our approach is based on the use of word embeddings to find related words in a sentence and a version of the Lesk algorithm to establish relationships between synsets. The results obtained are in line with those obtained by the other participants and they encourage us to continue working on this problem.", "BuzzSaw at SemEval-2017 Task 7: Global vs. Local Context for Interpreting and Locating Homographic English Puns with Sense Embeddings.\n\nThis paper describes our system participating in the SemEval-2017 Task 7, for the subtasks of homographic pun location and homographic pun interpretation. For pun interpretation, we use a knowledge-based Word Sense Disambiguation (WSD) method based on sense embeddings. Pun-based jokes can be divided into two parts, each containing information about the two distinct senses of the pun. To exploit this structure we split the context that is input to the WSD system into two local contexts and find the best sense for each of them. We use the output of pun interpretation for pun location. As we expect the two meanings of a pun to be very dissimilar, we compute sense embedding cosine distances for each sense-pair and select the word that has the highest distance. We describe experiments on different methods of splitting the context and compare our method to several baselines. We find evidence supporting our hypotheses and obtain competitive results for pun interpretation.", "YNUDLG at SemEval-2017 Task 4: A GRU-SVM Model for Sentiment Classification and Quantification in Twitter.\n\nSentiment analysis is one of the central issues in Natural Language Processing and has become more and more important in many fields. Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.,positive or negative). In this paper we describe our deep learning system(combining GRU and SVM) to solve both two-, three- and five- tweet polarity classifications. We first trained a gated recurrent neural network using pre-trained word embeddings, then we extracted features from GRU layer and input these features into support vector machine to fulfill both the classification and quantification subtasks. The proposed approach achieved 37th, 19th, and 14rd places in subtasks A, B and C, respectively.", "QLUT at SemEval-2017 Task 2: Word Similarity Based on Word Embedding and Knowledge Base.\n\nThis paper shows the details of our system submissions in the task 2 of SemEval 2017. We take part in the subtask 1 of this task, which is an English monolingual subtask. This task is designed to evaluate the semantic word similarity of two linguistic items. The results of runs are assessed by standard Pearson and Spearman correlation, contrast with official gold standard set. The best performance of our runs is 0.781 (Final). The techniques of our runs mainly make use of the word embeddings and the knowledge-based method. The results demonstrate that the combined method is effective for the computation of word similarity, while the word embeddings and the knowledge-based technique, respectively, needs more deeply improvement in details.", "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION.\n\nThis paper describes our approach to the SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications, specifically to Subtask (B): Classification of identified keyphrases. We explored three different deep learning approaches: a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-$F\\_1$-score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15\\% of the full data, namely, the original development set. When trained on the full data (training$+$development), our ensemble has a micro-$F\\_{1}$-score of 0.69. Our code is available from https://github.com/UKPLab/semeval2017-scienceie.", "BIT at SemEval-2017 Task 1: Using Semantic Information Space to Evaluate Semantic Textual Similarity.\n\nThis paper presents three systems for semantic textual similarity (STS) evaluation at SemEval-2017 STS task. One is an unsupervised system and the other two are supervised systems which simply employ the unsupervised one. All our systems mainly depend on the (SIS), which is constructed based on the semantic hierarchical taxonomy in WordNet, to compute non-overlapping information content (IC) of sentences. Our team ranked 2nd among 31 participating teams by the primary score of Pearson correlation coefficient (PCC) mean of 7 tracks and achieved the best performance on Track 1 (AR-AR) dataset.", "UdL at SemEval-2017 Task 1: Semantic Textual Similarity Estimation of English Sentence Pairs Using Regression Model over Pairwise Features.\n\nThis paper describes the model UdL we proposed to solve the semantic textual similarity task of SemEval 2017 workshop. The track we participated in was estimating the semantics relatedness of a given set of sentence pairs in English. The best run out of three submitted runs of our model achieved a Pearson correlation score of 0.8004 compared to a hidden human annotation of 250~pairs. We used random forest ensemble learning to map an expandable set of extracted pairwise features into a semantic similarity estimated value bounded between 0 and 5. Most of these features were calculated using word embedding vectors similarity to align Part of Speech (PoS) and Name Entities (NE) tagged tokens of each sentence pair. Among other pairwise features, we experimented a classical tf-idf weighted Bag of Words (BoW) vector model but with character-based range of n-grams instead of words. This sentence vector BoW-based feature gave a relatively high importance value percentage in the feature importances analysis of the ensemble learning.", "ResSim at SemEval-2017 Task 1: Multilingual Word Representations for Semantic Textual Similarity.\n\nShared Task 1 at SemEval-2017 deals with assessing the semantic similarity between sentences, either in the same or in different languages. In our system submission, we employ multilingual word representations, in which similar words in different languages are close to one another. Using such representations is advantageous, since the increasing amount of available parallel data allows for the application of such methods to many of the languages in the world. Hence, semantic similarity can be inferred even for languages for which no annotated data exists. Our system is trained and evaluated on all language pairs included in the shared task (English, Spanish, Arabic, and Turkish). Although development results are promising, our system does not yield high performance on the shared task test sets.", "ITNLP-AiKF at SemEval-2017 Task 1: Rich Features Based SVR for Semantic Textual Similarity Computing.\n\nSemantic Textual Similarity (STS) devotes to measuring the degree of equivalence in the underlying semantic of the sentence pair. We proposed a new system, ITNLP-AiKF, which applies in the SemEval 2017 Task1 Semantic Textual Similarity track 5 English monolingual pairs. In our system, rich features are involved, including Ontology based, word embedding based, Corpus based, Alignment based and Literal based feature. We leveraged the features to predict sentence pair similarity by a Support Vector Regression (SVR) model. In the result, a Pearson Correlation of 0.8231 is achieved by our system, which is a competitive result in the contest of this track.", "LSIS at SemEval-2017 Task 4: Using Adapted Sentiment Similarity Seed Words For English and Arabic Tweet Polarity Classification.\n\nWe present, in this paper, our contribution in SemEval2017 task 4 : ''Sentiment Analysis in Twitter'', subtask A: ''Message Polarity Classification'', for English and Arabic languages. Our system is based on a list of sentiment seed words adapted for tweets. The sentiment relations between seed words and other terms are captured by cosine similarity between the word embedding representations (word2vec). These seed words are extracted from datasets of annotated tweets available online. Our tests, using these seed words, show significant improvement in results compared to the use of Turney and Littman's (2003) seed words, on polarity classification of tweet messages.", "FuRongWang at SemEval-2017 Task 3: Deep Neural Networks for Selecting Relevant Answers in Community Question Answering.\n\nWe describes deep neural networks frameworks in this paper to address the community question answering (cQA) ranking task (SemEval-2017 task 3). Convolutional neural networks and bi-directional long-short term memory networks are applied in our methods to extract semantic information from questions and answers (comments). In addition, in order to take the full advantage of question-comment semantic relevance, we deploy interaction layer and augmented features before calculating the similarity. The results show that our methods have the great effectiveness for both subtask A and subtask C.", "LABDA at SemEval-2017 Task 10: Extracting Keyphrases from Scientific Publications by combining the BANNER tool and the UMLS Semantic Network.\n\nThis paper describes the system presented by the LABDA group at SemEval 2017 Task 10 ScienceIE, specifically for the subtasks of identification and classification of keyphrases from scientific articles. For the task of identification, we use the BANNER tool, a named entity recognition system, which is based on conditional random fields (CRF) and has obtained successful results in the biomedical domain. To classify keyphrases, we study the UMLS semantic network and propose a possible linking between the keyphrase types and the UMLS semantic groups. Based on this semantic linking, we create a dictionary for each keyphrase type. Then, a feature indicating if a token is found in one of these dictionaries is incorporated to feature set used by the BANNER tool. The final results on the test dataset show that our system still needs to be improved, but the conditional random fields and, consequently, the BANNER system can be used as a first approximation to identify and classify keyphrases.", "The NTNU System at SemEval-2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications Using Multiple Conditional Random Fields.\n\nThis study describes the design of the NTNU system for the ScienceIE task at the SemEval 2017 workshop. We use self-defined feature templates and multiple conditional random fields with extracted features to identify keyphrases along with categorized labels and their relations from scientific publications. A total of 16 teams participated in evaluation scenario 1 (subtasks A, B, and C), with only 7 teams competing in all sub-tasks. Our best micro-averaging F1 across the three subtasks is 0.23, ranking in the middle among all 16 submissions.", "FORGe at SemEval-2017 Task 9: Deep sentence generation based on a sequence of graph transducers.\n\nWe present the contribution of Universitat Pompeu Fabra's NLP group to the SemEval Task 9.2 (AMR-to-English Generation). The proposed generation pipeline comprises: (i) a series of rule-based graph-transducers for the syntacticization of the input graphs and the resolution of morphological agreements, and (ii) an off-the-shelf statistical linearization component.", "ELiRF-UPV at SemEval-2017 Task 4: Sentiment Analysis using Deep Learning.\n\nThis paper describes the participation of ELiRF-UPV team at task 4 of SemEval2017. Our approach is based on the use of convolutional and recurrent neural networks and the combination of general and specific word embeddings with polarity lexicons. We participated in all of the proposed subtasks both for English and Arabic languages using the same system with small variations.", "XJSA at SemEval-2017 Task 4: A Deep System for Sentiment Classification in Twitter.\n\nThis paper describes the XJSA System submission from XJTU. Our system was created for SemEval2017 Task 4 --- subtask A which is very popular and fundamental. The system is based on convolutional neural network and word embedding. We used two pre-trained word vectors and adopt a dynamic strategy for k-max pooling.", "Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model.\n\nThis paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.", "UPC-USMBA at SemEval-2017 Task 3: Combining multiple approaches for CQA for Arabic.\n\nThis paper presents a description of the participation of the UPC-USMBA team in the SemEval 2017 Task 3, subtask D, Arabic. Our approach for facing the task is based on a combination of a set of atomic classifiers. The atomic classifiers include lexical string based, based on vectorial representations and rulebased. Several combination approaches have been tried.", "Adullam at SemEval-2017 Task 4: Sentiment Analyzer Using Lexicon Integrated Convolutional Neural Networks with Attention.\n\nWe propose a sentiment analyzer for the prediction of document-level sentiments of English micro-blog messages from Twitter. The proposed method is based on lexicon integrated convolutional neural networks with attention (LCA). Its performance was evaluated using the datasets provided by SemEval competition (Task 4). The proposed sentiment analyzer obtained an average F1 of 55.2%, an average recall of 58.9% and an accuracy of 61.4%.", "EICA at SemEval-2017 Task 4: A Simple Convolutional Neural Network for Topic-based Sentiment Classification.\n\nThis paper describes our approach for SemEval-2017 Task 4 - Sentiment Analysis in Twitter (SAT). Its five subtasks are divided into two categories: (1) sentiment classification, i.e., predicting topic-based tweet sentiment polarity, and (2) sentiment quantification, that is, estimating the sentiment distributions of a set of given tweets. We build a convolutional sentence classification system for the task of SAT. Official results show that the experimental results of our system are comparative.", "RIGOTRIO at SemEval-2017 Task 9: Combining Machine Learning and Grammar Engineering for AMR Parsing and Generation.\n\nBy addressing both text-to-AMR parsing and AMR-to-text generation, SemEval-2017 Task 9 established AMR as a powerful semantic interlingua. We strengthen the interlingual aspect of AMR by applying the multilingual Grammatical Framework (GF) for AMR-to-text generation. Our current rule-based GF approach completely covered only 12.3% of the test AMRs, therefore we combined it with state-of-the-art JAMR Generator to see if the combination increases or decreases the overall performance. The combined system achieved the automatic BLEU score of 18.82 and the human Trueskill score of 107.2, to be compared to the plain JAMR Generator results. As for AMR parsing, we added NER extensions to our SemEval-2016 general-domain AMR parser to handle the biomedical genre, rich in organic compound names, achieving Smatch F1=54.0%.", "funSentiment at SemEval-2017 Task 4: Topic-Based Message Sentiment Classification by Exploiting  Word  Embeddings, Text Features and Target Contexts.\n\nThis paper describes the approach we used for SemEval-2017 Task 4: Sentiment Analysis in Twitter. Topic-based (target-dependent) sentiment analysis has become attractive and been used in some applications recently, but it is still a challenging research task. In our approach, we take the left and right context of a target into consideration when generating polarity classification features.  We use two types of word embeddings in our classifiers: the general word embeddings learned from 200 million tweets, and sentiment-specific word embeddings learned from 10 million tweets using distance supervision.  We also incorporate a text feature model in our algorithm. This model produces features based on text negation, tf.idf weighting scheme, and a Rocchio text classification method. We participated in four subtasks (B, C, D \\& E for English), all of which are about topic-based message polarity classification. Our team is ranked \\#6 in subtask B, \\#3 by MAEu and \\#9 by MAEm in subtask C, \\#3 using RAE and \\#6 using KLD in subtask D, and \\#3 in subtask E.", "KeLP at SemEval-2017 Task 3: Learning Pairwise Patterns in Community Question Answering.\n\nThis paper describes the KeLP system participating in the SemEval-2017 community Question Answering (cQA) task. The system is a refinement of the kernel-based sentence pair modeling we proposed for the previous year challenge. It is implemented within the Kernel-based Learning Platform called KeLP, from which we inherit the team's name. Our primary submission ranked first in subtask A, and third in subtasks B and C, being the only systems appearing in the top-3 ranking for all the English subtasks. This shows that the proposed framework, which has minor variations among the three subtasks, is extremely flexible and effective in tackling learning tasks defined on sentence pairs.", "ULISBOA at SemEval-2017 Task 12: Extraction and classification of temporal expressions and events.\n\nThis paper presents our approach to participate in the SemEval 2017 Task 12: Clinical TempEval challenge, specifically in the event and time expressions span and attribute identification subtasks (ES, EA, TS, TA). Our approach consisted in training Conditional Random Fields (CRF) classifiers using the provided annotations, and in creating manually curated rules to classify the attributes of each event and time expression. We used a set of common features for the event and time CRF classifiers, and a set of features specific to each type of entity, based on domain knowledge. Training only on the source domain data, our best F-scores were 0.683 and 0.485 for event and time span identification subtasks. When adding target domain annotations to the training data, the best F-scores obtained were 0.729 and 0.554, for the same subtasks. We obtained the second highest F-score of the challenge on the event polarity subtask (0.708). The source code of our system, Clinical Timeline Annotation (CiTA), is available at https://github.com/lasigeBioTM/CiTA.", "HCS at SemEval-2017 Task 5: Polarity detection in business news using convolutional neural networks.\n\nTask 5 of SemEval-2017 involves fine-grained sentiment analysis on financial microblogs and news.  Our solution for determining the sentiment score extends an earlier convolutional neural network for sentiment analysis in several ways. We explicitly encode a focus on a particular company, we apply a data augmentation   scheme, and use a larger data collection to complement the small training data provided by the task organizers.                          The best results were achieved by training a model on an external dataset and then tuning it using the provided training dataset.", "DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis.\n\nIn this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 ``Sentiment Analysis in Twitter''. We participated in all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification and quantification. We use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embeddings pre-trained on a big collection of Twitter messages. Also, we present a text processing tool suitable for social network messages, which performs tokenization, word normalization, segmentation and spell correction. Moreover, our approach uses no hand-crafted features or sentiment lexicons. We ranked 1st (tie) in Subtask A, and achieved very competitive results in the rest of the Subtasks. Both the word embeddings and our text processing tool are available to the research community.", "NLG301 at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News.\n\nShort length, multi-targets, target relation-ship, monetary expressions, and outside reference are characteristics of financial tweets. This paper proposes methods to extract target spans from a tweet and its referencing web page. Total 15 publicly available sentiment dictionaries and one sentiment dictionary constructed from training set, containing sentiment scores in binary or real numbers, are used to compute the sentiment scores of text spans. Moreover, the correlation coeffi-cients of the price return between any two stocks are learned with the price data from Bloomberg. They are used to capture the relationships between the interesting tar-get and other stocks mentioned in a tweet. The best result of our method in both sub-task are 56.68% and 55.43%, evaluated by evaluation method 2.", "MayoNLP at SemEval 2017 Task 10: Word Embedding Distance Pattern for Keyphrase Classification in Scientific Publications.\n\nIn this paper, we present MayoNLP's results from the participation in the ScienceIE share task at SemEval 2017. We focused on the keyphrase classification task (Subtask B). We explored semantic similarities and patterns of keyphrases in scientific publications using pre-trained word embedding models. Word Embedding Distance Pattern, which uses the head noun word embedding to generate distance patterns based on labeled keyphrases, is proposed as an incremental feature set to enhance the conventional Named Entity Recognition feature sets.  Support vector machine is used as the supervised classifier for keyphrase classification. Our system achieved an overall F1 score of 0.67 for keyphrase classification and 0.64 for keyphrase classification and relation detection.", "Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings.\n\nWe introduce a simple and effective method to learn discourse-specific word embeddings (DSWE) for implicit discourse relation recognition. Specifically, DSWE is learned by performing connective classification on massive explicit discourse data, and capable of capturing discourse relationships between words. On the PDTB data set, using DSWE as features achieves significant improvements over baselines.", "AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine.\n\nWe propose AliMe Chat, an open-domain chatbot engine that integrates the joint results of Information Retrieval (IR) and Sequence to Sequence (Seq2Seq) based generation models. AliMe Chat uses an attentive Seq2Seq based rerank model to optimize the joint results. Extensive experiments show our engine outperforms both IR and generation based models. We launch AliMe Chat for a real-world industrial application and observe better results than another public chatbot.", "English Event Detection With Translated Language Features.\n\nWe propose novel radical features from automatic translation for event extraction. Event detection is a complex language processing task for which it is expensive to collect training data, making generalisation challenging. We derive meaningful subword features from automatic translations into target language. Results suggest this method is particularly useful when using languages with writing systems that facilitate easy decomposition into subword features, e.g., logograms and Cangjie. The best result combines logogram features from Chinese and Japanese with syllable features from Korean, providing an additional 3.0 points f-score when added to state-of-the-art generalisation features on the TAC KBP 2015 Event Nugget task.", "Neural Architectures for Multilingual Semantic Parsing.\n\nIn this paper, we address semantic parsing in a multilingual context. We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations. We extend an existing sequence-to-tree model to a multi-task learning framework which shares the decoder for generating semantic representations. We report evaluation results on the multilingual GeoQuery corpus and introduce a new multilingual version of the ATIS corpus.", "Salience Rank: Efficient Keyphrase Extraction with Topic Modeling.\n\nTopical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents. The ranking procedure consists of running PageRank K times, where K is the number of topics used in the LDA model. In this paper, we propose a modification of TPR, called Salience Rank. Salience Rank only needs to run PageRank once and extracts comparable or better keyphrases on benchmark datasets. In addition to quality and efficiency benefit, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity.", "List-only Entity Linking.\n\nTraditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as seed mentions and disambiguate other mentions by comparing them with the seed mentions rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.", "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization.\n\nA fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.", "An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation.\n\nIn this paper, we propose a novel domain adaptation method named ``mixed fine tuning'' for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings.", "Implicitly-Defined Neural Networks for Sequence Labeling.\n\nIn this work, we propose a novel, implicitly-defined neural network architecture and describe a method to compute its components. The proposed architecture forgoes the causality assumption used to formulate recurrent neural networks and instead couples the hidden states of the network, allowing improvement on problems with complex, long-distance dependencies. Initial experiments demonstrate the new architecture outperforms both the Stanford Parser and baseline bidirectional networks on the Penn Treebank Part-of-Speech tagging task and a baseline bidirectional network on an additional artificial random biased walk task.", "Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary.\n\nWe speed up Neural Machine Translation (NMT) decoding by shrinking run-time target vocabulary. We experiment with two shrinking approaches: Locality Sensitive Hashing (LSH) and word alignments. Using the latter method, we get a 2x overall speed-up over a highly-optimized GPU implementation, without hurting BLEU. On certain low-resource language pairs, the same methods improve BLEU by 0.5 points. We also report a negative result for LSH on GPUs, due to relatively large overhead, though it was successful on CPUs. Compared with Locality Sensitive Hashing (LSH), decoding with word alignments is GPU-friendly, orthogonal to existing speedup methods and more robust across language pairs.", "Fast and Accurate Neural Word Segmentation for Chinese.\n\nNeural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. In this paper, we propose a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.", "An Analysis of Action Recognition Datasets for Language and Vision Tasks.\n\nA large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval. In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages. We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.", "Evaluating Compound Splitters Extrinsically with Textual Entailment.\n\nTraditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel task that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by taskinternal mechanisms in statistical machine translation. We show significant improvements using different compound splitting methods on a German textual entailment dataset.", "Lifelong Learning CRF for Supervised Aspect Extraction.\n\nThis paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.", "Learning Lexico-Functional Patterns for First-Person Affect.\n\nInformal first-person narratives are a unique resource for computational mod- els of everyday events and people's affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate's arguments. We present a method to learn proxies for these functions from first- person narratives. We construct a novel fine-grained test set, and show that the pat- terns we learn improve our ability to pre- dict first-person affective reactions to everyday events, from a Stanford sentiment baseline of .67F to .75F.", "Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages.\n\nWe present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters. The character composition model shows great improvement over the word-lookup model, especially for parsing agglutinative languages. These improvements are even better than using pre-trained word embeddings from extra data. On the SPMRL data sets, our system outperforms the previous best greedy parser (Ballesteros et. al, 2015) by a margin of 3% on average.", "A Generative Parser with a Discriminative Recognition Algorithm.\n\nGenerative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single- model language modeling score.", "Sentence Embedding for Neural Machine Translation Domain Adaptation.\n\nAlthough new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only focus on phrase-based machine translation. In this paper, we exploit the NMT's internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.", "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset.\n\nIn recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.", "Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings.\n\nThis paper introduces the concept of temporal word analogies: pairs of words which occupy the same semantic space at different points in time. One well-known property of word embeddings is that they are able to effectively model traditional word analogies (``word w1 is to word w2 as word w3 is to word w4'') through vector addition. Here, I show that temporal word analogies (``word w1 at time t\u03b1 is like word w2 at time t\u03b2'') can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space. When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as ``Ronald Reagan in 1987 is like Bill Clinton in 1997'', or ``Walkman in 1987 is like iPod in 2007''.", "Feature Hashing for Language and Dialect Identification.\n\nWe evaluate feature hashing for language identification (LID), a method not previously used for this task. Using a standard dataset, we first show that while feature performance is high, LID data is highly dimensional and mostly sparse (>99.5%) as it includes large vocabularies for many languages; memory requirements grow as languages are added. Next we apply hashing using various hash sizes, demonstrating that there is no performance loss with dimensionality reductions of up to 86%. We also show that using an ensemble of low-dimension hash-based classifiers further boosts performance. Feature hashing is highly useful for LID and holds great promise for future work in this area.", "English Multiword Expression-aware Dependency Parsing Including Named Entities.\n\nBecause syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system. In this work, we construct a corpus that ensures consistency between dependency structures and MWEs, including named entities. Further, we explore models that predict both MWE-spans and an MWE-aware dependency structure. Experimental results show that our joint model using additional MWE-span features achieves an MWE recognition improvement of 1.35 points over a pipeline model.", "Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings.\n\nWe propose a new method for extracting pseudo-parallel sentences from a pair of large monolingual corpora, without relying on any document-level information. Our method first exploits word embeddings in order to efficiently evaluate trillions of candidate sentence pairs and then a classifier to find the most reliable ones. We report significant improvements in domain adaptation for statistical machine translation when using a translation model trained on the sentence pairs extracted from in-domain monolingual corpora.", "How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models.\n\nGenerative conversational systems are attracting increasing attention in natural language processing (NLP). Recently, researchers have noticed the importance of context information in dialog processing, and built various models to utilize context. However, there is no systematic comparison to analyze how to use context effectively. In this paper, we conduct an empirical study to compare various models and investigate the effect of context information in dialog systems. We also propose a variant that explicitly weights context vectors by context-query relevance, outperforming the other baselines.", "Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model.\n\nThis paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis. The LSTM language model scores, along with other features, are used in a MaxEnt reranker to identify the most plausible analysis. We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the state-of-the-art in disfluency detection.", "EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text.\n\nParallel corpora are widely used in a variety of Natural Language Processing tasks, from Machine Translation to cross-lingual Word Sense Disambiguation, where parallel sentences can be exploited to automatically generate high-quality sense annotations on a large scale. In this paper we present EuroSense, a multilingual sense-annotated resource based on the joint disambiguation of the Europarl parallel corpus, with almost 123 million sense annotations for over 155 thousand distinct concepts and entities from a language-independent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation.", "How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers.\n\nIn dependency parsing, jackknifing taggers is indiscriminately used as a simple adaptation strategy. Here, we empirically evaluate when and how (not) to use jackknifing in parsing. On 26 languages, we reveal a preference that conflicts with, and surpasses the ubiquitous ten-folding. We show no clear benefits of tagging the training data in cross-lingual parsing.", "A Network Framework for Noisy Label Aggregation in Social Media.\n\nThis paper focuses on the task of noisy label aggregation in social media, where users with different social or culture backgrounds may annotate invalid or malicious tags for documents. To aggregate noisy labels at a small cost, a network framework is proposed by calculating the matching degree of a document's topics and the annotators' meta-data. Unlike using the back-propagation algorithm, a probabilistic inference approach is adopted to estimate network parameters. Finally, a new simulation method is designed for validating the effectiveness of the proposed framework in aggregating noisy labels.", "Neural Semantic Parsing over Multiple Knowledge-bases.\n\nA fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-of-the-art performance on the Overnight dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.", "Sentence Alignment Methods for Improving Text Simplification Systems.\n\nWe provide several methods for sentence-alignment of texts with different complexity levels. Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems. We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems.", "A Generative Attentional Neural Network Model for Dialogue Act Classification.\n\nWe propose a novel generative neural network architecture for Dialogue Act classification. Building upon the Recurrent Neural Network framework, our model incorporates a novel attentional technique and a label to label connection for sequence learning, akin to Hidden Markov Models. The experiments show that both of these innovations lead  our model to outperform strong baselines for dialogue act classification on MapTask and Switchboard corpora. We further empirically analyse the effectiveness of each of the new innovations.", "Improving Semantic Composition with Offset Inference.\n\nCount-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition.", "Bootstrapping for Numerical Open IE.\n\nWe design and release BONIE, the first open numerical relation extractor, for extracting Open IE tuples where one of the arguments is a number or a quantity-unit phrase. BONIE uses bootstrapping to learn the specific dependency patterns that express numerical relations in a sentence. BONIE's novelty lies in task-specific customizations, such as inferring implicit relations, which are clear due to context such as units (for e.g., \u2018square kilometers' suggests area, even if the word \u2018area' is missing in the sentence). BONIE obtains 1.5x yield and 15 point precision gain on numerical facts over a state-of-the-art Open IE system.", "Methodical Evaluation of Arabic Word Embeddings.\n\nMany unsupervised learning techniques have been proposed to obtain meaningful representations of words from text. In this study, we evaluate these various techniques when used to generate Arabic word embeddings. We first build a benchmark for the Arabic language that can be utilized to perform intrinsic evaluation of different word embeddings. We then perform additional extrinsic evaluations of the embeddings based on two NLP tasks.", "Lexical Features in Coreference Resolution: To be Used With Caution.\n\nLexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.", "Neural System Combination for Machine Translation.\n\nNeural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chinese-to-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.", "AMR-to-text Generation with Synchronous Node Replacement Grammar.\n\nThis paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result.", "Incorporating Uncertainty into Deep Learning for Spoken Language Assessment.\n\nThere is a growing demand for automatic assessment of spoken English proficiency. These systems need to handle large variations in input data owing to the wide range of candidate skill levels and L1s, and errors from ASR. Some candidates will be a poor match to the training data set, undermining the validity of the predicted grade. For high stakes tests it is essential for such systems not only to grade well, but also to provide a measure of their uncertainty in their predictions, enabling rejection to human graders. Previous work examined Gaussian Process (GP) graders which, though successful, do not scale well with large data sets. Deep Neural Network (DNN) may also be used to provide uncertainty using Monte-Carlo Dropout (MCD). This paper proposes a novel method to yield uncertainty and compares it to GPs and DNNs with MCD. The proposed approach explicitly teaches a DNN to have low uncertainty on training data and high uncertainty on generated artificial data. On experiments conducted on data from the Business Language Testing Service (BULATS), the proposed approach is found to outperform GPs and DNNs with MCD in uncertainty-based rejection whilst achieving comparable grading performance.", "Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization.\n\nCurrent Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.", "Argumentation Quality Assessment: Theory vs. Practice.\n\nArgumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other.", "Cross-lingual and cross-domain discourse segmentation of entire documents.\n\nDiscourse segmentation is a crucial step in building end-to-end discourse parsers. However, discourse segmenters only exist for a few languages and domains. Typically they only  detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains. In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold pre-annotations.  We also consider the problem of learning discourse segmenters when no labeled data is available for a language. Our fully supervised system obtains 89.5% F1 for English newswire, with slight drops in performance on other domains, and we report supervised and unsupervised (cross-lingual) results for five languages in total.", "The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective.\n\nThis study explores the role of speech register and prosody for the task of word segmentation. Since these two factors are thought to play an important role in early language acquisition, we aim to quantify their contribution for this task. We study a Japanese corpus containing both infant- and adult-directed speech and we apply four different word segmentation models, with and without knowledge of prosodic boundaries. The results showed that the difference between registers is smaller than previously reported and that prosodic boundary information helps more adult- than infant-directed speech.", "Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation.\n\nRecently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks. Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods. This work proposes a direct HMM with neural network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm. The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0% Bleu scores on two different translation tasks.", "Learning Topic-Sensitive Word Representations.\n\nDistributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word. We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document  we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.", "Data Augmentation for Low-Resource Neural Machine Translation.\n\nThe quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.", "A Conditional Variational Framework for Dialog Generation.\n\nDeep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.", "Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets.\n\nPrevious models for the assessment of commitment towards a predicate in a sentence (also known as factuality prediction) were trained and tested against a specific annotated dataset, subsequently limiting the generality of their results. In this work we propose an intuitive method for mapping three previously annotated corpora onto a single factuality scale, thereby enabling models to be tested across these corpora. In addition, we design a novel model for factuality prediction by first extending a previous rule-based factuality prediction system and applying it over an abstraction of dependency trees, and then using the output of this system in a supervised classifier. We show that this model outperforms previous methods on all three datasets. We make both the unified factuality corpus and our new model publicly available.", "Cardinal Virtues: Extracting Relation Cardinalities from Text.\n\nInformation extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect recall. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel problem of extracting cardinalities and discusses the specific challenges that set it apart from standard IE. We present a distant supervision method using conditional random fields. A preliminary evaluation results in precision between 3% and 55%, depending on the difficulty of relations.", "A Two-Stage Parsing Method for Text-Level Discourse Analysis.\n\nPrevious work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and relation), but did not achieve satisfactory performance. In this paper, we propose that transition-based model is more appropriate for parsing the naked discourse tree (i.e., identifying span and nuclearity) due to data sparsity. At the same time, we argue that relation labeling can benefit from naked tree structure and should be treated elaborately with consideration of three kinds of relations including within-sentence, across-sentence and across-paragraph relations. Thus, we design a pipelined two-stage parsing method for generating an RST tree from text. Experimental results show that our method achieves state-of-the-art performance, especially on span and nuclearity identification.", "A Deep Network with Visual Text Composition Behavior.\n\nWhile natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.", "Fine-Grained Entity Typing with High-Multiplicity Assignments.\n\nAs entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.", "Learning to Parse and Translate Improves Neural Machine Translation.\n\nThere has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.", "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection.\n\nLinguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.", "''Liar, Liar Pants on Fire'': A New Benchmark Dataset for Fake News Detection.\n\nAutomatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.", "Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation.\n\nRating scales are a widely used method for data annotation; however, they present several challenges, such as difficulty in maintaining inter- and intra-annotator consistency. Best---worst scaling (BWS) is an alternative method of annotation that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of rating scales. However, the veracity of this claim has never been systematically established. Here for the first time, we set up an experiment that directly compares the rating scale method with BWS. We show that with the same total number of annotations, BWS produces significantly more reliable results than the rating scale.", "Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging.\n\nWord segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation us- ing: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.", "Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function.\n\nIn this paper we define a measure of dependency between two random variables, based on the Jensen-Shannon (JS) divergence between their joint distribution and the product of their marginal distributions. Then, we show that word2vec's skip-gram with negative sampling embedding algorithm finds the optimal low-dimensional approximation of this JS dependency measure between the words and their contexts. The gap between the optimal score and the low-dimensional approximation is demonstrated on a standard text corpus.", "A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments.\n\nWe present a new framework for evaluating extractive summarizers, which is based on a principled representation as optimization problem. We prove that every extractive summarizer can be decomposed into an objective function  and an optimization technique. We perform a comparative analysis and evaluation of several objective functions embedded in well-known summarizers regarding their correlation with human judgments. Our comparison of these correlations across two datasets yields surprising insights into the role and performance of objective functions in the different  summarizers.", "Error-repair Dependency Parsing for Ungrammatical Texts.\n\nWe propose a new dependency parsing scheme which jointly parses a sentence and repairs grammatical errors by extending the non-directional transition-based formalism of Goldberg and Elhadad (2010) with three additional actions: SUBSTITUTE, DELETE, INSERT. Because these actions may cause an infinite loop in derivation, we also introduce simple constraints that ensure the parser termination. We evaluate our model with respect to dependency accuracy and grammaticality improvements for ungrammatical sentences, demonstrating the robustness and applicability of our scheme.", "Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?.\n\nAutomatic identification of good arguments on a controversial topic has applications in civics and education, to name a few. While in the civics context it might be acceptable to create separate models for  each topic, in the context of              scoring of students' writing there is a preference for a single model that applies to all responses. Given that good arguments for one topic are likely to be irrelevant for another, is a single model for detecting good arguments a contradiction in terms? We investigate the extent to which it is possible to close the performance gap between topic-specific and across-topics models for identification of good arguments.", "A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations.\n\nWe introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.", "Multi-Task Learning of Keyphrase Boundary Classification.\n\nKeyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.", "Temporal Orientation of Tweets for Predicting Income of Users.\n\nAutomatically estimating a user's socio-economic profile from their language use in social media can significantly help social science research and various downstream applications ranging from business to politics. The current paper presents the first study where user cognitive structure is used to build a predictive model of income. In particular, we first develop a classifier using a weakly supervised learning framework to automatically time-tag tweets as past, present, or future. We quantify a user's overall temporal orientation based on their distribution of tweets, and use it to build a predictive model of income. Our analysis uncovers a correlation between future temporal orientation and income. Finally, we measure the predictive power of future temporal orientation on income by performing regression.", "Representing Sentences as Low-Rank Subspaces.\n\nSentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences -- the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models,  including skip-thought vectors, by 15% on average.", "Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework.\n\nThe availability of the Rhetorical Structure Theory (RST) Discourse Treebank has spurred substantial research into discourse analysis of written texts; however, limited research has been conducted to date on RST annotation and parsing of spoken language, in particular, non-native spontaneous speech. Considering that the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of spoken language, we initiated a research effort to obtain RST annotations of a large number of non-native spoken responses from a standardized assessment of academic English proficiency. The resulting inter-annotator kappa agreements on the three different levels of Span, Nuclearity, and Relation are 0.848, 0.766, and 0.653, respectively. Furthermore, a set of features was explored to evaluate the discourse structure of non-native spontaneous speech based on these annotations; the highest performing feature resulted in a correlation of 0.612 with scores of discourse coherence provided by expert human raters.", "Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative Containers.\n\nWe present a neural architecture for containment relation identification between medical events and/or temporal expressions. We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus. Our model achieves an F-measure of 0.613 and outperforms the best result reported on this corpus to date.", "On the Distribution of Lexical Features at Multiple Levels of Analysis.\n\nNatural language processing has increasingly moved from modeling documents and words toward studying the people behind the language. This move to working with data at the user or community level has presented the field with different characteristics of linguistic data. In this paper, we empirically characterize various lexical distributions at different levels of analysis, showing that, while most features are decidedly sparse and non-normal at the message-level (as with traditional NLP), they follow the central limit theorem to become much more Log-normal or even Normal at the user- and county-levels. Finally, we demonstrate that modeling lexical features for the correct level of analysis leads to marked improvements in common social scientific prediction tasks.", "Recognizing Counterfactual Thinking in Social Media Texts.\n\nCounterfactual statements, describing events that did not occur and their consequents, have been studied in areas including problem-solving, affect management, and behavior regulation. People with more counterfactual thinking tend to perceive life events as more personally meaningful. Nevertheless, counterfactuals have not been studied in computational linguistics. We create a counterfactual tweet dataset and explore approaches for detecting counterfactuals using rule-based and supervised statistical approaches. A combined rule-based and statistical approach yielded the best results (F1 = 0.77) outperforming either approach used alone.", "Self-Crowdsourcing Training for Relation Extraction.\n\nIn this paper we introduce a self-training strategy for crowdsourcing. The training examples are automatically selected to train the crowd workers. Our experimental results show an impact of 5% Improvement in terms of F1 for relation extraction task, compared to the method based on distant supervision.", "Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter.\n\nPew research polls report 62 percent of U.S. adults get news on social media (Gottfried and Shearer, 2016). In a December poll, 64 percent of U.S. adults said that ``made-up news'' has caused a ``great deal of confusion'' about the facts of current events (Barthel et al., 2016). Fabricated stories in social media, ranging from deliberate propaganda to hoaxes and satire, contributes to this confusion in addition to having serious effects on global stability. In this work we build predictive models to classify 130 thousand news posts as suspicious or verified, and predict four sub-types of suspicious news --- satire, hoaxes, clickbait and propaganda. We show that neural network models trained on tweet content and social network interactions outperform lexical models. Unlike previous work on deception detection, we find that adding syntax and grammar features to our models does not improve performance. Incorporating linguistic features improves classification results, however, social interaction features are most in- formative for finer-grained separation be- tween four types of suspicious news posts.", "Improving Native Language Identification by Using Spelling Errors.\n\nIn this paper, we explore spelling errors as a source of information for detecting the native language of a writer, a previously under-explored area. We note that character n-grams from misspelled words are very indicative of the native language of the author. In combination with other lexical features, spelling error features lead to 1.2\\% improvement in accuracy on classifying texts in the TOEFL11 corpus by the author's native language, compared to systems participating in the NLI shared task.", "Arc-swift: A Novel Transition System for Dependency Parsing.\n\nTransition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments. Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation. This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition. This allows the parser to leverage lexical information more directly in transition decisions. Hence, arc-swift can achieve significantly better performance with a very small beam size. Our parsers reduce error by 3.7--7.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.", "Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary.\n\nCross-lingual model transfer is a compelling and popular method for predicting annotations in a low-resource language, whereby parallel corpora provide a bridge to a high-resource language, and its associated annotated corpora. However, parallel data is not readily available for many languages, limiting the applicability of these approaches. We address these drawbacks in our framework which takes advantage of cross-lingual word embeddings trained solely on a high coverage dictionary. We propose a novel neural network model for joint training from both sources of data based on cross-lingual word embeddings, and show substantial empirical improvements over baseline techniques. We also propose several active learning heuristics, which result in improvements over competitive benchmark methods.", "Alternative Objective Functions for Training MT Evaluation Metrics.\n\nMT evaluation metrics are tested for correlation with human judgments either at the sentence- or the corpus-level. Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentence-level exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than--and on average outperforms--both models on both objectives.", "Understanding and Detecting Diverse Supporting Arguments on Controversial Issues.\n\nWe investigate the problem of sentence-level supporting argument detection from relevant documents for user-specified claims. A dataset containing claims and associated citation articles is collected from online debate website idebate.org. We then manually label sentence-level supporting arguments from the documents along with their types as study, factual, opinion, or reasoning. We further characterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information.", "Pocket Knowledge Base Population.\n\nExisting Knowledge Base Population methods extract relations from a closed relational schema with limited coverage leading to sparse KBs. We propose Pocket Knowledge Base Population (PKBP), the task of dynamically constructing a KB of entities related to a query and finding the best characterization of relationships between entities. We describe novel Open Information Extraction methods which leverage the PKB to find informative trigger words. We evaluate using existing KBP shared-task data as well anew annotations collected for this work. Our methods produce high quality KB from just text with many more entities and relationships than existing KBP systems.", "A Neural Model for User Geolocation and Lexical Dialectology.\n\nWe propose a simple yet effective text-based user geolocation model based on a neural network with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word                                and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms. As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods.", "Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM.\n\nSelecting appropriate words to compose a sentence is one common problem faced by non-native Chinese learners. In this paper, we propose (bidirectional) LSTM sequence labeling models and explore various features to detect word usage errors in Chinese sentences. By combining CWINDOW word embedding features and POS information, the best bidirectional LSTM model achieves accuracy 0.5138 and MRR 0.6789 on the HSK dataset. For 80.79% of the test data, the model ranks the ground-truth within the top two at position level.", "Answering Complex Questions Using Open Information Extraction.\n\nWhile there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.", "On the Equivalence of Holographic and Complex Embeddings for Link Prediction.\n\nWe show the equivalence of two state-of-the-art models for link prediction/knowledge graph completion: Nickel et al's holographic embeddings and Trouillon et al.'s complex embeddings. We first consider a spectral version of the holographic embeddings, exploiting the frequency domain in the Fourier transform for efficient computation. The analysis of the resulting model reveals that it can be viewed as an instance of the complex embeddings with a certain constraint imposed on the initial vectors upon training. Conversely, any set of complex embeddings can be converted to a set of equivalent holographic embeddings.", "Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast.\n\nPeople around the globe respond to major real world events through social media.              To study targeted public sentiments across many languages and geographic locations, we introduce multilingual connotation frames: an extension from English connotation frames of Rashkin et al. (2016) with 10 additional European languages, focusing on the implied sentiments among event participants engaged in a frame. As a case study, we present large scale analysis on targeted public sentiments toward salient events and entities using 1.2 million multilingual connotation frames extracted from Twitter.", "A Corpus of Natural Language for Visual Reasoning.\n\nWe present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.", "Character-Aware Neural Morphological Disambiguation.\n\nWe develop a language-independent, deep learning-based approach to the task of morphological disambiguation. Guided by the intuition that the correct analysis should be ``most similar'' to the context, we propose dense representations for morphological analyses and surface context and a simple yet effective way of combining the two to perform disambiguation. Our approach improves on the language-dependent state of the art for two agglutinative languages (Turkish and Kazakh) and can be potentially applied to other morphologically complex languages.", "Chunk-Based Bi-Scale Decoder for Neural Machine Translation.\n\nIn typical neural machine translation~(NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.", "Vector space models for evaluating semantic fluency in autism.\n\nA common test administered during neurological examination is the semantic fluency test, in which the patient must list as many examples of a given semantic category as possible under timed conditions. Poor performance is associated with neurological conditions characterized by impairments in executive function, such as dementia, schizophrenia, and autism spectrum disorder (ASD). Methods for analyzing semantic fluency responses at the level of detail necessary to uncover these differences have typically relied on subjective manual annotation. In this paper, we explore automated approaches for scoring semantic fluency responses that leverage ontological resources and distributional semantic models to characterize the semantic fluency responses produced by young children with and without ASD. Using these methods, we find significant differences in the semantic fluency responses of children with ASD, demonstrating the utility of using objective methods for clinical language analysis. \\end{abstract}", "Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths.\n\nTemporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a ``common root'' assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.).", "Differentiable Scheduled Sampling for Credit Assignment.\n\nWe demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding in sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure--a well-known technique for correcting exposure bias--we introduce a new training objective that is continuous and differentiable everywhere and can provide informative gradients near points where previous decoding decisions change their value. By using a related approximation, we also demonstrate  a similar approach to sampled-based training. We show that our approach outperforms both standard cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.", "Question Answering through Transfer Learning from Large Fine-grained Supervision Data.\n\nWe show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure  achieves  the state of the art on an entailment task.", "Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks.\n\nExisting question answering methods infer answers either from a knowledge base or from raw text. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, web text contains millions of facts that are absent in the KB, however in an unstructured form. Universal schema can support reasoning on the union of both structured KBs and unstructured text by aligning them in a common embedded space. In this paper we extend universal schema to natural language question answering, employing Memory networks to attend to the large body of facts in the combination of text and KB. Our models can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on Spades fill-in-the-blank question answering dataset show that exploiting universal schema for question answering is better than using either a KB or text alone. This model also outperforms the current state-of-the-art by 8.5 F1 points.", "Oracle Summaries of Compressive Summarization.\n\nThis paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summarization paradigm in terms of ROUGE. The oracle summary is essential to reveal the upper bound performance of the paradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising  paradigm and encourage us to continue with the research to produce informative summaries.", "Automatic Compositor Attribution in the First Folio of Shakespeare.\n\nCompositor attribution, the clustering of pages in a historical printed document by the individual who set the type, is a bibliographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page. In this paper, we introduce a novel unsupervised model that jointly describes the textual and visual features needed to distinguish compositors. Applied to images of Shakespeare's First Folio, our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87%, even on text that is the output of OCR.", "Determining Whether and When People Participate in the Events They Tweet About.\n\nThis paper describes an approach to determine whether people participate in the events they tweet about. Specifically, we determine whether people are participants in events with respect to the tweet timestamp. We target all events expressed by verbs in tweets, including past, present and events that may occur in the future. We present new annotations using 1,096 event mentions, and experimental results showing that the task is challenging.", "Group Sparse CNNs for Question Classification with Answer Sets.\n\nQuestion classification is an important task with wide applications. However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data. In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders which refine question representation by utilizing group information in the answer set. We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse autoencoders into traditional CNNs. The proposed model significantly outperform strong baselines on four datasets.", "Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning.\n\nTwitter should be an ideal place to get a fresh read on how different issues are playing with the public, one that's potentially more reflective of democracy in this new media age than traditional polls. Pollsters typically ask people a fixed set of questions, while in social media people use their own voices to speak about whatever is on their minds. However, the demographic distribution of users on Twitter is not representative of the general population. In this paper, we present a demographic classifier for gender, age, political orientation and location on Twitter. We collected and curated a robust Twitter demographic dataset for this task. Our classifier uses a deep multi-modal multi-task learning architecture to reach a state-of-the-art performance, achieving an F1-score of 0.89, 0.82, 0.86, and 0.68 for gender, age, political orientation, and location respectively.", "Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task.\n\nWe consider the ROC story cloze task (Mostafazadeh et al., 2016) and present several findings. We develop a model that uses hierarchical recurrent networks with attention to encode the sentences in the story and score candidate endings. By discarding the large training set and only training on the validation set, we achieve an accuracy of 74.7%. Even when we discard the story plots (sentences before the ending) and only train to choose the better of two endings, we can still reach 72.5%. We then analyze this ``ending-only'' task setting. We estimate human accuracy to be 78% and find several types of clues that lead to this high accuracy, including those related to sentiment, negation, and general ending likelihood regardless of the story context.", "Exploring Neural Text Simplification Models.\n\nWe present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect                          grammaticality and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems", "EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering.\n\nA critical task for question answering is the final answer selection stage, which has to combine multiple signals available about each answer candidate. This paper proposes EviNets: a novel neural network architecture for factoid question answering. EviNets scores candidate answer entities by combining the available supporting evidence, e.g., structured knowledge bases and unstructured text documents. EviNets represents each piece of evidence with a dense embeddings vector, scores their relevance to the question, and aggregates the support for each candidate to predict their final scores. Each of the components is generic and allows plugging in a variety of models for semantic similarity scoring and information aggregation. We demonstrate the effectiveness of EviNets in experiments on the existing TREC QA and WikiMovies benchmarks, and on the new Yahoo! Answers dataset introduced in this paper. EviNets can be extended to other information types and could facilitate future work on combining evidence signals for joint reasoning in question answering.", "On the Challenges of Translating NLP Research into Commercial Products.\n\nThis paper highlights challenges in industrial research related to translating research in natural language processing into commercial products. While the interest in natural language processing from industry is significant, the transfer of research to commercial products is non-trivial and its challenges are often unknown to or underestimated by many researchers. I discuss current obstacles and provide suggestions for increasing the chances for translating research to commercial success based on my experience in industrial research.", "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes.\n\nWe propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and  message introduced by an user. These two modalities are used to train  an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.", "Incorporating Dialectal Variability for Socially Equitable Language Identification.\n\nLanguage identification (LID) is a critical first step for processing multilingual text.  Yet most LID systems are not designed to handle the linguistic diversity of global platforms like Twitter, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers.  We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our model achieves state-of-the-art performance on multiple LID benchmarks.  Furthermore, in a case study using Twitter for health tracking,  our method substantially increases the availability of texts written by underrepresented populations, enabling the development of ``socially inclusive'' NLP tools.", "Japanese Sentence Compression with a Large Training Dataset.\n\nIn English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network.", "Parser Adaptation for Social Media by Integrating Normalization.\n\nThis work explores different approaches of using normalization for parser adaptation.  Traditionally, normalization is used as separate pre-processing step. We show that integrating the normalization model into the parsing algorithm is more beneficial. This way, multiple normalization candidates can be leveraged, which improves parsing performance on social media. We test this hypothesis by modifying the Berkeley parser; out-of-the-box it achieves an F1 score of 66.52.                          Our integrated approach reaches a significant improvement with an F1 score of 67.36, while using the best normalization sequence results in an F1 score of only 66.94.", "Towards String-To-Tree Neural Machine Translation.\n\nWe present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.", "Demographic Inference on Twitter using Recursive Neural Networks.\n\nIn social media, demographic inference is a critical task in order to gain a better understanding of a cohort and to facilitate interacting with one's audience. Most previous work has made independence assumptions over topological, textual and label information on social networks. In this work, we employ recursive neural networks to break down these independence assumptions to obtain inference about demographic characteristics on Twitter. We show that our model performs better than existing models including the state-of-the-art.", "Feature-Rich Networks for Knowledge Base Completion.\n\nWe propose jointly modelling Knowledge Bases and aligned text with Feature-Rich Networks. Our models perform Knowledge Base Completion by learning to represent and compose diverse feature types from partially aligned and noisy resources. We perform experiments on Freebase utilizing additional entity type information and syntactic textual relations. Our evaluation suggests that the proposed models can better incorporate side information than previously proposed combinations of bilinear models with convolutional neural networks, showing large improvements when scoring the plausibility of unobserved facts with associated textual mentions.", "Attention Strategies for Multi-Source Sequence-to-Sequence Learning.\n\nModeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.", "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects.\n\nRecent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models.  We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects.  Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.", "Computational Characterization of Mental States: A Natural Language Processing Approach.\n\nPsychiatry is an area of medicine that strongly bases its diagnoses on the psychiatrist's subjective appreciation. More precisely, speech is used almost exclusively as a window to the patient's mind. Few other cues are available to objectively justify a diagnostic, unlike what happens in other disciplines which count on laboratory tests or imaging procedures, such as X-rays. Daily practice is based on the use of semi-structured interviews and standardized tests to build the diagnoses, heavily relying on her personal experience. This methodology has a big problem: diagnoses are commonly validated a posteriori in function of how the pharmacological treatment works. This validation cannot be done until months after the start of the treatment and, if the patient condition does not improve, the psychiatrist often changes the diagnosis and along with the pharmacological treatment. This delay prolongs the patient's suffering until the correct diagnosis is found. According to NIMH, more than 1% and 2% of US population is affected by Schizophrenia and Bipolar Disorder, respectively. Moreover, the WHO reported that the global cost of mental illness reached $2.5T in 2010 [1] . The task of diagnosis, largely simplified, mainly consists of understanding the mind state through the extraction of patterns from the patient's speech and finding the best matching pathology in the standard diagnostic literature. This pipeline, consisting of extracting patterns and then classifying them, loosely resembles the common pipelines used in supervised learning schema. Therefore, we propose to augment the psychiatrists' diagnosis toolbox with an artificial intelligence system based on natural language processing and machine learning algorithms. The proposed system would assist in the diagnostic using a patient's speech as input. The understanding and insights obtained from customizing these systems to specific pathologies is likely to be more broadly applicable to other NLP tasks, therefore we expect to make contributions not only for psychiatry but also within the computer science community. We intend to develop these ideas and evaluate them beyond the lab setting. Our end goal is to make it possible for a practitioner to integrate our tools into her daily practice with minimal effort", "nQuery - A Natural Language Statement to SQL Query Generator.\n\nIn this research, an intelligent system is designed between the user and the database system which accepts natural language input and then converts it into an SQL query. The research focuses on incorporating complex queries along with simple queries irrespective of the database. The system accommodates aggregate functions, multiple conditions in WHERE clause, advanced clauses like ORDER BY, GROUP BY and HAVING. The system handles single sentence natural language inputs, which are with respect to selected database. The research currently concentrates on MySQL database system. The natural language statement goes through various stages of Natural Language Processing like morphological, lexical, syntactic and semantic analysis resulting in SQL query formation.", "V for Vocab: An Intelligent Flashcard Application.\n\nStudents choose to use flashcard applications available on the Internet to help memorize word-meaning pairs. This is helpful for tests such as GRE, TOEFL or IELTS, which emphasize on verbal skills. However, monotonous nature of flashcard applications can be diminished with the help of Cognitive Science through Testing Effect. Experimental evidences have shown that memory tests are an important tool for long term retention (Roediger and Karpicke, 2006). Based on these evidences, we developed a novel flashcard application called ``V for Vocab'' that implements short answer based tests for learning new words. Furthermore, we aid this by implementing our short answer grading algorithm which automatically scores the user's answer. The algorithm makes use of an alternate thesaurus instead of traditional Wordnet and delivers state-of-the-art performance on popular word similarity datasets. We also look to lay the foundation for analysis based on implicit data collected from our application.", "Are You Asking the Right Questions? Teaching Machines to Ask Clarification Questions.\n\nInquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this thesis work, we explore how can we teach machines to ask clarification questions when faced with uncertainty, a goal of increasing importance in today's automated society. We do a preliminary study using data from StackExchange, a plentiful online resource where people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We build neural network models inspired by the idea of the expected value of perfect information: a good question is one whose expected answer is going to be most useful. To build generalizable systems, we propose two future research directions: a template-based model and a sequence-to-sequence based neural generative model.", "Building a Non-Trivial Paraphrase Corpus Using Multiple Machine Translation Systems.\n\nWe propose a novel sentential paraphrase acquisition method. To build a well-balanced corpus for Paraphrase Identification, we especially focus on acquiring both non-trivial positive and negative instances. We use multiple machine translation systems to generate positive candidates and a monolingual corpus to extract negative candidates. To collect non-trivial instances, the candidates are uniformly sampled by word overlap rate. Finally, annotators judge whether the candidates are either positive or negative. Using this method, we built and released the first evaluation corpus for Japanese paraphrase identification, which comprises 655 sentence pairs.", "Segmentation Guided Attention Networks for Visual Question Answering.\n\nIn this paper we propose to solve the problem of Visual Question answering by using a novel segmentation guided attention based networks which we call SegAttendNet. We use image segmentation maps, generated by a Fully Convolutional Deep Neural Network to refine our attention maps and use these refined attention maps to make the model focus on the relevant parts of the image to answer a question. The refined attention maps are used by the LSTM network to learn to produce the answer. We presently train our model on the visual7W dataset and do a category wise evaluation of the 7 question categories. We achieve state of the art results on this dataset and beat the previous benchmark on this dataset by a 1.5% margin improving the question answering accuracy from 54.1% to 55.6% and demonstrate improvements in each of the question categories. We also visualize our generated attention maps and note their improvement over the attention maps generated by the previous best approach.", "Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks.\n\nWe propose a convolutional neural network model for text-based speaker identification on multiparty dialogues extracted from the TV show, Friends. While most previous works on this task rely heavily on acoustic features, our approach attempts to identify speakers in dialogues using their speech patterns as captured by transcriptions to the TV show. It has been shown that different individual speakers exhibit distinct idiolectal styles. Several convolutional neural network models are developed to discriminate between differing speech patterns. Our results confirm the promise of text-based approaches, with the best performing model showing an accuracy improvement of over 6% upon the baseline CNN model.", "Variation Autoencoder Based Network Representation Learning for Classification.\n\nNetwork representation is the basis of many applications and of extensive interest in various fields, such as information retrieval, social network analysis, and recommendation systems. Most previous methods for network representation only consider the incomplete aspects of a problem, including link structure, node information, and partial integration. The present study introduces a deep network representation model that seamlessly integrates the text information and structure of a network. The model captures highly non-linear relationships between nodes and complex features of a network by exploiting the variational autoencoder (VAE), which is a deep unsupervised generation algorithm. The representation learned with a paragraph vector model is merged with that learned with the VAE to obtain the network representation, which preserves both structure and text information. Comprehensive experiments is conducted on benchmark datasets and find that the introduced model performs better than state-of-the-art techniques.", "Blind Phoneme Segmentation With Temporal Prediction Errors.\n\nPhonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural networks. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame.Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods", "Automatic Generation of Jokes in Hindi.\n\nWhen it comes to computational language processing systems, humour is a relatively unexplored domain, especially more so for Hindi (or rather, most languages other than English). Most researchers agree that a joke consists of two main parts - the setup and the punchline, which the humour being encoded in the incongruity between the two. In this paper, we look at Dur se Dekha jokes, a restricted domain of humorous three liner poetry in Hindi. We analyze their structure to understand how humour is encoded in them and formalize it. We then develop a system which is successfully able to generate a basic form of these jokes.", "Word Embedding for Response-To-Text Assessment of Evidence.\n\nManually grading the Response to Text Assessment (RTA) is labor intensive. Therefore, an automatic method is being developed for scoring analytical writing when the RTA is administered in large numbers of classrooms. Our long-term goal is to also use this scoring method to provide formative feedback to students and teachers about students' writing quality.  As a first step towards this goal, interpretable features for automatically scoring the evidence rubric of the RTA have been developed. In this paper, we present a simple but promising method for improving evidence scoring by employing the word embedding model. We evaluate our method on corpora of responses written by upper elementary students.", "Domain Specific Automatic Question Generation from Text.\n\nThe goal of my doctoral thesis is to automatically generate interrogative sentences from descriptive sentences of Turkish biology text. We employ syntactic and semantic approaches to parse descriptive sentences. Syntactic and semantic approaches utilize syntactic (constituent or dependency) parsing and semantic role labeling systems respectively. After parsing step, question statements whose answers are embedded in the descriptive sentences are going to be formulated by using some predefined rules and templates. Syntactic parsing is done using an open source dependency parser called MaltParser (Nivre et al. 2007). Whereas to accomplish semantic parsing, we will construct a biological proposition bank (BioPropBank) and a corpus annotated with semantic roles. Then we will employ supervised methods to automatic label the semantic roles of a sentence.", "SoccEval: An Annotation Schema for Rating Soccer Players.\n\nThis paper describes the SoccEval Annotation Project, an annotation schema designed to support machine-learning classification efforts to evaluate the performance of soccer players based on match reports taken from online news sources. In addition to factual information about player attributes and actions, the schema annotates subjective opinions about them. After explaining the annotation schema and annotation process, we describe a machine learning experiment. Classifiers trained on features derived from annotated data performed better than a baseline trained on unigram features. Initial results suggest that improvements can be made to the annotation scheme and guidelines as well as the amount of data annotated. We believe our schema could be potentially expanded to extract more information about soccer players and teams.", "Accent Adaptation for the Air Traffic Control Domain.\n\nAutomated speech recognition (ASR) plays a significant role in training and simulation systems for air traffic controllers. However, because English is the default language used in air traffic control (ATC), ASR systems often encounter difficulty with speakers' non-native accents, for which there is a paucity of data. This paper examines the effects of accent adaptation on the recognition of non-native English speech in the ATC domain. Accent adaptation has been demonstrated to be an effective way to model under-resourced speech, and can be applied to a variety of models. We use Subspace Gaussian Mixture Models (SGMMs) with the Kaldi Speech Recognition Toolkit to adapt acoustic models from American English to German-accented English, and compare it against other adaptation methods. Our results provide additional evidence that SGMMs can be an efficient and effective way to approach this problem, particularly with smaller amounts of accented training data.", "Generating Steganographic Text with LSTMs.\n\nMotivated by concerns for user privacy, we design a steganographic system (''stegosystem'') that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. In this paper, we propose a novel linguistic stegosystem based on a Long-Short Term Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.", "Predicting Depression for Japanese Blog Text.\n\nThis study aims to predict clinical depression, a prevalent mental disorder, from blog posts written in Japanese by using machine learning approaches. The study focuses on how data quality and various types of linguistic features (characters, tokens, and lemmas) affect prediction outcome. Depression prediction achieved 95.5% accuracy using selected lemmas as features.", "Fast Forward Through Opportunistic Incremental Meaning Representation Construction.\n\nOne of the challenges semantic parsers face involves upstream errors originating from pre-processing modules such as ASR and syntactic parsers, which undermine the end result from the get go. We report the work in progress on a novel incremental semantic parsing algorithm that supports simultaneous application of independent heuristics and facilitates the construction of partial but potentially actionable meaning representations to overcome this problem. Our contribution to this point is mainly theoretical. In future work we intend to evaluate the algorithm as part of a dialogue understanding system on state of the art benchmarks.", "Modeling Situations in Neural Chat Bots.\n\nSocial media accumulates vast amounts of online conversations that enable data-driven modeling of chat dialogues. It is, however, still hard to utilize the neural network-based SEQ2SEQ model for dialogue modeling in spite of its acknowledged success in machine translation. The main challenge comes from the high degrees of freedom of responses in dialogues. In this study, we explore neural conversational models that have general mechanisms for handling a variety of situations that affect our response. In our experiments, we confirmed the effectiveness of the proposed method in a response selection test by using massive dialogue data we have collected from Twitter.", "An Empirical Study on End-to-End Sentence Modelling.\n\nAccurately representing the meaning of a piece of text, otherwise known as sentence modelling, is an important component in many natural language inference tasks. We survey the spectrum of these methods, which lie along two dimensions: input representation granularity and composition model complexity. Using this framework, we reveal in our quantitative and qualitative experiments the limitations of the current state-of-the-art model in the context of sentence similarity tasks.", "Varying Linguistic Purposes of Emoji in (Twitter) Context.\n\nResearch into emoji in textual communication has, thus far, focused on high-frequency usages and the ambiguity of interpretations. Investigation of emoji uses across a wide range of uses can divide them into different linguistic functions: function and content words, or multimodal affective markers. Identifying where an emoji is merely replacing part of the text allows NLP tools the possibility of parsing them as any other word or phrase. Smiling emoticons are usually left out of data sets, but if they are used as the noun ''smile'' or the verb ''smiling'', we should be able to predict their part of speech. We report on an annotation task on English Twitter data with the goal of classifying emoji usage by these categories, and on the effectiveness of a classifier trained on these annotations. We find that it is possible to train a classifier to tell the difference between those emoji used as linguistic content words and those used as paralinguistic or affective multimodal markers even with a small amount of training data, but that accurate sub-classification of these multimodal emoji into specific classes like attitude, topic, or gesture will require more data and more feature engineering.", "Negotiation of Antibiotic Treatment in Medical Consultations: A Corpus Based Study.\n\nDoctor-patient conversation is considered a contributing factor to antibiotic over-prescription. Some language practices have been identified as parent pressuring doctors for prescribing; other practices are considered as likely to engender parent resistance to non-antibiotic treatment recommendations. In social science studies, approaches such as conversation analysis have been applied to identify those language practices. Current research for dialogue systems offer an alternative approach. Past research proved that corpus-based approaches have been effectively used for research involving modeling dialogue acts and sequential relations. In this proposal, we propose a corpus-based study of doctor-patient conversations of antibiotic treatment negotiation in pediatric consultations. Based on findings from conversation analysis studies, we use a computational linguistic approach to assist annotating and modeling of doctor-patient language practices, and analyzing their influence on antibiotic over-prescribing.", "Improving Distributed Representations of Tweets - Present and Future.\n\nUnsupervised representation learning for tweets is an important research field which helps in solving several business applications such as sentiment analysis, hashtag prediction, paraphrase detection and microblog ranking. A good tweet representation learning model must handle the idiosyncratic nature of tweets which poses several challenges such as short length, informal words, unusual grammar and misspellings. However, there is a lack of prior work which surveys the representation learning models with a focus on tweets. In this work, we organize the models based on its objective function which aids the understanding of the literature. We also provide interesting future directions, which we believe are fruitful in advancing this field by building high-quality tweet representation learning models.", "Bilingual Word Embeddings with Bucketed CNN for Parallel Sentence Extraction.\n\nWe propose a novel model which can be used to align the sentences of two different languages using neural architectures. First, we train our model to get the bilingual word embeddings and then, we create a similarity matrix between the words of the two sentences. Because of different lengths of the sentences involved, we get a matrix of varying dimension. We dynamically pool the similarity matrix into a matrix of fixed dimension and use Convolutional Neural Network (CNN) to classify the sentences as aligned or not. To further improve upon this technique, we bucket the sentence pairs to be classified into different groups and train CNN's separately. Our approach not only solves sentence alignment problem but our model can be regarded as a generic bag-of-words similarity measure for monolingual or bilingual corpora.", "Domain-Specific New Words Detection in Chinese.\n\nWith the explosive growth of Internet, more and more domain-specific environments appear, such as forums, blogs, MOOCs and etc. Domain-specific words appear in these areas and always play a critical role in the domain-specific NLP tasks. This paper aims at extracting Chinese domain-specific new words automatically. The extraction of domain-specific new words has two parts including both new words in this domain and the especially important words. In this work, we propose a joint statistical model to perform these two works simultaneously. Compared to traditional new words detection models, our model doesn't need handcraft features which are labor intensive. Experimental results demonstrate that our joint model achieves a better performance compared with the state-of-the-art methods.", "Parsing Graphs with Regular Graph Grammars.\n\nRecently, several datasets have become available which represent natural language phenomena as graphs. Hyperedge Replacement Languages (HRL) have been the focus of much attention as a formalism to represent the graphs in these datasets. Chiang et al. (2013) prove that HRL graphs can be parsed in polynomial time with respect to the size of the input graph. We believe that HRL are more expressive than is necessary to represent semantic graphs and we propose the use of Regular Graph Languages (RGL; Courcelle 1991), which is a subfamily of HRL, as a possible alternative. We provide a top-down parsing algorithm for RGL that runs in time linear in the size of the input graph.", "Predictive Linguistic Features of Schizophrenia.\n\nSchizophrenia is one of the most disabling and difficult to treat of all human medical/health conditions, ranking in the top ten causes of disability worldwide. It has been a puzzle in part due to difficulty in identifying its basic, fundamental components. Several studies have shown that some manifestations of schizophrenia (e.g., the negative symptoms that include blunting of speech prosody, as well as the disorganization symptoms that lead to disordered language) can be understood from the perspective of linguistics. However, schizophrenia research has not kept pace with technologies in computational linguistics, especially in semantics and pragmatics. As such, we examine the writings of schizophrenia patients analyzing their syntax, semantics and pragmatics. In addition, we analyze tweets of (self proclaimed) schizophrenia patients who publicly discuss their diagnoses. For writing samples dataset, syntactic features are found to be the most successful in classification whereas for the less structured Twitter dataset, a combination of  features performed the best.", "Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible.\n\nWord embeddings are supposed to provide easy access to semantic relations such as ``male of'' (man---woman). While this claim has been investigated for concepts, little is known about the distributional behavior of relations of (Named) Entities. We describe two word embedding-based models that predict values for relational attributes of entities, and analyse them. The task is challenging, with major performance differences between relations. Contrary to many NLP tasks, high difficulty for a relation does not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings.", "Issues of Mass and Count: Dealing with `Dual-Life' Nouns.\n\nThe topics of mass and count have been studied for many decades in philosophy (e.g., Quine, 1960; Pelletier, 1975), linguistics (e.g., McCawley, 1975; Allen, 1980; Krifka, 1991) and psychology (e.g., Middleton et al, 2004; Barner et al, 2009).                          More recently, interest from within computational linguistics has studied the issues involved (e.g., Pustejovsky, 1991; Bond, 2005; Schmidtke \\& Kuperman, 2016), to name just a few.  As is pointed out in these works, there are many difficult conceptual issues involved in the study of this contrast. In this article we study one of these issues -- the ``Dual-Life'' of being simultaneously +mass and +count -- by means of an unusual combination of human annotation, online lexical resources, and online corpora.", "Semantic Frames and Visual Scenes: Learning Semantic Role Inventories from Image and Video Descriptions.\n\nFrame-semantic parsing and semantic role labelling, that aim to automatically assign semantic roles to arguments of verbs in a sentence, have become an active strand of research in NLP. However, to date these methods have relied on a predefined inventory of semantic roles. In this paper, we present a method to automatically learn argument role inventories for verbs from large corpora of text, images and videos. We evaluate the method against manually constructed role inventories in FrameNet and show that the visual model outperforms the language-only model and operates with a high precision.", "Acquiring Predicate Paraphrases from News Tweets.\n\nWe present a simple method for ever-growing extraction of predicate paraphrases from news headlines in Twitter. Analysis of the output of ten weeks of collection shows that the accuracy of paraphrases with different support levels is estimated between 60-86\\%. We also demonstrate that our resource is to a large extent complementary to existing resources, providing many novel paraphrases. Our resource is publicly available, continuously expanding based on daily news.", "Emotion Intensities in Tweets.\n\nThis paper examines the task of detecting intensity of emotion from text. We create the first datasets of tweets annotated for anger, fear, joy, and sadness intensities. We use a technique called best---worst scaling (BWS) that improves annotation consistency and obtains reliable fine-grained scores. We show that emotion-word hashtags often impact emotion intensity, usually conveying a more intense emotion. Finally, we create a benchmark regression system and conduct experiments to determine: which features are useful for detecting emotion intensity; and, the extent to which two emotions are similar in terms of how they manifest in language.", "Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks.\n\nHumans as well as animals are good at imitation. Inspired by this, the learning by demonstration view of machine learning learns to perform a task from detailed example demonstrations. In this paper, we introduce the task of question answering using natural language demonstrations where the question answering system is provided with detailed demonstrative solutions to questions in natural language. As a case study, we explore the task of learning to solve geometry problems using demonstrative solutions available in textbooks. We collect a new dataset of demonstrative geometry solutions from textbooks and explore approaches that learn to interpret these demonstrations as well as to use these interpretations to solve geometry problems. Our approaches show improvements over the best previously published system for solving geometry problems.", "Aligning Script Events with Narrative Texts.\n\nScript knowledge plays a central role in text understanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible.", "Evaluating Semantic Parsing against a Simple Web-based Question Answering Model.\n\nSemantic parsing shines at analyzing complex natural language that involves composition and computation over multiple pieces of evidence. However, datasets for semantic parsing contain many factoid questions that can be answered from a single web document. In this paper, we propose to evaluate semantic parsing-based question answering models by comparing them to a question answering baseline that queries the web and extracts the answer only from web snippets, without access to the target knowledge-base. We investigate this approach on COMPLEXQUESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (\u223c35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our model performs well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives.", "Ways of Asking and Replying in Duplicate Question Detection.\n\nThis paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task. This study permits to gain a novel insight on the different levels of robustness of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios.", "Classifying Semantic Clause Types: Modeling Context and Genre Characteristics with Recurrent Neural Networks and Attention.\n\nDetecting aspectual properties of clauses in the form of situation entity types has been shown to depend on a combination of syntactic-semantic and contextual features. We explore this task in a deep-learning framework, where tuned word representations capture lexical, syntactic and semantic features. We introduce an attention mechanism that pinpoints relevant context not only for the current instance, but also for the larger context. Apart from implicitly capturing task relevant features, the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another.", "Embedded Semantic Lexicon Induction with Joint Global and Local Optimization.\n\nCreating annotated frame lexicons such as PropBank and FrameNet is expensive and labor intensive. We present a method to induce an embedded frame lexicon in an minimally supervised fashion using nothing more than unlabeled predicate-argument word pairs. We hypothesize that aggregating such pair selectional preferences across training leads us to a global understanding that captures predicate-argument frame structure. Our approach revolves around a novel integration between a predictive embedding model and an Indian Buffet Process posterior regularizer. We show, through our experimental evaluation, that we outperform baselines on two tasks and can learn an embedded frame lexicon that is able to capture some interesting generalities in relation to hand-crafted semantic frames.", "Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for Croatian.\n\nThe Practical Lexical Function (PLF) model is a model of computational distributional semantics that attempts to strike a balance between expressivity and learnability in predicting phrase meaning and shows competitive results. We investigate how well the PLF carries over to free word order languages, given that it builds on observations of predicate-argument combinations that are harder to recover in free word order languages. We evaluate variants of the PLF for Croatian, using a new lexical substitution dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant.", "Semantic Frame Labeling with Target-based Neural Model.\n\nThis paper explores the automatic learning of distributed representations of the target's context for semantic frame labeling with target-based neural model. We constrain the whole sentence as the model's input without feature extraction from the sentence. This is different from many previous works in which local feature extraction of the targets is widely used. This constraint makes the task harder, especially with long sentences, but also makes our model easily applicable to a range of resources and other similar tasks. We evaluate our model on several resources and get the state-of-the-art result on subtask 2 of SemEval 2015 task 15. Finally, we extend the task to word-sense disambiguation task and we also achieve a strong result in comparison to state-of-the-art work.", "Logical Metonymy in a Distributional Model of Sentence Comprehension.\n\nIn theoretical linguistics, logical metonymy is defined as the combination of an event-subcategorizing verb with an entity-denoting direct object (e.g., The author began the book), so that the interpretation of the VP requires the retrieval of a covert event (e.g., writing). Psycholinguistic studies have revealed extra processing costs for logical metonymy, a phenomenon generally explained with the introduction of new semantic structure. In this paper, we present a general distributional model for sentence comprehension inspired by the Memory, Unification and Control model by Hagoort (2013,2016). We show that our distributional framework can account for the extra processing costs of logical metonymy and can identify the covert event in a classification task.", "A Mixture Model for Learning Multi-Sense Word Embeddings.\n\nWord embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.", "Deep Learning Models For Multiword Expression Identification.\n\nMultiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are unpredictable with respect to their component words. In this paper we propose the first deep learning models for token-level identification of MWEs. Specifically, we consider a layered feedforward network, a recurrent neural network, and convolutional neural networks. In experimental results we show that convolutional neural networks are able to outperform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance.", "The (too Many) Problems of Analogical Reasoning with Word Vectors.\n\nThis paper explores the possibilities of analogical reasoning with vector space models. Given two pairs of words with the same relation (e.g. man:woman :: king:queen), it was proposed that the offset between one pair of the corresponding word vectors can be used to identify the unknown member of the other pair (king - man + woman = queen). We argue against such ``linguistic regularities'' as a model for linguistic relations in vector space models and as a benchmark, and we show that the vector offset (as well as two other, better-performing methods) suffers from dependence on vector similarity.", "Double Trouble: The Problem of Construal in Semantic Annotation of Adpositions.\n\nWe consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that an adposition's lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition's lexical function so they can be annotated at scale\u2014supporting automatic, statistical processing of domain-general language\u2014and discuss how this representation would allow for a simpler inventory of labels.", "Mapping the Paraphrase Database to WordNet.\n\nWordNet has facilitated important research in natural language processing but its usefulness is somewhat limited by its relatively small lexical coverage. The Paraphrase Database (PPDB) covers 650 times more words, but lacks the semantic structure of WordNet that would make it more directly useful for downstream tasks. We present a method for mapping words from PPDB to WordNet synsets with 89\\% accuracy. The mapping also lays important groundwork for incorporating WordNet's relations into PPDB so as to increase its utility for semantic reasoning in applications.", "Generating Pattern-Based Entailment Graphs for Relation Extraction.\n\nRelation extraction is the task of recognizing and extracting relations between entities or concepts in texts. A common approach is to exploit existing knowledge to learn linguistic patterns expressing the target relation and use these patterns for extracting new relation mentions. Deriving relation patterns automatically usually results in large numbers of candidates, which need to be filtered to derive a subset of patterns that reliably extract correct relation mentions. We address the pattern selection task by exploiting the knowledge represented by entailment graphs, which capture semantic relationships holding among the learned pattern candidates. This is motivated by the fact that a pattern may not express the target relation explicitly, but still be useful for extracting instances for which the relation holds, because its meaning entails the meaning of the target relation. We evaluate the usage of both automatically generated and gold-standard entailment graphs in a relation extraction scenario and present favorable experimental results, exhibiting the benefits of structuring and selecting patterns based on entailment graphs.", "Decoding Sentiment from Distributed Representations of Sentences.\n\nDistributed representations of sentences have been developed recently to represent their meaning as real-valued vectors. However, it is not clear how much information such representations retain about the polarity of sentences. To study this question, we decode sentiment from unsupervised sentence representations learned with different architectures (sensitive to the order of words, the order of sentences, or none) in 9 typologically diverse languages. Sentiment results from the (recursive) composition of lexical items and grammatical strategies such as negation and concession. The results are manifold: we show that there is no `one-size-fits-all' representation architecture outperforming the others across the board. Rather, the top-ranking architectures depend on the language at hand. Moreover, we find that in several cases the additive composition model based on skip-gram word vectors may surpass supervised state-of-art architectures such as bi-directional LSTMs. Finally, we provide a possible explanation of the observed variation based on the type of negative constructions in each language.", "Comparing Approaches for Automatic Question Identification.\n\nCollecting spontaneous speech corpora that are open-ended, yet topically constrained, is increasingly popular for research in spoken dialogue systems and speaker state, inter alia. Typically, these corpora are labeled by human annotators, either in the lab or through crowd-sourcing; however, this is cumbersome and time-consuming for large corpora. We present four different approaches to automatically tagging a corpus when general topics of the conversations are known. We develop these approaches on the Columbia X-Cultural Deception corpus and find accuracy that significantly exceeds the baseline. Finally, we conduct a cross-corpus evaluation by testing the best performing approach on the Columbia/SRI/Colorado corpus.", "Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles.\n\nWe study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10% gains over baselines.", "Deep Active Learning for Dialogue Generation.\n\nWe propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.", "Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection.\n\nWe introduce WHiC, a challenging testbed for detecting hypernymy, an asymmetric relation between words. While previous work has focused on detecting hypernymy between word types, we ground the meaning of words in specific contexts drawn from WordNet examples, and require predictions to be sensitive to changes in contexts. WHiC lets us analyze complementary properties of two approaches of inducing vector representations of word meaning in context. We show that such contextualized word representations also improve detection of a wider range of semantic relations in context.", "Learning Antonyms with Paraphrases and a Morphology-Aware Neural Network.\n\nRecognizing and distinguishing antonyms from other types of semantic relations is an essential part of language understanding systems. In this paper, we present a novel method for deriving antonym pairs using paraphrase pairs containing negation markers. We further propose a neural network model, AntNET, that integrates morphological features indicative of antonymy into a path-based relation detection algorithm. We demonstrate that our model outperforms state-of-the-art models in distinguishing antonyms from other semantic relations and is capable of efficiently handling multi-word expressions.", "What Analogies Reveal about Word Vectors and their Compositionality.\n\nAnalogy completion via vector arithmetic has become a common means of demonstrating the compositionality of word embeddings. Previous work have shown that this strategy works more reliably for certain types of analogical word relationships than for others, but these studies have not offered a convincing account for why this is the case. We arrive at such an account through an experiment that targets a wide variety of analogy questions and defines a baseline condition to more accurately measure the efficacy of our system. We find that the most reliably solvable analogy categories involve either 1) the application of a morpheme with clear syntactic effects, 2) male--female alternations, or 3) named entities. These broader types do not pattern cleanly along a syntactic--semantic divide. We suggest instead that their commonality is distributional, in that the difference between the distributions of two words in any given pair encompasses a relatively small number of word types. Our study offers a needed explanation for why analogy tests succeed and fail where they do and provides nuanced insight into the relationship between word distributions and the theoretical linguistic domains of syntax and semantics.", "Merging knowledge bases in different languages.\n\nRecently, different systems which learn to populate and extend a knowledge base (KB) from the web in different languages have been presented. Although a large set of concepts should be learnt independently from the language used to read, there are facts which are expected to be more easily gathered in local language (e.g., culture or geography). A system that merges KBs learnt in different languages will benefit from the complementary information as long as common beliefs are identified, as well as from redundancy present in web pages written in different languages. In this paper, we deal with the problem of identifying equivalent beliefs (or concepts) across language specific KBs, assuming that they share the same ontology of categories and relations. In a case study with two KBs independently learnt from different inputs, namely web pages written in English and web pages written in Portuguese respectively, we report on the results of two methodologies: an approach based on personalized PageRank and an inference technique to find out common relevant paths through the KBs. The proposed inference technique efficiently identifies relevant paths, outperforming the baseline (a dictionary-based classifier) in the vast majority of tested categories.", "Work Hard, Play Hard: Email Classification on the Avocado and Enron Corpora.\n\nIn this paper, we present an empirical study of email classification into two main categories ``Business'' and ``Personal''.  We train on the Enron email corpus, and test on the Enron and Avocado email corpora. We show that information from the email exchange networks improves the performance of classification. We represent the email exchange networks as social networks with graph structures. For this classification task, we extract social networks features from the graphs in addition to lexical features from email content and we compare the performance of SVM and Extra-Trees classifiers using these features.  Combining graph features with lexical features improves the performance on both classifiers. We also provide manually annotated sets of the Avocado and Enron email corpora as a supplementary contribution.", "On the ``Calligraphy'' of Books.\n\nAuthorship attribution is a natural language processing task that has been widely studied, often by considering small order statistics. In this paper, we explore a complex network approach to assign the authorship of texts based on their mesoscopic representation, in an attempt to capture the flow of the narrative.  Indeed, as reported in this work, such an approach allowed the identification of the dominant narrative structure of the studied authors. This has been achieved due to the ability of the mesoscopic approach to take into account relationships between different, not necessarily adjacent, parts of the text, which is able to capture the story flow. The potential of the proposed approach has been illustrated through principal component analysis, a comparison with the chance baseline method, and network visualization. Such visualizations reveal individual characteristics of the authors, which can be understood as a kind of calligraphy.", "Adapting predominant and novel sense discovery algorithms for identifying corpus-specific sense differences.\n\nWord senses are not static and may have temporal, spatial or corpus-specific scopes. Identifying such scopes might benefit the existing WSD systems largely. In this paper, while studying corpus specific word senses, we adapt three existing predominant and novel-sense discovery algorithms to identify these corpus-specific senses. We make use of text data available in the form of millions of digitized books and newspaper archives as two different sources of corpora and propose automated methods to identify corpus-specific word senses at various time points. We conduct an extensive and thorough human judgement experiment to rigorously evaluate and compare the performance of these approaches. Post adaptation, the output of the three algorithms are in the same format and the accuracy results are also comparable, with roughly 45-60% of the reported corpus-specific senses being judged as genuine.", "Graph Methods for Multilingual FrameNets.\n\nThis paper introduces a new, graph-based view of the data of the FrameNet project, which we hope will make it easier to understand the mixture of semantic and syntactic information contained in FrameNet annotation.  We show how English FrameNet and other Frame Semantic resources can be represented as sets of interconnected graphs of frames, frame elements, semantic types, and annotated instances of them in text.  We display examples of the new graphical representation based on the annotations, which combine Frame Semantics and Construction Grammar, thus capturing most of the syntax and semantics of each sentence.  We consider how graph theory could help researchers to make better use of FrameNet data for tasks such as automatic Frame Semantic role labeling, paraphrasing, and translation.              Finally, we describe the development of FrameNet-like lexical resources for other languages in the current Multilingual FrameNet project.  which seeks to discover cross-lingual alignments, both in the lexicon (for frames and lexical units within frames) and across parallel or comparable texts.  We conclude with an example showing graphically the semantic and syntactic similarities and differences between parallel sentences in English and Japanese.  We will release software for displaying such graphs from the current data releases.", "A Graph Based Semi-Supervised Approach for Analysis of Derivational Nouns in Sanskrit.\n\nDerivational nouns are widely used in Sanskrit corpora and represent an important cornerstone of productivity in the language. Currently there exists no analyser that identifies the derivational nouns. We propose a semi supervised approach for identification of derivational nouns in Sanskrit. We not only identify the derivational words, but also link them to their corresponding source words. Our novelty comes in the design of the network structure for the task. The edge weights are featurised based on the phonetic, morphological, syntactic and the semantic similarity shared between the words to be identified. We find that our model is effective for the task, even when we employ a labelled dataset which is only 5 \\% to that of the entire dataset.", "Evaluating text coherence based on semantic similarity graph.\n\nCoherence is a crucial feature of text because it is indispensable for conveying its communication purpose and meaning to its readers. In this paper, we propose an unsupervised text coherence scoring based on graph construction in which edges are established between semantically similar sentences represented by vertices. The sentence similarity is calculated based on the cosine similarity of semantic vectors representing sentences. We provide three graph construction methods establishing an edge from a given vertex to a preceding adjacent vertex, to a single similar vertex, or to multiple similar vertices. We evaluated our methods in the document discrimination task and the insertion task by comparing our proposed methods to the supervised (Entity Grid) and unsupervised (Entity Graph) baselines. In the document discrimination task, our method outperformed the unsupervised baseline but could not do the supervised baseline, while in the insertion task, our method outperformed both baselines.", "Spectral Graph-Based Method of Multimodal Word Embedding.\n\nIn this paper, we propose a novel method for multimodal word embedding, which exploit a generalized framework of multi-view spectral graph embedding to take into account visual appearances or scenes denoted by words in a corpus. We evaluated our method through word similarity tasks and a concept-to-image search task, having found that it provides word representations that reflect visual information, while somewhat trading-off the performance on the word similarity tasks. Moreover, we demonstrate that our method captures multimodal linguistic regularities, which enable recovering relational similarities between words and images by vector arithmetics.", "Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings.\n\nWord embeddings are high-dimensional vector representations of words and are thus difficult to interpret. In order to deal with this, we introduce an unsupervised parameter free method for creating a hierarchical graphical clustering of the full ensemble of word vectors and show that this structure is a geometrically meaningful representation of the original relations between the words. This newly obtained representation can be used for better understanding and thus improving the embedding algorithm and exhibits semantic meaning, so it can also be utilized in a variety of language processing tasks like categorization or measuring similarity.", "Extract with Order for Coherent Multi-Document Summarization.\n\nIn this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability.  We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence."], "labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "meta": ["ALW1: Hamdy Mubarak, Kareem Darwish, Walid Magdy: \"Abusive Language Detection on Arabic Social Media\"", "ALW1: Isobelle Clarke, Dr. Jack Grieve: \"Dimensions of Abusive Language on Twitter\"", "ALW1: Varada Kolhatkar, Maite Taboada: \"Constructive Language in News Comments\"", "ALW1: Lucas Wright, Derek Ruths, Kelly P Dillon, Haji Mohammad Saleem, Susan Benesch: \"Vectors for Counterspeech on Twitter\"", "ALW1: Niloofar Safi Samghabadi, Suraj Maharjan, Alan Sprague, Raquel Diaz-Sprague, Thamar Solorio: \"Detecting Nastiness in Social Media\"", "ALW1: Hui-Po Su, Zhen-Jie Huang, Hao-Tsung Chang, Chuan-Jie Lin: \"Rephrasing Profanity in Chinese Text\"", "ALW1: George Kennedy, Andrew McCollough, Edward Dixon, Alexei Bastidas, John Ryan, Chris Loo, Saurav Sahay: \"Technology Solutions to Combat Online Harassment\"", "ALW1: Zeerak Waseem, Thomas Davidson, Dana Warmsley, Ingmar Weber: \"Understanding Abuse: A Typology of Abusive Language Detection Subtasks\"", "ALW1: Bj\u00f6rn Gamb\u00e4ck, Utpal Kumar Sikdar: \"Using Convolutional Neural Networks to Classify Hate-Speech\"", "ALW1: Alexis Palmer, Melissa Robinson, Kristy K. Phillips: \"Illegal is not a Noun: Linguistic Form for Detection of Pejorative Nominalizations\"", "ALW1: Joan Serr\u00e0, Ilias Leontiadis, Dimitris Spathis, Gianluca Stringhini, Jeremy Blackburn, Athena Vakali: \"Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary Words\"", "ALW1: John Pavlopoulos, Prodromos Malakasiotis, Ion Androutsopoulos: \"Deep Learning for User Comment Moderation\"", "ALW1: Ji Ho Park, Pascale Fung: \"One-step and Two-step Classification for Abusive Language Detection on Twitter\"", "ALW1: Darja Fi\u0161er, Toma\u017e Erjavec, Nikola Ljube\u0161i\u0107: \"Legal Framework, Dataset and Annotation Schema for Socially Unacceptable Online Discourse Practices in Slovene\"", "BioNLP: Wen-wai Yim, Dario Tedesco, Catherine Curtin, Tina Hernandez-Boussard: \"Annotation of pain and anesthesia events for surgery-related processes and outcomes extraction\"", "BioNLP: Subhradeep Kayal, Zubair Afzal, George Tsatsaronis, Sophia Katrenko, Pascal Coupet, Marius Doornenbal, Michelle Gregory: \"Tagging Funding Agencies and Grants in Scientific Articles using Sequential Learning Models\"", "BioNLP: Pieter Fivez, Simon Suster, Walter Daelemans: \"Unsupervised Context-Sensitive Spelling Correction of Clinical Free-Text with Word and Character N-Gram Embeddings\"", "BioNLP: Kevin Patel, Divya Patel, Mansi Golakiya, Pushpak Bhattacharyya, Nilesh Birari: \"Adapting Pre-trained Word Embeddings For Use In Medical Coding\"", "BioNLP: Timothy Miller, Steven Bethard, Hadi Amiri, Guergana Savova: \"Unsupervised Domain Adaptation for Clinical Negation Detection\"", "BioNLP: Rahul V S S Patchigolla, Sunil Sahu, Ashish Anand: \"Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models\"", "BioNLP: Denis Newman-Griffis, Albert Lai, Eric Fosler-Lussier: \"Insights into Analogy Completion from the Biomedical Domain\"", "BioNLP: Georg Wiese, Dirk Weissenborn, Mariana Neves: \"Neural Question Answering at BioASQ 5B\"", "BioNLP: Sunil Mohan, Nicolas Fiorini, Sun Kim, Zhiyong Lu: \"Deep Learning for Biomedical Information Retrieval: Learning Textual Relevance from Click Logs\"", "BioNLP: Aakanksha Naik, Chris Bogart, Carolyn Rose: \"Extracting Personal Medical Events for User Timeline Construction using Minimal Supervision\"", "BioNLP: Khyathi Chandu, Aakanksha Naik, Aditya Chandrasekar, Zi Yang, Niloy Gupta, Eric Nyberg: \"Tackling Biomedical Text Summarization: OAQA at BioASQ 5B\"", "BioNLP: Vaden Masrani, Gabriel Murray, Thalia Field, Giuseppe Carenini: \"Detecting Dementia through Retrospective Analysis of Routine Blog Posts by Bloggers with Dementia\"", "BioNLP: Seth Polsley, Atif Tahir, Muppala Raju, Akintayo Akinleye, Duane Steward: \"Role-Preserving Redaction of Medical Records to Enable Ontology-Driven Processing\"", "BioNLP: Wael Salloum, Greg Finley, Erik Edwards, Mark Miller, David Suendermann-Oeft: \"Automated Preamble Detection in Dictated Medical Reports\"", "BioNLP: Simon Baker, Anna Korhonen: \"Initializing neural networks for hierarchical multi-label text classification\"", "BioNLP: Helen Cook, Rudolfs Berzins, Cristina Leal Rodr\u0131guez, Juan Miguel Cejuela, Lars Juhl Jensen: \"Creation and evaluation of a dictionary-based tagger for virus species and proteins\"", "BioNLP: Sudha Rao, Daniel Marcu, Kevin Knight, Hal Daum\u00e9 III: \"Biomedical Event Extraction using Abstract Meaning Representation\"", "BioNLP: Danchen Zhang, Daqing He, Sanqiang Zhao, Lei Li: \"Enhancing Automatic ICD-9-CM Code Assignment for \\\\Medical Texts with PubMed\"", "BioNLP: Archna Bhatia, Bonnie Dorr, Kristy Hollingshead, Samuel L. Phillips, Barbara McKenzie: \"Characterization of Divergence in Impaired Speech of ALS Patients\"", "BioNLP: Devi Ganesan, Ashish V. Tendulkar, Sutanu Chakraborti: \"Protein Word Detection using Text Segmentation Techniques\"", "BioNLP: Wael Salloum, Greg Finley, Erik Edwards, Mark Miller, David Suendermann-Oeft: \"Deep Learning for Punctuation Restoration in Medical Reports\"", "BioNLP: Danielle Mowery, Brett South, Olga Patterson, Shu-Hong Zhu, Mike Conway: \"Investigating the Documentation of Electronic Cigarette Use in the Veteran Affairs Electronic Health Record: A Pilot Study\"", "BioNLP: Sarvnaz Karimi, Xiang Dai, Hamedh Hassanzadeh, Anthony Nguyen: \"Automatic Diagnosis Coding of Radiology Reports: A Comparison of Deep Learning and Conventional Classification Methods\"", "BioNLP: Nazneen Fatema Rajani, Mihaela Bornea, Ken Barker: \"Stacking With Auxiliary Features for Entity Linking in the Medical Domain\"", "BioNLP: Maolin Li, Nhung Nguyen, Sophia Ananiadou: \"Proactive Learning for Named Entity Recognition\"", "BioNLP: Mariana Neves, Fabian Eckert, Hendrik Folkerts, Matthias Uflacker: \"Assessing the performance of Olelo, a real-time biomedical question answering application\"", "BioNLP: Wojciech Kusa, Michael Spranger: \"External Evaluation of Event Extraction Classifiers for Automatic Pathway Curation: An extended study of the mTOR pathway\"", "BioNLP: Diego Molla: \"Macquarie University at BioASQ 5b -- Query-based Summarisation Techniques for Selecting the Ideal Answers\"", "BioNLP: Masaki Asada, Makoto Miwa, Yutaka Sasaki: \"Extracting Drug-Drug Interactions with Attention CNNs\"", "BioNLP: Farrokh Mehryary, Kai Hakala, Suwisa Kaewphan, Jari Bj\u00f6rne, Tapio Salakoski, Filip Ginter: \"End-to-End System for Bacteria Habitat Extraction\"", "BioNLP: Zan-Xia Jin, Bo-Wen Zhang, Fan Fang, Le-Le Zhang, Xu-Cheng Yin: \"A Multi-strategy Query Processing Approach for Biomedical Question Answering: USTB\\\\_PRIR at BioASQ 2017 Task 5B\"", "BioNLP: Leonardo Campillos Llanos, Sophie Rosset, Pierre Zweigenbaum: \"Automatic classification of doctor-patient questions for a virtual patient record query task\"", "BioNLP: Mourad Sarrouti, Said Ouatik El Alaoui: \"A Biomedical Question Answering System in BioASQ 2017\"", "BioNLP: Sam Henry, Clint Cuffy, Bridget McInnes: \"Evaluating Feature Extraction Methods for Knowledge-based Biomedical Word Sense Disambiguation\"", "BioNLP: Samir Gupta, A.S.M. Ashique Mahmood, Karen Ross, Cathy Wu, K. Vijay-Shanker: \"Identifying Comparative Structures in Biomedical Text\"", "BioNLP: Bridget McInnes, Ted Pedersen: \"Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors\"", "BioNLP: Ari Klein, Abeed Sarker, Masoud Rouhizadeh, Karen O'Connor, Graciela Gonzalez: \"Detecting Personal Medication Intake in Twitter: An Annotated Corpus and Baseline Classification System\"", "BioNLP: Hans Moen, Kai Hakala, Farrokh Mehryary, Laura-Maria Peltonen, Tapio Salakoski, Filip Ginter, Sanna Salanter\u00e4: \"Detecting mentions of pain and acute confusion in Finnish clinical text\"", "BioNLP: Arnaud Ferr\u00e9, Pierre Zweigenbaum, Claire N\u00e9dellec: \"Representation of complex terms in a vector space structured by an ontology for a normalization task\"", "BioNLP: Jake Lever, Steven Jones: \"Painless Relation Extraction with Kindred\"", "BioNLP: Adyasha Maharana, Meliha Yetisgen: \"Clinical Event Detection with Hybrid Neural Architecture\"", "BioNLP: Yifan Peng, Zhiyong Lu: \"Deep learning for extracting protein-protein interactions from biomedical literature\"", "BioNLP: Gang Li, Cathy Wu, K. Vijay-Shanker: \"Noise Reduction Methods for Distantly Supervised Biomedical Relation Extraction\"", "BioNLP: Joel Adams, Steven Bedrick, Gerasimos Fergadiotis, Kyle Gorman, Jan van Santen: \"Target word prediction and paraphasia classification in spoken discourse\"", "BioNLP: Rezarta Islamaj Dogan, Andrew Chatr-aryamontri, Sun Kim, Chih-Hsuan Wei, Yifan Peng, Donald Comeau, Zhiyong Lu: \"BioCreative VI Precision Medicine Track: creating a training corpus for mining protein-protein interactions affected by mutations\"", "BioNLP: Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara, Georgios Paliouras, Ioannis Kakadiaris: \"Results of the fifth edition of the BioASQ Challenge\"", "BioNLP: Chen Lin, Timothy Miller, Dmitriy Dligach, Steven Bethard, Guergana Savova: \"Representations of Time Expressions for Temporal Relation Extraction with Convolutional Neural Networks\"", "BioNLP: Emilia Apostolova, Tom Velez: \"Toward Automated Early Sepsis Alerting: Identifying Infection Patients from Nursing Notes\"", "BUCC: Alp \u00d6ktem, Mireia Farr\u00fas, Leo Wanner: \"Automatic Extraction of Parallel Speech Corpora from Dubbed Movies\"", "BUCC: Mariana Neves: \"A parallel collection of clinical trials in Portuguese and English\"", "BUCC: Michael Bloodgood, Benjamin Strauss: \"Acquisition of Translation Lexicons for Historically Unwritten Languages via Bridging Loanwords\"", "BUCC: Jeenu Grover, Pabitra Mitra: \"Sentence Alignment using Unfolding Recursive Autoencoders\"", "BUCC: Andoni Azpeitia, Thierry Etchegoyhen, Eva Mart\u00ednez Garcia: \"Weighted Set-Theoretic Alignment of Comparable Sentences\"", "BUCC: Francis Gr\u00e9goire, Philippe Langlais: \"BUCC 2017 Shared Task: a First Attempt Toward a Deep Learning Framework for Identifying Parallel Sentences in Comparable Corpora\"", "BUCC: Zheng Zhang, Pierre Zweigenbaum: \"zNLP: Identifying Parallel Sentences in Chinese-English Comparable Corpora\"", "BUCC: Sainik Mahata, Dipankar Das, Sivaji Bandyopadhyay: \"BUCC2017: A Hybrid Approach for Identifying Parallel Sentences in Comparable Corpora\"", "BUCC: Phillippe Langlais: \"Users and Data: The Two Neglected Children of Bilingual Natural Language Processing Research\"", "BUCC: Pierre Zweigenbaum, Serge Sharoff, Reinhard Rapp: \"Overview of the Second BUCC Shared Task: Spotting Parallel Sentences in Comparable Corpora\"", "BUCC: J\u00e9r\u00e9my Ferrero, Laurent Besacier, Didier Schwab, Fr\u00e9d\u00e9ric Agn\u00e8s: \"Deep Investigation of Cross-Language Plagiarism Detection Methods\"", "BUCC: Dmitrijs Milajevs: \"Toward a Comparable Corpus of Latvian, Russian and English Tweets\"", "CLPsych: Jude Mikal, Samantha Hurst, Mike Conway: \"Investigating Patient Attitudes Towards the use of Social Media Data to Augment Depression Diagnosis and Treatment: a Qualitative Study\"", "CLPsych: Judy Hanwen Shen, Frank Rudzicz: \"Detecting Anxiety through Reddit\"", "CLPsych: Michelle Morales, Stefan Scherer, Rivka Levitan: \"A Cross-modal Review of Indicators for Depression Detection Systems\"", "CLPsych: Melissa Roemmele, Paola Mardo, Andrew Gordon: \"Natural-language Interactive Narratives in Imaginal Exposure Therapy for Obsessive-Compulsive Disorder\"", "CLPsych: Zunaira Jamil, Diana Inkpen, Prasadith Buddhitha, Kenton White: \"Monitoring Tweets for Depression to Detect At-risk Users\"", "CLPsych: Jia-Wen Guo, Danielle L Mowery, Djin Lai, Katherine Sward, Mike Conway: \"A Corpus Analysis of Social Connections and Social Isolation in Adolescents Suffering from Depressive Disorders\"", "CLPsych: Kate Loveys, Patrick Crutchley, Emily Wyatt, Glen Coppersmith: \"Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language\"", "CLPsych: Kate Niederhoffer, Jonathan Schler, Patrick Crutchley, Kate Loveys, Glen Coppersmith: \"In your wildest dreams: the language and psychological features of dreams\"", "CLPsych: Rohan Kshirsagar, Robert Morris, Samuel Bowman: \"Detecting and Explaining Crisis\"", "CLPsych: Micah Iserman, Molly Ireland: \"A Dictionary-Based Comparison of Autobiographies by People and Murderous Monsters\"", "CoNLL: Diego Marcheggiani, Anton Frolov, Ivan Titov: \"A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling\"", "CoNLL: Fatemeh Torabi Asr, Michael Jones: \"An Artificial Language Evaluation of Distributional Semantic Models\"", "CoNLL: Shafiq Joty, Preslav Nakov, Llu\u00eds M\u00e0rquez, Israa Jaradat: \"Cross-language Learning with Adversarial Neural Networks\"", "CoNLL: Georg Wiese, Dirk Weissenborn, Mariana Neves: \"Neural Domain Adaptation for Biomedical Question Answering\"", "CoNLL: SHOAIB JAMEEL, STEVEN SCHOCKAERT: \"Modeling Context Words as Regions: An Ordinal Regression Approach to Word Embedding\"", "CoNLL: Yan Song, Chia-Jung Lee, Fei Xia: \"Learning Word Representations with Regularization from Prior Knowledge\"", "CoNLL: Dominik Schlechtweg, Stefanie Eckmann, Enrico Santus, Sabine Schulte im Walde, Daniel Hole: \"German in Flux: Detecting Metaphoric Change via Word Entropy\"", "CoNLL: Haoruo Peng, Snigdha Chaturvedi, Dan Roth: \"A Joint Model for Semantic Sequences: Frames, Entities, Sentiments\"", "CoNLL: Ed Collins, Isabelle Augenstein, Sebastian Riedel: \"A Supervised Approach to Extractive Summarisation of Scientific Papers\"", "CoNLL: Olga Uryupina, Alessandro Moschitti: \"Collaborative Partitioning for Coreference Resolution\"", "CoNLL: Younes Samih, Mohamed Eldesouki, Mohammed Attia, Kareem Darwish, Ahmed Abdelali, Hamdy Mubarak, Laura Kallmeyer: \"Learning from Relatives: Unified Dialectal Arabic Segmentation\"", "CoNLL: Phong Le, Ivan Titov: \"Optimizing Differentiable Relaxations of Coreference Evaluation Metrics\"", "CoNLL: Adithya Renduchintala, Philipp Koehn, Jason Eisner: \"Knowledge Tracing in Sequential Learning of Inflected Vocabulary\"", "CoNLL: Yftah Ziser, Roi Reichart: \"Neural Structural Correspondence Learning for Domain Adaptation\"", "CoNLL: I-Hsuan Chen, Yunfei Long, Qin Lu, Chu-Ren Huang: \"Leveraging Eventive Information for Better Metaphor Detection and Classification\"", "CoNLL: Massimo Nicosia, Alessandro Moschitti: \"Learning Contextual Embeddings for Structural Semantic Similarity using Categorical Information\"", "CoNLL: Fei Dong, Yue Zhang, Jie Yang: \"Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring\"", "CoNLL: Abulhair Saparov, Vijay Saraswat, Tom Mitchell: \"A Probabilistic Generative Grammar for Semantic Parsing\"", "CoNLL: Huadong Chen, Shujian Huang, David Chiang, XIN-YU DAI, Jiajun CHEN: \"Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation\"", "CoNLL: Henry Y. Chen, Ethan Zhou, Jinho D. Choi: \"Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts\"", "CoNLL: Quanzhi Li, Sameena Shah: \"Learning Stock Market Sentiment Lexicon and Sentiment-Oriented Word Vector from StockTwits\"", "CoNLL: Long Duong, Hadi Afshar, Dominique Estival, Glen Pink, Philip Cohen, Mark Johnson: \"Multilingual Semantic Parsing And Code-Switching\"", "CoNLL: Kairit Sirts, Olivier Piguet, Mark Johnson: \"Idea density for predicting Alzheimer's disease from transcribed speech\"", "CoNLL: Shraey Bhatia, Jey Han Lau, Timothy Baldwin: \"An Automatic Approach for Document-level Topic Model Evaluation\"", "CoNLL: Go Inoue, Hiroyuki Shindo, Yuji Matsumoto: \"Joint Prediction of Morphosyntactic Categories for Fine-Grained Arabic Part-of-Speech Tagging Exploiting Tag Dictionary Information\"", "CoNLL: Mans Hulden: \"A phoneme clustering algorithm based on the obligatory contour principle\"", "CoNLL: Tatyana Ruzsics, Tanja Samardzic: \"Neural Sequence-to-sequence Learning of Internal Word Structure\"", "CoNLL: Chris Dyer: \"Should Neural Network Architecture Reflect Linguistic Structure?\"", "CoNLL: Naomi Feldman: \"Rational Distortions of Learners' Linguistic Input\"", "CoNLL: Weiwei Sun, Yantao Du, Xiaojun Wan: \"Parsing for Grammatical Relations via Graph Merging\"", "CoNLL: \u00c9mile Enguehard, Yoav Goldberg, Tal Linzen: \"Exploring the Syntactic Abilities of RNNs with Multi-task Learning\"", "CoNLL: Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci, Roberto Navigli: \"Embedding Words and Senses Together via Joint Knowledge-Enhanced Training\"", "CoNLL: Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer: \"Zero-Shot Relation Extraction via Reading Comprehension\"", "CoNLL: Xun Zhang, Weiwei Sun, Xiaojun Wan: \"The Covert Helps Parse the Overt\"", "CoNLL: Van-Khanh Tran, Le-Minh Nguyen: \"Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks\"", "CoNLL: Yotam Eshel, Noam Cohen, Kira Radinsky, Shaul Markovitch, Ikuya Yamada, Omer Levy: \"Named Entity Disambiguation for Noisy Text\"", "CoNLL: Afra Alishahi, Marie Barking, Grzegorz Chrupa\u0142a: \"Encoding of phonology in a recurrent neural model of grounded speech\"", "CoNLL: Rebecca Sharp, Mihai Surdeanu, Peter Jansen, Marco A. Valenzuela-Esc\u00e1rcega, Peter Clark, Michael Hammond: \"Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification\"", "CoNLL: Desh Raj, SUNIL SAHU, Ashish Anand: \"Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text\"", "CoNLL: Michael J. Paul: \"Feature Selection as Causal Inference: Experiments with Text Classification\"", "CoNLL: Ivan Vuli\u0107, Roy Schwartz, Ari Rappoport, Roi Reichart, Anna Korhonen: \"Automatic Selection of Context Configurations for Improved Class-Specific Word Representations\"", "CoNLL: Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, Noah A. Smith: \"The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task\"", "CoNLL: Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, Dragomir Radev: \"Graph-based Neural Multi-Document Summarization\"", "CoNLL: Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Dan Roth: \"Learning What is Essential in Questions\"", "CoNLL: Dirk Weissenborn, Georg Wiese, Laura Seiffe: \"Making Neural QA as Simple as Possible but not Simpler\"", "demos: Gus Hahn-Powell, Marco A. Valenzuela-Esc\u00e1rcega, Mihai Surdeanu: \"Swanson linking revisited: Accelerating literature-based discovery across domains using a conceptual influence graph\"", "demos: Jason Kessler: \"Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ\"", "demos: Wei-Nan Zhang, Ting Liu, Bing Qin, Yu Zhang, Wanxiang Che, Yanyan Zhao, Xiao Ding: \"Benben: A Chinese Intelligent Conversational Robot\"", "demos: Farhad Bin Siddique, Onno Kampman, Yang Yang, Anik Dey, Pascale Fung: \"Zara Returns: Improved Personality Induction and Adaptation by an Empathetic Virtual Agent\"", "demos: Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, Kevin Knight: \"Hafez: an Interactive Poetry Generation System\"", "demos: Lei Cui, Shaohan Huang, Furu Wei, Chuanqi Tan, Chaoqun Duan, Ming Zhou: \"SuperAgent: A Customer Service Chatbot for E-commerce Websites\"", "demos: Andreas R\u00fcckl\u00e9, Iryna Gurevych: \"End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights\"", "demos: Tuan Duc Nguyen, Khai Mai, Thai-Hoang Pham, Minh Trung Nguyen, Truc-Vien T. Nguyen, Takashi Eguchi, Ryohei Sasano, Satoshi Sekine: \"Extended Named Entity Recognition API and Its Applications in Language Education\"", "demos: Mennatallah El-Assady, Annette Hautli-Janisz, Valentin Gold, Miriam Butt, Katharina Holzinger, Daniel Keim: \"Interactive Visual Analysis of Transcribed Multi-Party Discourse\"", "demos: Stefan Ultes, Lina M. Rojas Barahona, Pei-Hao Su, David Vandyke, Dongho Kim, I\u00f1igo Casanueva, Pawe\u0142 Budzianowski, Nikola Mrk\u0161i\u0107, Tsung-Hsien Wen, Milica Gasic, Steve Young: \"PyDial: A Multi-domain Statistical Dialogue System Toolkit\"", "demos: Omri Abend, Shai Yerushalmi, Ari Rappoport: \"UCCAApp: Web-application for Syntactic and Semantic Phrase-based Annotation\"", "demos: Erik Faessler, Udo Hahn: \"Semedico: A Comprehensive Semantic Search Engine for the Life Sciences\"", "demos: Mariana Neves, Hendrik Folkerts, Marcel Jankrift, Julian Niedermeier, Toni Stachewicz, S\u00f6ren Tietb\u00f6hl, Milena Kraus, Matthias Uflacker: \"Olelo: A Question Answering Application for Biomedicine\"", "demos: Johannes Hellrich, Udo Hahn: \"Exploring Diachronic Lexical Semantics with JeSemE\"", "demos: Xiang Ren, Jiaming Shen, Meng Qu, Xuan Wang, Zeqiu Wu, Qi Zhu, Meng Jiang, Fangbo Tao, Saurabh Sinha, David Liem, Peipei Ping, Richard Weinshilboum, Jiawei Han: \"Life-iNet: A Structured Network-Based Knowledge Exploration and Analytics System for Life Sciences\"", "demos: Anita Ramm, Sharid Lo\u00e1iciga, Annemarie Friedrich, Alexander Fraser: \"Annotating tense, mood and voice for English, French and German\"", "demos: Iain Marshall, Jo\u00ebl Kuiper, Edward Banner, Byron C. Wallace: \"Automating Biomedical Evidence Synthesis: RobotReviewer\"", "demos: Dustin Arendt, Svitlana Volkova: \"ESTEEM: A Novel Framework for Qualitatively Evaluating and Visualizing Spatiotemporal Embeddings in Social Media\"", "demos: Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander Rush: \"OpenNMT: Open-Source Toolkit for Neural Machine Translation\"", "demos: Niket Tandon, Gerard de Melo, Gerhard Weikum: \"WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation\"", "demos: Kateryna Tymoshenko, Alessandro Moschitti, Massimo Nicosia, Aliaksei Severyn: \"RelTextRank: An Open Source Framework for Building Relational Syntactic-Semantic Text Pair Representations\"", "EventStory: Andrey Kutuzov, Erik Velldal, Lilja \u00d8vrelid: \"Tracing armed conflicts with diachronic word embedding models\"", "EventStory: Susan Brown, Claire Bonial, Leo Obrst, Martha Palmer: \"The Rich Event Ontology\"", "EventStory: William Croft, Pavlina Peskova, Michael Regan: \"Integrating Decompositional Event Structures into Storylines\"", "EventStory: Georg Rehm, Julian Moreno Schneider, peter bourgonje, Ankit Srivastava, Jan Nehring, Armin Berger, Luca K\u00f6nig, S\u00f6ren R\u00e4uchle, Jens Gerth: \"Event Detection and Semantic Storytelling: Generating a Travelogue from a large Collection of Personal Letters\"", "EventStory: Martin Atkinson, Jakub Piskorski, Hristo Tanev, Vanni Zavarella: \"On the Creation of a Security-Related Event Corpus\"", "EventStory: Yin Jou Huang, Sadao Kurohashi: \"Improving Shared Argument Identification in Japanese Event Knowledge Acquisition\"", "EventStory: Evangelia Spiliopoulou, Eduard Hovy, Teruko Mitamura: \"Event Detection Using Frame-Semantic Parser\"", "EventStory: Zhichao Hu, Elahe Rahimtoroghi, Marilyn Walker: \"Inference of Fine-Grained Event Causality from Blogs and Films\"", "EventStory: Philippe Laban, Marti Hearst: \"newsLens: building and visualizing long-ranging news stories\"", "EventStory: Tommaso Caselli, Piek Vossen: \"The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction\"", "EventStory: Roxane Segers, Tommaso Caselli, Piek Vossen: \"The Circumstantial Event Ontology (CEO)\"", "EventStory: Yunli Wang, Cyril Goutte: \"Detecting Changes in Twitter Streams using Temporal Clusters of Hashtags\"", "EventStory: Natalie Ahn: \"Inducing Event Types and Roles in Reverse: Using Function to Discover Theme\"", "LaTeCH-CLfL: Conor Kelleher, Mark Keane: \"Plotting Markson's ``Mistress''\"", "LaTeCH-CLfL: Amrith Krishna, Pavan Kumar Satuluri, Pawan Goyal: \"A Dataset for Sanskrit Word Segmentation\"", "LaTeCH-CLfL: \u00c9milie Pag\u00e9-Perron, Maria Sukhareva, Ilya Khait, Christian Chiarcos: \"Machine Translation and Automated Analysis of the Sumerian Language\"", "LaTeCH-CLfL: Andre Blessing, Nora Echelmeyer, Markus John, Nils Reiter: \"An End-to-end Environment for Research Question-Driven Entity Extraction and Network Analysis\"", "LaTeCH-CLfL: Evgeny Kim, Sebastian Pad\u00f3, Roman Klinger: \"Investigating the Relationship between Literary Genres and Emotional Plot Development\"", "LaTeCH-CLfL: Stefania Degaetano-Ortlieb, Elke Teich: \"Modeling intra-textual variation with entropy and surprisal: topical vs. stylistic patterns\"", "LaTeCH-CLfL: Liviu P. Dinu, Ana Sabina Uban: \"Finding a Character's Voice: Stylome Classification on Literary Characters\"", "LaTeCH-CLfL: Pablo Ruiz, Clara Mart\u00ednez Cant\u00f3n, Thierry Poibeau, Elena Gonz\u00e1lez-Blanco: \"Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets\"", "LaTeCH-CLfL: Maria Pia di Buono: \"An Ontology-Based Method for Extracting and Classifying Domain-Specific Compositional Nominal Compounds\"", "LaTeCH-CLfL: G\u00e9raldine Walther, Beno\u00eet Sagot: \"Speeding up corpus development for linguistic research: language documentation and acquisition in Romansh Tuatschin\"", "LaTeCH-CLfL: Maciej Ogrodniczuk, Mateusz Kope\u0107: \"Lexical Correction of Polish Twitter Political Data\"", "LaTeCH-CLfL: Vaibhav Kesarwani, Diana Inkpen, Stan Szpakowicz, Chris Tanasescu: \"Metaphor Detection in a Poetry Corpus\"", "LaTeCH-CLfL: Nina Seemann, Marie-Luis Merten, Michaela Geierhos, Doris Tophinke, Eyke H\u00fcllermeier: \"Annotation Challenges for Reconstructing the Structural Elaboration of Middle Low German\"", "LaTeCH-CLfL: Maria Sukhareva, Francesco Fuscagni, Johannes Daxenberger, Susanne G\u00f6rke, Doris Prechel, Iryna Gurevych: \"Distantly Supervised POS Tagging of Low-Resource Languages under Extreme Data Sparsity: The Case of Hittite\"", "LaTeCH-CLfL: Christopher Hench: \"Phonological Soundscapes in Medieval Poetry\"", "NLPandCSS: Akshita Jha, Radhika Mamidi: \"When does a compliment become sexist? Analysis and classification of ambivalent sexism using twitter data\"", "NLPandCSS: Nikola Ljube\u0161i\u0107, Darja Fi\u0161er, Toma\u017e Erjavec: \"Language-independent Gender Prediction on Twitter\"", "NLPandCSS: Goran Glava\u0161, Federico Nanni, Simone Paolo Ponzetto: \"Cross-Lingual Classification of Topics in Political Texts\"", "NLPandCSS: Andrea Zielinski, Peter Mutschke: \"Mining Social Science Publications for Survey Variables\"", "NLPandCSS: Shrimai Prabhumoye, Samridhi Choudhary, Evangelia Spiliopoulou, Christopher Bogart, Carolyn Rose, Alan W Black: \"Linguistic Markers of Influence in Informal Interactions\"", "NLPandCSS: Rachael Tatman, Leo Stewart, Amandalynne Paullada, Emma Spiro: \"Non-lexical Features Encode Political Affiliation on Twitter\"", "NLPandCSS: Daniel Preo\u0163iuc-Pietro, Jordan Carpenter, Lyle Ungar: \"Personality Driven Differences in Paraphrase Preference\"", "NLPandCSS: Gabriel Murray: \"Modelling Participation in Small Group Social Sequences with Markov Rewards Analysis\"", "NLPandCSS: Michael Yoder, Shruti Rijhwani, Carolyn Ros\u00e9, Lori Levin: \"Code-Switching as a Social Act: The Case of Arabic Wikipedia Talk Pages\"", "NLPandCSS: Zach Wood-Doughty, Michael Smith, David Broniatowski, Mark Dredze: \"How Does Twitter User Behavior Vary Across Demographic Groups?\"", "NLPandCSS: Kristen Johnson, I-Ta Lee, Dan Goldwasser: \"Ideological Phrase Indicators for Classification of Political Discourse Framing on Twitter\"", "NLPandCSS: Trevor Martin: \"community2vec: Vector representations of online communities encode semantic relationships\"", "NLPandCSS: Aseel Addawood, Rezvaneh Rezapour, Omid Abdar, Jana Diesner: \"Telling Apart Tweets Associated with Controversial versus Non-Controversial Topics\"", "NMT: Michael Denkowski, Graham Neubig: \"Stronger Baselines for Trustable Results in Neural Machine Translation\"", "NMT: Makoto Morishita, Yusuke Oda, Graham Neubig, Koichiro Yoshino, Katsuhito Sudoh, Satoshi Nakamura: \"An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation\"", "NMT: Boxing Chen, Colin Cherry, George Foster, Samuel Larkin: \"Cost Weighting for Neural Machine Translation Domain Adaptation\"", "NMT: Isao Goto, Hideki Tanaka: \"Detecting Untranslated Content for Neural Machine Translation\"", "NMT: Markus Freitag, Yaser Al-Onaizan: \"Beam Search Strategies for Neural Machine Translation\"", "NMT: Philipp Koehn, Rebecca Knowles: \"Six Challenges for Neural Machine Translation\"", "NMT: Sajal Choudhary, Prerna Srivastava, Joao Sedoc, Lyle Ungar: \"Domain Aware Neural Dialogue System\"", "NMT: Marine Carpuat, Yogarshi Vyas, Xing Niu: \"Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation\"", "NMT: Jan Niehues, Eunah Cho, Thanh-Le Ha, Alex Waibel: \"Analyzing Neural MT Search and Model Performance\"", "NMT: Raphael Shu, Hideki Nakayama: \"An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation\"", "NMT: Jaesong Lee, JoongHwi Shin, Jun-Seok Kim: \"Interactive Beam Search for Visualizing Neural Machine Translation (extended abstract)\"", "NMT: Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima'an: \"Graph Convolutional Encoders for Syntax-aware Neural Machine Translation\"", "papers: Pengtao Xie, Eric Xing: \"A Constituent-Centric Neural Architecture for Reading Comprehension\"", "papers: Yixin Cao, Lifu Huang, Heng Ji, Xu Chen, Juanzi Li: \"Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding\"", "papers: Roee Aharoni, Yoav Goldberg: \"Morphological Inflection Generation with Hard Monotonic Attention\"", "papers: Jian Ni, Georgiana Dinu, Radu Florian: \"Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection\"", "papers: Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos, Bing Xiang, Bowen Zhou: \"Improved Neural Relation Detection for Knowledge Base Question Answering\"", "papers: Xiaoshi Zhong, Aixin Sun, Erik Cambria: \"Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules\"", "papers: Nikola Mrk\u0161i\u0107, Diarmuid \u00d3 S\u00e9aghdha, Tsung-Hsien Wen, Blaise Thomson, Steve Young: \"Neural Belief Tracker: Data-Driven Dialogue State Tracking\"", "papers: Anil Ramakrishna, Victor R. Mart\u00ednez, Nikolaos Malandrakis, Karan Singla, Shrikanth Narayanan: \"Linguistic analysis of differences in portrayal of movie characters\"", "papers: Leandro Santos, Edilson Anselmo Corr\u00eaa J\u00fanior, Osvaldo Oliveira Jr, Diego Amancio, Let\u00edcia Mansur, Sandra Alu\u00edsio: \"Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts\"", "papers: Steffen Eger, Johannes Daxenberger, Iryna Gurevych: \"Neural End-to-End Learning for Computational Argumentation Mining\"", "papers: Jonas Gehring, Michael Auli, David Grangier, Yann Dauphin: \"A Convolutional Encoder Model for Neural Machine Translation\"", "papers: Meng Zhang, Yang Liu, Huanbo Luan, Maosong Sun: \"Adversarial Training for Unsupervised Bilingual Lexicon Induction\"", "papers: Weiwei Sun, Junjie Cao, Xiaojun Wan: \"Semantic Dependency Parsing via Book Embedding\"", "papers: Ben Athiwaratkun, Andrew Wilson: \"Multimodal Word Distributions\"", "papers: Lianhui Qin, Zhisong Zhang, Hai Zhao, Zhiting Hu, Eric Xing: \"Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification\"", "papers: Saku Sugawara, Yusuke Kido, Hikaru Yokono, Akiko Aizawa: \"Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability\"", "papers: Jinchao Zhang, Mingxuan Wang, Qun Liu, Jie Zhou: \"Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation\"", "papers: Shulin Liu, Yubo Chen, Kang Liu, Jun Zhao: \"Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms\"", "papers: Mingxuan Wang, Zhengdong Lu, Jie Zhou, Qun Liu: \"Deep Neural Machine Translation with Linear Associative Unit\"", "papers: Xinya Du, Junru Shao, Claire Cardie: \"Learning to Ask: Neural Question Generation for Reading Comprehension\"", "papers: Mohammad Taher Pilehvar, Jose Camacho-Collados, Roberto Navigli, Nigel Collier: \"Towards a Seamless Integration of Word Senses into Downstream NLP Applications\"", "papers: Christopher Bryant, Mariano Felice, Ted Briscoe: \"Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction\"", "papers: Qiaolin Xia, Lei Sha, Baobao Chang, Zhifang Sui: \"A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data\"", "papers: Jianbo Ye, Yanran Li, Zhaohui Wu, James Z. Wang, Wenjie Li, Jia Li: \"Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering\"", "papers: Yiming Cui, Zhipeng Chen, si wei, Shijin Wang, Ting Liu, Guoping Hu: \"Attention-over-Attention Neural Networks for Reading Comprehension\"", "papers: Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder, Amir Zadeh, Louis-Philippe Morency: \"Context-Dependent Sentiment Analysis in User-Generated Videos\"", "papers: Satoshi Akasaki, Nobuhiro Kaji: \"Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems\"", "papers: Ting Liu, Yiming Cui, Qingyu Yin, Wei-Nan Zhang, Shijin Wang, Guoping Hu: \"Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution\"", "papers: Daniel Hershcovich, Omri Abend, Ari Rappoport: \"A Transition-Based Directed Acyclic Graph Parser for UCCA\"", "papers: Rie Johnson, Tong Zhang: \"Deep Pyramid Convolutional Neural Networks for Text Categorization\"", "papers: Shizhu He, Cao Liu, Kang Liu, Jun Zhao: \"Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning\"", "papers: Chengyu Wang, Junchi Yan, Aoying Zhou, Xiaofeng He: \"Transductive Non-linear Learning for Chinese Hypernym Prediction\"", "papers: Hesam Amoualian, Wei Lu, Eric Gaussier, Georgios Balikas, Massih R Amini, Marianne Clausel: \"Topical Coherence in LDA-based Models through Induced Segmentation\"", "papers: Liangming Pan, Chengjiang Li, Juanzi Li, Jie Tang: \"Prerequisite Relation Learning for Concepts in MOOCs\"", "papers: Jianpeng Cheng, Siva Reddy, Vijay Saraswat, Mirella Lapata: \"Learning Structured Natural Language Representations for Semantic Parsing\"", "papers: Milan Gritta, Mohammad Taher Pilehvar, Nut Limsopatham, Nigel Collier: \"Vancouver Welcomes You! Minimalist Location Metonymy Resolution\"", "papers: Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, Bo Xu: \"Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\"", "papers: Tomer Cagan, Stefan L. Frank, Reut Tsarfaty: \"Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG)\"", "papers: Alina Wr\u00f3blewska, Katarzyna Krasnowska-Kiera\u015b: \"Polish evaluation dataset for compositional distributional semantics models\"", "papers: Shervin Malmasi, Mark Dras, Mark Johnson, Lan Du, Magdalena Wolska: \"Unsupervised Text Segmentation Based on Native Language Characteristics\"", "papers: Alex Gittens, Dimitris Achlioptas, Michael W. Mahoney: \"Skip-Gram - Zipf + Uniform = Vector Additivity\"", "papers: Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen Gong, Steven Truong, Jianfeng Gao: \"A Nested Attention Neural Hybrid Model for Grammatical Error Correction\"", "papers: Tiancheng Zhao, Ran Zhao, Maxine Eskenazi: \"Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders\"", "papers: Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu, Jun Zhao: \"An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge\"", "papers: Jey Han Lau, Timothy Baldwin, Trevor Cohn: \"Topically Driven Neural Language Model\"", "papers: Mingbin Xu, Hui Jiang, Sedtawut Watcharawittayakul: \"A Local Detection Approach for Named Entity Recognition and Mention Detection\"", "papers: Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, Diana Inkpen: \"Enhanced LSTM for Natural Language Inference\"", "papers: Ruidan He, Wee Sun Lee, Hwee Tou Ng, Daniel Dahlmeier: \"An Unsupervised Neural Attention Model for Aspect Extraction\"", "papers: Marek Rei: \"Semi-supervised Multitask Learning for Sequence Labeling\"", "papers: Ella Rabinovich, Noam Ordan, Shuly Wintner: \"Found in Translation: Reconstructing Phylogenetic Language Trees from Translations\"", "papers: Junjie Cao, Sheng Huang, Weiwei Sun, Xiaojun Wan: \"Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs\"", "papers: Yilin Niu, Ruobing Xie, Zhiyuan Liu, Maosong Sun: \"Improved Word Representation Learning with Sememes\"", "papers: Soichiro Murakami, Akihiko Watanabe, Akira Miyazawa, Keiichi Goshima, Toshihiko Yanase, Hiroya Takamura, Yusuke Miyao: \"Learning to Generate Market Comments from Stock Prices\"", "papers: Dat Tien Nguyen, Shafiq Joty: \"A Neural Local Coherence Model\"", "papers: Xinchi Chen, Zhan Shi, Xipeng Qiu, Xuanjing Huang: \"Adversarial Multi-Criteria Learning for Chinese Word Segmentation\"", "papers: Qiao Qian, Minlie Huang, Jinhao Lei, xiaoyan zhu: \"Linguistically Regularized LSTM for Sentiment Classification\"", "papers: Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou: \"Selective Encoding for Abstractive Sentence Summarization\"", "papers: Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, Ming Zhou: \"Gated Self-Matching Networks for Reading Comprehension and Question Answering\"", "papers: Shuhei Kurita, Daisuke Kawahara, Sadao Kurohashi: \"Neural Joint Model for Transition-based Chinese Syntactic Analysis\"", "papers: Xuepeng Wang, Kang Liu, Jun Zhao: \"Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors\"", "papers: Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min Zhang, Guodong Zhou: \"Modeling Source Syntax for Neural Machine Translation\"", "papers: Hai Ye, Wenhan Chao, Zhunchen Luo, Zhoujun Li: \"Jointly Extracting Relations with Class Ties via Effective Deep Ranking\"", "papers: Jie Yang, Yue Zhang, Fei Dong: \"Neural Word Segmentation with Rich Pretraining\"", "papers: Jiyuan Zhang, Yang Feng, Dong Wang, Yang Wang, Andrew Abel, Shiyue Zhang, Andi Zhang: \"Flexible and Creative Chinese Poetry Generation Using Neural Memory\"", "papers: Yankai Lin, Zhiyuan Liu, Maosong Sun: \"Neural Relation Extraction with Multi-lingual Attention\"", "papers: Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, Jun Zhao: \"Automatically Labeled Data Generation for Large Scale Event Extraction\"", "papers: Pengfei Liu, Xipeng Qiu, Xuanjing Huang: \"Adversarial Multi-task Learning for Text Classification\"", "papers: Hiroki Ouchi, Hiroyuki Shindo, Yuji Matsumoto: \"Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis\"", "papers: Wei Song, Dong Wang, Ruiji Fu, Lizhen Liu, Ting Liu, Guoping Hu: \"Discourse Mode Identification in Essays\"", "papers: Marcel Bollmann, Joachim Bingel, Anders S\u00f8gaard: \"Learning attention for historical text normalization by learning to pronounce\"", "papers: Vlad Niculae, Joonsuk Park, Claire Cardie: \"Argument Mining with Structured SVMs and RNNs\"", "papers: Yu Wu, wei wu, Chen Xing, Ming Zhou, Zhoujun Li: \"Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots\"", "papers: Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun: \"CANE: Context-Aware Network Embedding for Relation Modeling\"", "papers: Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li, Ming Zhou: \"Sequence-to-Dependency Neural Machine Translation\"", "papers: Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini: \"Creating Training Corpora for NLG Micro-Planners\"", "papers: Ellie Pavlick, Marius Pasca: \"Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition\"", "papers: Yevgeni Berzak, Chie Nakamura, Suzanne Flynn, Boris Katz: \"Predicting Native Language from Gaze\"", "papers: Abhijit Mishra, Kuntal Dey, Pushpak Bhattacharyya: \"Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network\"", "papers: Chenhao Tan, Dallas Card, Noah A. Smith: \"Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts\"", "papers: Katharina Kann, Ryan Cotterell, Hinrich Sch\u00fctze: \"One-Shot Neural Cross-Lingual Transfer for Paradigm Completion\"", "papers: Maxime Peyrard, Judith Eckle-Kohler: \"Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization\"", "papers: Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer: \"Affect-LM: A Neural Language Model for Customizable Affective Text Generation\"", "papers: Hongmin Wang, Yue Zhang, GuangYong Leonard Chan, Jie Yang, Hai Leong Chieu: \"Universal Dependencies Parsing for Colloquial Singaporean English\"", "papers: Kyle Richardson, Jonas Kuhn: \"Learning Semantic Correspondences in Technical Documentation\"", "papers: Masashi Yoshikawa, Hiroshi Noji, Yuji Matsumoto: \"A* CCG Parsing with a Supertag and Dependency Factored Model\"", "papers: Martin Villalba, Christoph Teichmann, Alexander Koller: \"Generating Contrastive Referring Expressions\"", "papers: Yangfeng Ji, Noah A. Smith: \"Neural Discourse Structure for Text Categorization\"", "papers: William Foland, James H. Martin: \"Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks\"", "papers: Omri Abend, Ari Rappoport: \"The State of the Art in Semantic Representation\"", "papers: Iacer Calixto, Qun Liu, Nick Campbell: \"Doubly-Attentive Decoder for Multi-modal Neural Machine Translation\"", "papers: Navid Rekabsaz, Mihai Lupu, Artem Baklanov, Alexander D\u00fcr, Linda Andersson, Allan Hanbury: \"Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models\"", "papers: Mikel Artetxe, Gorka Labaka, Eneko Agirre: \"Learning bilingual word embeddings with (almost) no bilingual data\"", "papers: Avinesh PVS, Christian M. Meyer: \"Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback\"", "papers: Clara Vania, Adam Lopez: \"From Characters to Words to in Between: Do We Capture Morphology?\"", "papers: Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aur\u00e9lie Herbelot, Moin Nabi, Enver Sangineto, Raffaella Bernardi: \"FOIL it! Find One mismatch between Image and Language caption\"", "papers: Takaaki Hori, Shinji Watanabe, John Hershey: \"Joint CTC/attention decoding for end-to-end speech recognition\"", "papers: Sina Zarrie\u00df, David Schlangen: \"Obtaining referential word meanings from visual and distributional information: Experiments on object naming\"", "papers: Shonosuke Ishiwatari, Jingtao Yao, Shujie Liu, Mu Li, Ming Zhou, Naoki Yoshinaga, Masaru Kitsuregawa, Weijia Jia: \"Chunk-based Decoder for Neural Machine Translation\"", "papers: Ver\u00f3nica P\u00e9rez-Rosas, Rada Mihalcea, Kenneth Resnicow, Satinder Singh, Lawrence An: \"Understanding and Predicting Empathic Behavior in Counseling Therapy\"", "papers: Grzegorz Chrupa\u0142a, Lieke Gelderloos, Afra Alishahi: \"Representations of language in a model of visually grounded speech signal\"", "papers: Ivan Vuli\u0107, Nikola Mrk\u0161i\u0107, Roi Reichart, Diarmuid \u00d3 S\u00e9aghdha, Steve Young, Anna Korhonen: \"Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules\"", "papers: Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass: \"What do Neural Machine Translation Models Learn about Morphology?\"", "papers: Ruochen Xu, Yiming Yang: \"Cross-lingual Distillation for Text Classification\"", "papers: Alexander Fonarev, Oleksii Grinchuk, Gleb Gusev, Pavel Serdyukov, Ivan Oseledets: \"Riemannian Optimization for Skip-Gram Negative Sampling\"", "papers: Jeffrey Lund, Connor Cook, Kevin Seppi, Jordan Boyd-Graber: \"Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling\"", "papers: Muhammad Abdul-Mageed, Lyle Ungar: \"EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks\"", "papers: Jason D Williams, Kavosh Asadi, Geoffrey Zweig: \"Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning\"", "papers: Frederick Liu, Han Lu, Chieh Lo, Graham Neubig: \"Learning Character-level Compositionality with Visual Features\"", "papers: Kazuya Kawakami, Chris Dyer, Phil Blunsom: \"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling\"", "papers: Zhe Gan, Chunyuan Li, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin: \"Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling\"", "papers: Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom: \"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems\"", "papers: Lanbo She, Joyce Chai: \"Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication\"", "papers: Jiwei Tan, Xiaojun Wan, Jianguo Xiao: \"Abstractive Document Summarization with a Graph-Based Attentional Neural Model\"", "papers: Matthew Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power: \"Semi-supervised sequence tagging with bidirectional language models\"", "papers: Chris Hokamp, Qun Liu: \"Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search\"", "papers: Joao Sedoc, Jean Gallier, Dean Foster, Lyle Ungar: \"Semantic Word Clusters Using Signed Spectral Clustering\"", "papers: Ines Rehbein, Josef Ruppenhofer: \"Detecting annotation noise in automatically labelled data\"", "papers: David Harwath, James Glass: \"Learning Word-Like Units from Joint Audio-Visual Analysis\"", "papers: Jan Buys, Phil Blunsom: \"Robust Incremental Neural Semantic Graph Parsing\"", "papers: Umashanthi Pavalanathan, Jim Fitzpatrick, Scott Kiesling, Jacob Eisenstein: \"A Multidimensional Lexicon for Interpersonal Stancetaking\"", "papers: Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William Cohen: \"Semi-Supervised QA with Generative Domain-Adaptive Nets\"", "papers: Ramakanth Pasunuru, Mohit Bansal: \"Multi-Task Video Captioning with Video and Entailment Generation\"", "papers: Young-Bum Kim, Karl Stratos, Dongchan Kim: \"Adversarial Adaptation of Synthetic or Stale Data\"", "papers: An Thanh Nguyen, Byron Wallace, Junyi Jessy Li, Ani Nenkova, Matthew Lease: \"Aggregating and Predicting Sequence Labels from Crowd Annotations\"", "papers: Hao Peng, Sam Thomson, Noah A. Smith: \"Deep Multitask Learning for Semantic Dependency Parsing\"", "papers: Julia Kreutzer, Artem Sokolov, Stefan Riezler: \"Bandit Structured Prediction for Neural Sequence-to-Sequence Learning\"", "papers: Huadong Chen, Shujian Huang, David Chiang, Jiajun Chen: \"Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder\"", "papers: Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, Ni Lao: \"Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision\"", "papers: Michael Bloodgood, Benjamin Strauss: \"Using Global Constraints and Reranking to Improve Cognates Detection\"", "papers: Daniel Preo\u0163iuc-Pietro, Ye Liu, Daniel Hopkins, Lyle Ungar: \"Beyond Binary Labels: Political Ideology Prediction of Twitter Users\"", "papers: Swee Kiat Lim, Aldrian Obaja Muis, Wei Lu, Chen Hui Ong: \"MalwareTextDB: A Database for Annotated Malware Articles\"", "papers: Fan Zhang, Homa B. Hashemi, Rebecca Hwa, Diane Litman: \"A Corpus of Annotated Revisions for Studying Argumentative Writing\"", "papers: Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, Li Deng: \"Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access\"", "papers: Yasuhide Miura, Motoki Taniguchi, Tomoki Taniguchi, Tomoko Ohkuma: \"Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction\"", "papers: Arzoo Katiyar, Claire Cardie: \"Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees\"", "papers: Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, Joelle Pineau: \"Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses\"", "papers: Yang Xu, David Reitter: \"Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance\"", "papers: Luheng He, Kenton Lee, Mike Lewis, Luke Zettlemoyer: \"Deep Semantic Role Labeling: What Works and What's Next\"", "papers: Abhisek Chakrabarty, Onkar Arun Pandit, Utpal Garain: \"Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks\"", "papers: Jack Hopkins, Douwe Kiela: \"Automatically Generating Rhythmic Verse with Neural Networks\"", "papers: Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, Jonathan Berant: \"Coarse-to-Fine Question Answering for Long Documents\"", "papers: Yusuke Oda, Philip Arthur, Graham Neubig, Koichiro Yoshino, Satoshi Nakamura: \"Neural Machine Translation via Binary Code Prediction\"", "papers: Chunting Zhou, Graham Neubig: \"Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction\"", "papers: Bishan Yang, Tom Mitchell: \"Leveraging Knowledge Bases in LSTMs for Improving Machine Reading\"", "papers: Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, Ruslan Salakhutdinov: \"Gated-Attention Readers for Text Comprehension\"", "papers: Shruti Rijhwani, Royal Sequiera, Monojit Choudhury, Kalika Bali, Chandra Shekhar Maddila: \"Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique\"", "papers: Pradeep Dasigi, Waleed Ammar, Chris Dyer, Eduard Hovy: \"Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\"", "papers: Omid Bakhshandeh, James Allen: \"Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task\"", "papers: Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi: \"Deep Keyphrase Generation\"", "papers: Preksha Nema, Mitesh M. Khapra, Anirban Laha, Balaraman Ravindran: \"Diversity driven attention model for query-based abstractive summarization\"", "papers: Sida I. Wang, Samuel Ginn, Percy Liang, Christopher D. Manning: \"Naturalizing a Programming Language via Interactive Learning\"", "papers: Maxim Rabinovich, Mitchell Stern, Dan Klein: \"Abstract Syntax Networks for Code Generation and Semantic Parsing\"", "papers: Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, Heng Ji: \"Cross-lingual Name Tagging and Linking for 282 Languages\"", "papers: Gabriel Doyle, Amir Goldberg, Sameer Srivastava, Michael Frank: \"Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations\"", "papers: Edmund Tong, Amir Zadeh, Cara Jones, Louis-Philippe Morency: \"Combating Human Trafficking with Multimodal Deep Models\"", "papers: Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes: \"Reading Wikipedia to Answer Open-Domain Questions\"", "papers: Liangguo Wang, Jing Jiang, Hai Leong Chieu, Chen Hui Ong, Dandan Song, Lejian Liao: \"Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains\"", "papers: Tarek Sakakini, Suma Bhat, Pramod Viswanath: \"MORSE: Semantic-ally Drive-n MORpheme SEgment-er\"", "papers: Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, Luke Zettlemoyer: \"Learning a Neural Semantic Parser from User Feedback\"", "papers: Kristen Johnson, Di Jin, Dan Goldwasser: \"Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter\"", "papers: John Wieting, Kevin Gimpel: \"Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings\"", "papers: Jing Ma, Wei Gao, Kam-Fai Wong: \"Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning\"", "papers: Fangzhao Wu, Yongfeng Huang, Jun Yan: \"Active Sentiment Domain Adaptation\"", "papers: Ryan Cotterell, Jason Eisner: \"Probabilistic Typology: Deep Generative Models of Vowel Inventories\"", "papers: Yanzhuo Ding, Yang Liu, Huanbo Luan, Maosong Sun: \"Visualizing and Understanding Neural Machine Translation\"", "papers: Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer: \"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\"", "papers: Danilo Croce, Simone Filice, Giuseppe Castellucci, Roberto Basili: \"Deep Learning in Semantic Kernel Spaces\"", "papers: Dmitry Ustalov, Alexander Panchenko, Chris Biemann: \"Automatic Induction of Synsets from a Graph of Synonyms\"", "papers: Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu, Maosong Sun: \"Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization\"", "papers: Jacob Andreas, Anca Dragan, Dan Klein: \"Translating Neuralese\"", "papers: Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer: \"Neural AMR: Sequence-to-Sequence Models for Parsing and Generation\"", "papers: Kechen Qin, Lu Wang, Joseph Kim: \"Joint Modeling of Content and Discourse Relations in Dialogues\"", "papers: Adams Wei Yu, Hongrae Lee, Quoc Le: \"Learning to Skim Text\"", "papers: Abigail See, Peter J. Liu, Christopher D. Manning: \"Get To The Point: Summarization with Pointer-Generator Networks\"", "papers: Nicholas Andrews, Mark Dredze, Benjamin Van Durme, Jason Eisner: \"Bayesian Modeling of Lexical Resources for Low-Resource Settings\"", "papers: Mohit Iyyer, Wen-tau Yih, Ming-Wei Chang: \"Search-based Neural Structured Learning for Sequential Question Answering\"", "papers: He He, Anusha Balakrishnan, Mihail Eric, Percy Liang: \"Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings\"", "papers: Iryna Haponchyk, Alessandro Moschitti: \"Don't understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures\"", "papers: Kelvin Guu, Panupong Pasupat, Evan Liu, Percy Liang: \"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood\"", "papers: Akira Sasaki, Kazuaki Hanawa, Naoaki Okazaki, Kentaro Inui: \"Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization\"", "papers: Yun Chen, Yang Liu, Yong Cheng, Victor O.K. Li: \"A Teacher-Student Framework for Zero-Resource Neural Machine Translation\"", "papers: Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy: \"An Interpretable Knowledge Transfer Model for Knowledge Base Completion\"", "papers: Young-Bum Kim, Karl Stratos, Dongchan Kim: \"Domain Attention with an Ensemble of Experts\"", "papers: Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing Zhu, Songfang Huang, Rui Yan, Dongyan Zhao: \"Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix\"", "papers: Anssi Yli-Jyr\u00e4, Carlos G\u00f3mez-Rodr\u00edguez: \"Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing\"", "papers: Mitchell Stern, Jacob Andreas, Dan Klein: \"A Minimal Span-Based Neural Constituency Parser\"", "papers: Yassine Mrabet, Halil Kilicoglu, Dina Demner-Fushman: \"TextFlow: A Text Similarity Measure based on Continuous Sequences\"", "papers: Jing Lu, Vincent Ng: \"Joint Learning for Event Coreference Resolution\"", "papers: Maxwell Forbes, Yejin Choi: \"Verb Physics: Relative Physical Knowledge of Actions and Objects\"", "papers: Takuya Matsuzaki, Takumi Ito, Hidenao Iwane, Hirokazu Anai, Noriko H. Arai: \"Semantic Parsing of Pre-university Math Problems\"", "papers: Vivek Srikumar: \"An Algebra for Feature Extraction\"", "papers: Pengcheng Yin, Graham Neubig: \"A Syntactic Neural Model for General-Purpose Code Generation\"", "papers: Corina Florescu, Cornelia Caragea: \"PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents\"", "papers: Daniel Fern\u00e1ndez-Gonz\u00e1lez, Carlos G\u00f3mez-Rodr\u00edguez: \"A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing\"", "papers: Lotem Peled, Roi Reichart: \"Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation\"", "Repl4NLP: Ye Zhang, Matthew Lease, Byron Wallace: \"Active Discriminative Text Representation Learning\"", "Repl4NLP: Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, Sune Lehmann: \"Using millions of emoji occurrences to pretrain any-domain models for detecting emotion, sentiment and sarcasm\"", "Repl4NLP: Yonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, James Glass: \"Evaluating Layers of Representation in Neural Machine Translation on Syntactic and Semantic Tagging\"", "Repl4NLP: Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro Sordoni, Philip Bachman, Saizheng Zhang, Sandeep Subramanian, Adam Trischler: \"Machine Comprehension by Text-to-Text Neural Question Generation\"", "Repl4NLP: Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester: \"Emergent Predication Structure in Hidden State Vectors of Neural Readers\"", "Repl4NLP: Joe Cheri, Pushpak Bhattacharyya: \"Towards Harnessing Memory Networks for Coreference Resolution\"", "Repl4NLP: Dongyun Liang, Weiran Xu, Yinge Zhao: \"Combining Word-Level and Character-Level Representations for Relation Classification of Informal Text\"", "Repl4NLP: Anna Potapenko, Artem Popov: \"Regularized Topic Models for Sparse Interpretable Word Embeddings\"", "Repl4NLP: Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam T. Kalai: \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\"", "Repl4NLP: Xing Fan, Emilio Monti, Lambert Mathias, Markus Dreyer: \"Transfer Learning for Neural Semantic Parsing\"", "Repl4NLP: Guang-He Lee, Yun-Nung Chen: \"MUSE: Modularizing Unsupervised Sense Embeddings\"", "Repl4NLP: Yelong Shen, Po-Sen Huang, Ming-Wei Chang, Jianfeng Gao: \"Modeling Large-Scale Structured Relationships with Shared Memory for Knowledge Base Completion\"", "Repl4NLP: Rudolf Kadlec, Ondrej Bajgar, Jan Kleindienst: \"Knowledge Base Completion: Baselines Strike Back\"", "Repl4NLP: Sebastian Brarda, Philip Yeres, Samuel Bowman: \"Sequential Attention: A Context-Aware Alignment Function for Machine Reading\"", "Repl4NLP: Jan Rygl, Jan Pomik\u00e1lek, Radim \u0158eh\u016f\u0159ek, Michal R\u016f\u017ei\u010dka, V\u00edt Novotn\u00fd, Petr Sojka: \"Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines\"", "Repl4NLP: Nanyun Peng, Mark Dredze: \"Multi-task Domain Adaptation for Sequence Tagging\"", "Repl4NLP: Shyam Upadhyay, Kai-Wei Chang, Matt Taddy, Adam Kalai, James Zou: \"Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context\"", "Repl4NLP: Sheng Chen, Akshay Soni, Aasish Pappu, Yashar Mehdad: \"DocTag2Vec: An Embedding Based Multi-label Learning Approach for Document Tagging\"", "Repl4NLP: Karol Grzegorczyk, Marcin Kurdziel: \"Binary Paragraph Vectors\"", "Repl4NLP: Dennis Singh Moirangthem, Jegyung Son, Minho Lee: \"Representing Compositionality based on Multiple Timescales Gated Recurrent Neural Networks with Adaptive Temporal Hierarchy for Character-Level Language Models\"", "Repl4NLP: Pranava Swaroop Madhyastha, Cristina Espa\u00f1a-Bonet: \"Learning Bilingual Projections of Embeddings for Vocabulary Expansion in Machine Translation\"", "Repl4NLP: Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling: \"Learning to Compose Words into Sentences with Reinforcement Learning\"", "Repl4NLP: Teresa Botschen, Hatem Mousselly Sergieh, Iryna Gurevych: \"Prediction of Frame-to-Frame Relations in the FrameNet Hierarchy with Frame Embeddings\"", "Repl4NLP: Holger Schwenk, Matthijs Douze: \"Learning Joint Multilingual Sentence Representations with Neural Machine Translation\"", "Repl4NLP: Julius Kunze, Louis Kirsch, Ilia Kurenkov, Andreas Krug, Jens Johannsmeier, Sebastian Stober: \"Transfer Learning for Speech Recognition on a Budget\"", "Repl4NLP: Shima Asaadi, Sebastian Rudolph: \"Gradual Learning of Matrix-Space Models of Language for Sentiment Analysis\"", "Repl4NLP: Fr\u00e9deric Godin, Joni Dambre, Wesley De Neve: \"Improving Language Modeling using Densely Connected Recurrent Neural Networks\"", "Repl4NLP: Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman: \"NewsQA: A Machine Comprehension Dataset\"", "Repl4NLP: Lawrence Phillips, Kyle Shaffer, Dustin Arendt, Nathan Hodas, Svitlana Volkova: \"Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in Twitter Streams\"", "Repl4NLP: Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, Virginia de Sa: \"Rethinking Skip-thought: A Neighborhood based Approach\"", "Repl4NLP: Hannes Schulz, Jeremie Zumer, Layla El Asri, Shikhar Sharma: \"A Frame Tracking Model for Memory-Enhanced Dialogue Systems\"", "Repl4NLP: Pablo Gamallo: \"Sense Contextualization in a Dependency-Based Compositional Distributional Model\"", "Repl4NLP: Caglar Gulcehre, Francis Dutil, Adam Trischler, Yoshua Bengio: \"Plan, Attend, Generate: Character-Level Neural Machine Translation with Planning\"", "Repl4NLP: Paul Michel, Abhilasha Ravichander, Shruti Rijhwani: \"Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology-Based Representations\"", "Repl4NLP: Sandeep Subramanian, Sai Rajeswar, Francis Dutil, Chris Pal, Aaron Courville: \"Adversarial Generation of Natural Language\"", "Repl4NLP: Yanyao Shen, Hyokun Yun, Zachary Lipton, Yakov Kronrod, Animashree Anandkumar: \"Deep Active Learning for Named Entity Recognition\"", "Repl4NLP: Andrew Drozdov, Samuel Bowman: \"The Coadaptation Problem when Learning How and What to Compose\"", "Repl4NLP: Lifu Tu, Kevin Gimpel, Karen Livescu: \"Learning to Embed Words in Context for Syntactic Tasks\"", "Repl4NLP: Alexander Johansen, Richard Socher: \"Learning when to skim and when to read\"", "Repl4NLP: Franziska Horn: \"Context encoders as a simple but powerful extension of word2vec\"", "RoboNLP: Bed\u0159ich Pi\u0161l, David Mare\u010dek: \"Communication with Robots using Multilayer Recurrent Networks\"", "RoboNLP: Yordan Hristov, Svetlin Penkov, Alex Lascarides, Subramanian Ramamoorthy: \"Grounding Symbols in Multi-Modal Instructions\"", "RoboNLP: Matthew Marge, Claire Bonial, Ashley Foots, Cory Hayes, Cassidy Henry, Kimberly Pollard, Ron Artstein, Clare Voss, David Traum: \"Exploring Variation of Natural Human Commands to a Robot in a Collaborative Navigation Task\"", "RoboNLP: Siddharth Karamcheti, Edward Clem Williams, Dilip Arumugam, Mina Rhee, Nakul Gopalan, Lawson L.S. Wong, Stefanie Tellex: \"A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented Instructions\"", "RoboNLP: Peter Lindes, Aaron Mininger, James R. Kirk, John E. Laird: \"Grounding Language for Interactive Task Learning\"", "RoboNLP: Li Lucy, Jon Gauthier: \"Are Distributional Representations Ready for the Real World? Evaluating Word Vectors for Grounded Perceptual Meaning\"", "RoboNLP: Jekaterina Novikova, Christian Dondrup, Ioannis Papaioannou, Oliver Lemon: \"Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction\"", "RoboNLP: Anjali Narayan-Chen, Colin Graber, Mayukh Das, Md Rakibul Islam, Soham Dan, Sriraam Natarajan, Janardhan Rao Doppa, Julia Hockenmaier, Martha Palmer, Dan Roth: \"Towards Problem Solving Agents that Communicate and Learn\"", "RoboNLP: Yanchao Yu, Arash Eshghi, Oliver Lemon: \"Learning how to Learn: An Adaptive Dialogue Agent for Incrementally Learning Visually Grounded Word Meanings\"", "RoboNLP: Jesse Thomason, Jivko Sinapov, Raymond Mooney: \"Guiding Interaction Behaviors for Multi-modal Grounded Language Learning\"", "RoboNLP: Andrea Vanzo, Danilo Croce, Roberto Basili, Daniele Nardi: \"Structured Learning for Context-aware Spoken Language Understanding of Robotic Commands\"", "RoboNLP: Muhannad Alomari, Paul Duckworth, Majd Hawasly, David C. Hogg, Anthony G. Cohn: \"Natural Language Grounding and Grammar Induction for Robotic Manipulation Commands\"", "SemEval: Ted Pedersen: \"Duluth at SemEval-2017 Task 7 : Puns Upon a Midnight Dreary, Lexical Semantics for the Weak and Weary\"", "SemEval: Junqing He, Long Wu, Xuemin Zhao, Yonghong Yan: \"HCCL at SemEval-2017 Task 2: Combining Multilingual Word Embeddings and Transliteration Model for Semantic Similarity\"", "SemEval: Georgios Balikas: \"TwiSe at SemEval-2017 Task 4: Five-point Twitter Sentiment Classification and Quantification\"", "SemEval: Rik van Noord, Johan Bos: \"The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing\"", "SemEval: Sergio Jimenez, George Due\u00f1as, Lorena Gaitan, Jorge Segura: \"RUFINO at SemEval-2017 Task 2: Cross-lingual lexical similarity by extending PMI and word embeddings systems with a Swadesh's-like list\"", "SemEval: Andrew Moore, Paul Rayson: \"Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines\"", "SemEval: Mickael Rouvier: \"LIA at SemEval-2017 Task 4: An Ensemble of Neural Networks for Sentiment Classification\"", "SemEval: Jan Milan Deriu, Mark Cieliebak: \"SwissAlps at SemEval-2017 Task 3: Attention-based Convolutional Neural Network for Community Question Answering\"", "SemEval: Mirela-Stefania Duma, Wolfgang Menzel: \"SEF@UHH at SemEval-2017 Task 1: Unsupervised Knowledge-Free Semantic Textual Similarity via Paragraph Vector\"", "SemEval: Simon M\u00fcller, Tobias Huonder, Jan Deriu, Mark Cieliebak: \"TopicThunder at SemEval-2017 Task 4: Sentiment Classification Using a Convolutional Neural Network with Distant Supervision\"", "SemEval: Roman Kern, Stefan Falk, Andi Rexha: \"Know-Center at SemEval-2017 Task 10: Sequence Classification with the CODE Annotator\"", "SemEval: Sarath P R, Manikandan R, Yoshiki Niwa: \"Hitachi at SemEval-2017 Task 12: System for temporal information extraction from clinical notes\"", "SemEval: Sabino Miranda-Jim\u00e9nez, Mario Graff, Eric Sadit Tellez, Daniela Moctezuma: \"INGEOTEC at SemEval 2017 Task 4: A B4MSA Ensemble based on Genetic Programming for Twitter Sentiment Analysis\"", "SemEval: Ankit Vadehra: \"UWAV at SemEval-2017 Task 7: Automated feature-based system for locating puns\"", "SemEval: Samuel Doogan, Aniruddha Ghosh, Hanyang Chen, Tony Veale: \"Idiom Savant at Semeval-2017 Task 7: Detection and Interpretation of English Puns\"", "SemEval: Enrico Mensa, Daniele P. Radicioni, Antonio Lieto: \"MERALI at SemEval-2017 Task 2 Subtask 1: a Cognitively Inspired approach\"", "SemEval: Quanzhi Li, Sameena Shah, Armineh Nourbakhsh, Rui Fang, Xiaomo Liu: \"funSentiment at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs  Using Word  Vectors Built from StockTwits and Twitter\"", "SemEval: Narges Tabari, Armin Seyeditabari, Wlodek Zadrozny: \"SentiHeros at SemEval-2017 Task 5: An application of Sentiment Analysis on Financial Tweets\"", "SemEval: Julien Tourille, Olivier Ferret, Xavier Tannier, Aur\u00e9lie N\u00e9v\u00e9ol: \"LIMSI-COT at SemEval-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives\"", "SemEval: Christos Baziotis, Nikos Pelekis, Christos Doulkeridis: \"DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison\"", "SemEval: Deger Ayata, Murat Saraclar, Arzucan Ozgur: \"BUSEM at SemEval-2017 Task 4A Sentiment Analysis with Word Embedding and Long Short Term Memory RNN Approaches\"", "SemEval: Tzu-Hsuan Yang, Tzu-Hsuan Tseng, Chia-Ping Chen: \"deepSA at SemEval-2017 Task 4: Interpolated Deep Neural Networks for Sentiment Analysis in Twitter\"", "SemEval: David Lozi\u0107, Doria \u0160ari\u0107, Ivan Toki\u0107, Zoran Medi\u0107, Jan \u0160najder: \"TakeLab at SemEval-2017 Task 4: Recent Deaths and the Power of Nostalgia in Sentiment Analysis in Twitter\"", "SemEval: Gerasimos Lampouras, Andreas Vlachos: \"Sheffield at SemEval-2017 Task 9: Transition-based language generation from AMR.\"", "SemEval: Marin Kukova\u010dec, Juraj Malenica, Ivan Mr\u0161i\u0107, Antonio \u0160ajatovi\u0107, Domagoj Alagi\u0107, Jan \u0160najder: \"TakeLab at SemEval-2017 Task 6: \\#RankingHumorIn4Pages\"", "SemEval: Robert Speer, Joanna Lowry-Duda: \"ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge\"", "SemEval: Biswanath Barik, Erwin Marsi: \"NTNU-2 at SemEval-2017 Task 10: Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents\"", "SemEval: Behrang QasemiZadeh, Laura Kallmeyer: \"HHU at SemEval-2017 Task 2: Fast Hash-Based Embeddings for Semantic Word Similarity Assessment\"", "SemEval: Filip \u0160aina, Toni Kukurin, Lukrecija Pulji\u0107, Mladen Karan, Jan \u0160najder: \"TakeLab-QA at SemEval-2017 Task 3: Classification Experiments for Answer Retrieval in Community QA\"", "SemEval: Sarah Kohail, Amr Rekaby Salama, Chris Biemann: \"STS-UHH at SemEval-2017 Task 1: Scoring Semantic Textual Similarity Using Supervised and Unsupervised Ensemble\"", "SemEval: Sean MacAvaney, Arman Cohan, Nazli Goharian: \"GUIR at SemEval-2017 Task 12: A Framework for Cross-Domain Clinical Temporal Information Extraction\"", "SemEval: Symeon Symeonidis, John Kordonis, Dimitrios Effrosynidis, Avi Arampatzis: \"DUTH at SemEval-2017 Task 5: Sentiment Predictability in Financial Microblogging and News Articles\"", "SemEval: Yichun Yin, Yangqiu Song, Ming Zhang: \"NNEMBs at SemEval-2017 Task 4: Neural Twitter Sentiment Classification: a Simple Ensemble Method with Different Embeddings\"", "SemEval: Leon Rotim, Martin Tutek, Jan \u0160najder: \"TakeLab at SemEval-2017 Task 5: Linear aggregation of word embeddings for fine-grained sentiment analysis of financial news\"", "SemEval: Omar Enayet, Samhaa R. El-Beltagy: \"NileTMRG at SemEval-2017 Task 8: Determining Rumour and Veracity Support for Rumours on Twitter.\"", "SemEval: V\u00edctor Su\u00e1rez-Paniagua, Isabel Segura-Bedmar, Paloma Mart\u00ednez: \"LABDA at SemEval-2017 Task 10: Relation Classification between keyphrases via Convolutional Neural Network\"", "SemEval: Samhaa R. El-Beltagy, Mona El kalamawy, Abu Bakr Soliman: \"NileTMRG at SemEval-2017 Task 4: Arabic Sentiment Analysis\"", "SemEval: Joe Barrow, Denis Peskov: \"UMDeep at SemEval-2017 Task 1: End-to-End Shared Weight LSTM Model for Semantic Textual Similarity\"", "SemEval: Animesh Prasad, Min-Yen Kan: \"WING-NUS at SemEval-2017 Task 10: Keyphrase Extraction and Classification as Joint Sequence Labeling\"", "SemEval: Vineet John, Olga Vechtomova: \"UW-FinSent at SemEval-2017 Task 5: Sentiment Analysis on Financial News Headlines using Training Dataset Augmentation\"", "SemEval: Elena Kochkina, Maria Liakata, Isabelle Augenstein: \"Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM\"", "SemEval: Liang Wang, Sujian Li: \"PKU\\_ICL at SemEval-2017 Task 10: Keyphrase Extraction with Model Ensemble and External Knowledge\"", "SemEval: Marianela Garc\u00eda Lozano, Hanna Lilja, Edward Tj\u00f6rnhammar, Maja Karasalo: \"Mama Edha at SemEval-2017 Task 8: Stance Classification with CNN and Rules\"", "SemEval: Haowei Zhang, Jin Wang, Jixian Zhang, Xuejie Zhang: \"YNU-HPCC at SemEval 2017 Task 4: Using A Multi-Channel CNN-LSTM Model for Sentiment Classification\"", "SemEval: John Henderson, Elizabeth Merkhofer, Laura Strickhart, Guido Zarrella: \"MITRE at SemEval-2017 Task 1: Simple Semantic Similarity\"", "SemEval: Sudipta Kar, Suraj Maharjan, Thamar Solorio: \"RiTUAL-UH at SemEval-2017 Task 5: Sentiment Analysis on Financial Data Using Neural Networks\"", "SemEval: Nada Almarwani, Mona Diab: \"GW\\_QA at SemEval-2017 Task 3: Question Answer Re-ranking on Arabic Fora\"", "SemEval: Asma Ben Abacha, Dina Demner-Fushman: \"NLM\\_NIH at SemEval-2017 Task 3: from Question Entailment to Question Similarity for Community Question Answering\"", "SemEval: Amit Ajit Deshmane, Jasper Friedrichs: \"TSA-INF at SemEval-2017 Task 4: An Ensemble of Deep Learning Architectures Including Lexicon Features for Twitter Sentiment Analysis\"", "SemEval: Ankit Srivastava, Georg Rehm, Julian Moreno Schneider: \"DFKI-DKT at SemEval-2017 Task 8: Rumour Detection and Classification using Cascading Heuristics\"", "SemEval: Andrew Cattle, Xiaojuan Ma: \"SRHR at SemEval-2017 Task 6: Word Associations for Humour Recognition\"", "SemEval: Raj Kumar Gupta, Yinping Yang: \"CrystalNest at SemEval-2017 Task 4: Using Sarcasm Detection for Enhancing Sentiment Classification and Quantification\"", "SemEval: Ji Young Lee, Franck Dernoncourt, Peter Szolovits: \"MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional Neural Networks\"", "SemEval: Junfeng Tian, Zhiheng Zhou, Man Lan, Yuanbin Wu: \"ECNU at SemEval-2017 Task 1: Leverage Kernel-based Traditional NLP features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity\"", "SemEval: David Donahue, Alexey Romanov, Anna Rumshisky: \"HumorHawk at SemEval-2017 Task 6: Mixing Meaning and Sound for Humor Recognition\"", "SemEval: Tomoki Tsujimura, Makoto Miwa, Yutaka Sasaki: \"TTI-COIN at SemEval-2017 Task 10: Investigating Embeddings for End-to-End Relation Extraction from Scientific Papers\"", "SemEval: Jos\u00e9 Abreu, Iv\u00e1n Castro, Claudia Mart\u00ednez, Sebasti\u00e1n Oliva, Yoan Guti\u00e9rrez: \"UCSC-NLP at SemEval-2017 Task 4: Sense n-grams for Sentiment Analysis in Twitter\"", "SemEval: Kim Schouten, Flavius Frasincar, Franciska de Jong: \"COMMIT at SemEval-2017 Task 5: Ontology-based Method for Sentiment Analysis of Financial Headlines\"", "SemEval: Artuur Leeuwenberg, Marie-Francine Moens: \"KULeuven-LIIR at SemEval-2017 Task 12: Cross-Domain Temporal Information Extraction from Clinical Records\"", "SemEval: Niloofar Ranjbar, Fatemeh Mashhadirajab, Mehrnoush Shamsfard, Rayeheh Hosseini pour, Aryan Vahid pour: \"Mahtab at SemEval-2017 Task 2: Combination of Corpus-based and Knowledge-based Methods to Measure Semantic Word Similarity\"", "SemEval: I-Ta Lee, Mahak Goindani, Chang Li, Di Jin, Kristen Marie Johnson, Xiao Zhang, Maria Leonor Pacheco, Dan Goldwasser: \"PurdueNLP at SemEval-2017 Task 1: Predicting Semantic Textual Similarity with Paraphrase and Event Embeddings\"", "SemEval: Yuta Koreeda, Takuya Hashito, Yoshiki Niwa, Misa Sato, Toshihiko Yanase, Kenzo Kurotsuchi, Kohsuke Yanai: \"bunji at SemEval-2017 Task 3: Combination of Neural Similarity Features and Comment Plausibility Features\"", "SemEval: Wenzheng Feng, Yu Wu, Wei Wu, Zhoujun Li, Ming Zhou: \"Beihang-MSRA at SemEval-2017 Task 3: A Ranking System with Neural Matching Features for Community Question Answering\"", "SemEval: Iuliana Alexandra Fle\u0219can-Lovin-Arseni, Ramona Andreea Turcu, Cristina Sirbu, Larisa Alexa, Sandra Maria Amarandei, Nichita Herciu, Constantin Scutaru, Diana Trandabat, Adrian Iftene: \"\\#WarTeam at SemEval-2017 Task 6: Using Neural Networks for Discovering Humorous Tweets\"", "SemEval: Claudio Delli Bovi, Alessandro Raganato: \"Sew-Embed at SemEval-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched Wikipedia\"", "SemEval: Marwan Torki, Maram Hasanain, Tamer Elsayed: \"QU-BIGIR at SemEval 2017 Task 3: Using Similarity Features for Arabic Community Question Answering Forums\"", "SemEval: Guoshun Wu, Yixuan Sheng, Man Lan, Yuanbin Wu: \"ECNU at SemEval-2017 Task 3: Using Traditional and Deep Learning Methods to Address Community Question Answering Task\"", "SemEval: Feixiang Wang, Man Lan, Yuanbin Wu: \"ECNU at SemEval-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models\"", "SemEval: Mengxiao Jiang, Man Lan, Yuanbin Wu: \"ECNU at SemEval-2017 Task 5: An Ensemble of Regression Algorithms with Effective Features for Fine-Grained Sentiment Analysis in Financial Domain\"", "SemEval: Yunxiao Zhou, Man Lan, Yuanbin Wu: \"ECNU at SemEval-2017 Task 4: Evaluating Effective Features on Machine Learning Methods for Twitter Message Polarity Classification\"", "SemEval: R\u0103zvan-Gabriel Rotari, Ionut Hulub, Stefan Oprea, Mihaela Plamada-Onofrei, Alina Beatrice Lorent, Raluca Preisler, Adrian Iftene, Diana Trandabat: \"Wild Devs' at SemEval-2017 Task 2: Using Neural Networks to Discover Word Similarity\"", "SemEval: Yuhuan Xiu, Man Lan, Yuanbin Wu: \"ECNU at SemEval-2017 Task 7: Using Supervised and Unsupervised Methods to Detect and Locate English Puns\"", "SemEval: Vijayasaradhi Indurthi, Subba Reddy Oota: \"Fermi at SemEval-2017 Task 7: Detection and Interpretation of Homographic puns in English Language\"", "SemEval: Pablo Gamallo: \"Citius at SemEval-2017 Task 2: Cross-Lingual Similarity from Comparable Corpora and Dependency-Based Contexts\"", "SemEval: Surya Agustian, Hiroya Takamura: \"UINSUSKA-TiTech at SemEval-2017 Task 3: Exploiting Word Importance Levels for Similarity Features for CQA\"", "SemEval: Vikram Singh, Sunny Narayan, Md Shad Akhtar, Asif Ekbal, Pushpak Bhattacharyya: \"IITP at SemEval-2017 Task 8 : A Supervised Approach for Rumour Evaluation\"", "SemEval: Ergun Bi\u00e7ici: \"RTM at SemEval-2017 Task 1: Referential Translation Machines for Predicting Semantic Similarity\"", "SemEval: G\u00e1bor Berend: \"SZTE-NLP at SemEval-2017 Task 10: A High Precision Sequence Model for Keyphrase Extraction Utilizing Sparse Coding for Feature Generation\"", "SemEval: Ignacio Arroyo-Fern\u00e1ndez, Ivan Vladimir Meza Ruiz: \"LIPN-IIMAS at SemEval-2017 Task 1: Subword Embeddings, Attention Recurrent Neural Networks and Cross Word Alignment for Semantic Textual Similarity\"", "SemEval: Abhishek Kumar, Abhishek Sethi, Md Shad Akhtar, Asif Ekbal, Chris Biemann, Pushpak Bhattacharyya: \"IITPB at SemEval-2017 Task 5: Sentiment Prediction in Financial Text\"", "SemEval: Deepanway Ghosal, Shobhit Bhatnagar, Md Shad Akhtar, Asif Ekbal, Pushpak Bhattacharyya: \"IITP at SemEval-2017 Task 5: An Ensemble of Deep Learning and Feature Based Models for Financial Sentiment Analysis\"", "SemEval: Pedro Saleiro, Eduarda Mendes Rodrigues, Carlos Soares, Eug\u00e9nio Oliveira: \"FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings\"", "SemEval: Byron Galbraith, Bhanu Pratap, Daniel Shank: \"Talla at SemEval-2017 Task 3: Identifying Similar Questions Through Paraphrase Detection\"", "SemEval: Rutal Mahajan, Mukesh Zaveri: \"SVNIT @ SemEval 2017 Task-6: Learning a Sense of Humor Using Supervised Approach\"", "SemEval: Salud Mar\u00eda Jim\u00e9nez-Zafra, Arturo Montejo-R\u00e1ez, Maite Martin, L. Alfonso Urena Lopez: \"SINAI at SemEval-2017 Task 4: User based classification\"", "SemEval: Marek Kubis, Pawe\u0142 Sk\u00f3rzewski, Tomasz Zi\u0119tkiewicz: \"EUDAMU at SemEval-2017 Task 11: Action Ranking and Type Matching for End-User Development\"", "SemEval: Jose Camacho-Collados, Mohammad Taher Pilehvar, Nigel Collier, Roberto Navigli: \"SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity\"", "SemEval: Leon Derczynski, Kalina Bontcheva, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, Arkaitz Zubiaga: \"SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours\"", "SemEval: Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, Andrew McCallum: \"SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications\"", "SemEval: Tristan Miller, Christian Hempelmann, Iryna Gurevych: \"SemEval-2017 Task 7: Detection and Interpretation of English Puns\"", "SemEval: Keith Cortis, Andr\u00e9 Freitas, Tobias Daudert, Manuela Huerlimann, Manel Zarrouk, Siegfried Handschuh, Brian Davis: \"SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News\"", "SemEval: Jonathan May, Jay Priyadarshi: \"SemEval-2017 Task 9: Abstract Meaning Representation Parsing and Generation\"", "SemEval: Sara Rosenthal, Noura Farra, Preslav Nakov: \"SemEval-2017 Task 4: Sentiment Analysis in Twitter\"", "SemEval: Steven Bethard, Guergana Savova, Martha Palmer, James Pustejovsky: \"SemEval-2017 Task 12: Clinical TempEval\"", "SemEval: Peter Potash, Alexey Romanov, Anna Rumshisky: \"SemEval-2017 Task 6: \\#HashtagWars: Learning a Sense of Humor\"", "SemEval: Youness Mansar, Lorenzo Gatti, Sira Ferradans, Marco Guerini, Jacopo Staiano: \"Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines\"", "SemEval: Simon David Hernandez, Davide Buscaldi, Thierry Charnois: \"LIPN at SemEval-2017 Task 10:  Filtering Candidate Keyphrases from Scientific Publications with Part-of-Speech Tag Sequences to Train a Sequence Labeling Model\"", "SemEval: Juliano Sales, Siegfried Handschuh, Andr\u00e9 Freitas: \"SemEval-2017 Task 11: End-User Development using Natural Language\"", "SemEval: Preslav Nakov, Doris Hoogeveen, Llu\u00eds M\u00e0rquez, Alessandro Moschitti, Hamdy Mubarak, Timothy Baldwin, Karin Verspoor: \"SemEval-2017 Task 3: Community Question Answering\"", "SemEval: Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, Lucia Specia: \"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation\"", "SemEval: Mohammed R. H. Qwaider, Abed Alhakim Freihat, Fausto Giunchiglia: \"TrentoTeam at SemEval-2017 Task 3: An application of Grice Maxims in Ranking Community Question Answers\"", "SemEval: Abeed Sarker, Graciela Gonzalez: \"HLP@UPenn at SemEval-2017 Task 4A: A simple, self-optimizing text classification system combining dense and sparse vectors\"", "SemEval: Miguel J. Rodrigues, Francisco M Couto: \"MoRS at SemEval-2017 Task 3: Easy to use SVM in Ranking Tasks\"", "SemEval: Olga Vechtomova: \"UWaterloo at SemEval-2017 Task 7: Locating the Pun Using Syntactic Characteristics and Corpus-based Metrics\"", "SemEval: Hareesh Bahuleyan, Olga Vechtomova: \"UWaterloo at SemEval-2017 Task 8: Detecting Stance towards Rumours with Topic Independent Features\"", "SemEval: Elena Mikhalkova, Yuri Karyakin: \"PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in Automatic Pun Recognition and Interpretation\"", "SemEval: Angel Deborah S, S Milton Rajendram, T T Mirnalinee: \"SSN\\_MLRG1 at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis Using Multiple Kernel Gaussian Process Regression Model\"", "SemEval: Enkhzol Dovdon, Jos\u00e9 Saias: \"ej-sa-2017 at SemEval-2017 Task 4: Experiments for Target oriented Sentiment Analysis in Twitter\"", "SemEval: Po-Yu Huang, Hen-Hsen Huang, Yu-Wun Wang, Ching Huang, Hsin-Hsi Chen: \"NTU-1 at SemEval-2017 Task 12: Detection and classification of temporal events in clinical data with domain adaptation\"", "SemEval: Zarmeen Nasim: \"IBA-Sys at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News\"", "SemEval: Mathieu Cliche: \"BB\\_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs\"", "SemEval: Raphael Troncy, Enrico Palumbo, Efstratios Sygkounas, Giuseppe Rizzo: \"SentiME++ at SemEval-2017 Task 4: Stacking State-of-the-Art Classifiers to Enhance Sentiment Classification\"", "SemEval: Xinru Yan, Ted Pedersen: \"Duluth at SemEval-2017 Task 6: Language Models in Humor Detection\"", "SemEval: Yufei Xie, Maoquan Wang, Jing Ma, Jian Jiang, Zhao Lu: \"EICA Team at SemEval-2017 Task 3: Semantic and Metadata-based Features for Community Question Answering\"", "SemEval: Aniket Pramanick, Dipankar Das: \"JU CSE NLP @ SemEval 2017 Task 7: Employing Rules to Detect and Interpret English Puns\"", "SemEval: Khoa Nguyen, Dang Nguyen: \"UIT-DANGNT-CLNLP at SemEval-2017 Task 9: Building Scientific Concept Fixing Patterns for Improving CAMR\"", "SemEval: Alon Rozental, Daniel Fleischer: \"Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter\"", "SemEval: Yu Long, Zhijing Li, Xuan Wang, Chen Li: \"XJNLP at SemEval-2017 Task 12: Clinical temporal information ex-traction with a Hybrid Model\"", "SemEval: Naveen Kumar Laskari, Suresh Kumar Sanampudi: \"TWINA at SemEval-2017 Task 4: Twitter Sentiment Analysis with Ensemble Gradient Boost Tree Classifier\"", "SemEval: Giuseppe Attardi, Antonio Carta, Federico Errica, Andrea Madotto, Ludovica Pannitto: \"FA3L at SemEval-2017 Task 3: A ThRee Embeddings Recurrent Neural Network for Question Answering\"", "SemEval: Hala Mulki, Hatem Haddad, Mourad Gridach, Ismail Babao\u011flu: \"Tw-StAR at SemEval-2017 Task 4: Sentiment Classification of Arabic Tweets\"", "SemEval: Xiwu Han, Gregory Toner: \"QUB at SemEval-2017 Task 6: Cascaded Imbalanced Classification for Humor Analysis in Twitter\"", "SemEval: Chukwuyem Onyibe, Nizar Habash: \"OMAM at SemEval-2017 Task 4: English Sentiment Analysis with Conditional Random Fields\"", "SemEval: Athanasia Kolovou, Filippos Kokkinos, Aris Fergadis, Pinelopi Papalampidi, Elias Iosif, Nikolaos Malandrakis, Elisavet Palogiannidi, Haris Papageorgiou, Shrikanth Narayanan, Alexandros Potamianos: \"Tweester at SemEval-2017 Task 4: Fusion of Semantic-Affective and pairwise classification models for sentiment analysis in Twitter\"", "SemEval: Nikolay Karpov: \"NRU-HSE at SemEval-2017 Task 4: Tweet Quantification Using Deep Learning Architecture\"", "SemEval: Jan Buys, Phil Blunsom: \"Oxford at SemEval-2017 Task 9: Neural AMR Parsing with Pointer-Augmented Attention\"", "SemEval: Josu\u00e9 Melka, Gilles Bernard: \"Jmp8 at SemEval-2017 Task 2: A simple and general distributional approach to estimate word similarity\"", "SemEval: Waleed Ammar, Matthew Peters, Chandra Bhagavatula, Russell Power: \"The AI2 system at SemEval-2017 Task 10 (ScienceIE): semi-supervised end-to-end entity and relation extraction\"", "SemEval: Ramy Baly, Gilbert Badaro, Ali Hamdi, Rawan Moukalled, Rita Aoun, Georges El-Khoury, Ahmad Al Sallab, Hazem Hajj, Nizar Habash, Khaled Shaban, Wassim El-Hajj: \"OMAM at SemEval-2017 Task 4: Evaluation of English State-of-the-Art Sentiment Analysis Models for Arabic and a New Topic-based Model\"", "SemEval: Nabin Maharjan, Rajendra Banjade, Dipesh Gautam, Lasang J. Tamang, Vasile Rus: \"DT\\_Team at SemEval-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and Gaussian Mixture Model Output\"", "SemEval: Le Qi, Yu Zhang, Ting Liu: \"SCIR-QA at SemEval-2017 Task 3: CNN Model Based on Similar and Dissimilar Information between Keywords for Question Similarity\"", "SemEval: Naman Goyal: \"LearningToQuestion at SemEval 2017 Task 3: Ranking Similar Questions by Learning to Rank Using Rich Features\"", "SemEval: Yi-Chin Chen, Zhao-Yang Liu, Hung-Yu Kao: \"IKM at SemEval-2017 Task 8: Convolutional Neural Networks for stance detection and rumor verification\"", "SemEval: Pedro Fialho, Hugo Patinho Rodrigues, Lu\u00edsa Coheur, Paulo Quaresma: \"L2F/INESC-ID at SemEval-2017 Tasks 1 and 2: Lexical and semantic features in word and textual similarity\"", "SemEval: \u00d6zge Sevgili, Nima Ghotbi, Selma Tekir: \"N-Hance at SemEval-2017 Task 7: A Computational Approach using Word Association for Puns\"", "SemEval: Jingjing Zhao, Yan Yang, Bing Xu: \"MI\\\\&T Lab at SemEval-2017 task 4: An Integrated Training Method of Word Vector for Sentiment Classification\"", "SemEval: Delphine Charlet, Geraldine Damnati: \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity between Questions for Community Question Answering\"", "SemEval: Basma Hassan, Samir AbdelRahman, Reem Bahgat, Ibrahim Farag: \"FCICU at SemEval-2017 Task 1: Sense-Based Language Independent Semantic Textual Similarity Approach\"", "SemEval: Yang Shao: \"HCTI at SemEval-2017 Task 1: Use convolutional neural network to evaluate Semantic Textual Similarity\"", "SemEval: Edilson Anselmo Corr\u00eaa J\u00fanior, Vanessa Queiroz Marinho, Leandro Borges dos Santos: \"NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis\"", "SemEval: Erwin Marsi, Utpal Kumar Sikdar, Cristina Marco, Biswanath Barik, Rune S\u00e6tre: \"NTNU-1@ScienceIE at SemEval-2017 Task 10: Identifying and Labelling Keyphrases with Conditional Random Fields\"", "SemEval: Mohammed Jabreel, Antonio Moreno: \"SiTAKA at SemEval-2017 Task 4: Sentiment Analysis in Twitter Based on a Rich Set of Features\"", "SemEval: Hussam Hamdan: \"Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification\"", "SemEval: El Moatez Billah NAGOUDI, J\u00e9r\u00e9my Ferrero, Didier Schwab: \"LIM-LIG at SemEval-2017 Task1: Enhancing the Semantic Similarity for Arabic Sentences with Vectors Weighting\"", "SemEval: Tobias Cabanski, Julia Romberg, Stefan Conrad: \"HHU at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Data using Machine Learning Methods\"", "SemEval: Martyna \u015apiewak, Piotr Sobecki, Daniel Kara\u015b: \"OPI-JSA at SemEval-2017 Task 1: Application of Ensemble learning for computing semantic textual similarity\"", "SemEval: Symeon Symeonidis, Dimitrios Effrosynidis, John Kordonis, Avi Arampatzis: \"DUTH at SemEval-2017 Task 4: A Voting Classification Approach for Twitter Sentiment Analysis\"", "SemEval: Angel Deborah S, S Milton Rajendram, T T Mirnalinee: \"SSN\\_MLRG1 at SemEval-2017 Task 4: Sentiment Analysis in Twitter Using Multi-Kernel Gaussian Process Classifier\"", "SemEval: Tiago Zini, Karin Becker, Marcelo Dias: \"INF-UFRGS at SemEval-2017 Task 5: A Supervised Identification of Sentiment Score in Tweets and Headlines\"", "SemEval: J\u00e9r\u00e9my Ferrero, Laurent Besacier, Didier Schwab, Fr\u00e9d\u00e9ric Agn\u00e8s: \"CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity\"", "SemEval: Titas Nandi, Chris Biemann, Seid Muhie Yimam, Deepak Gupta, Sarah Kohail, Asif Ekbal, Pushpak Bhattacharyya: \"IIT-UHH at SemEval-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification\"", "SemEval: Cristina Espa\u00f1a-Bonet, Alberto Barr\u00f3n-Cede\u00f1o: \"Lump at SemEval-2017 Task 1: Towards an Interlingua Semantic Similarity\"", "SemEval: Fanqing Meng, Wenpeng Lu, Yuteng Zhang, Jinyong Cheng, Yuehan Du, Shuwang Han: \"QLUT at SemEval-2017 Task 1: Semantic Textual Similarity Based on Word Embeddings\"", "SemEval: Llu\u00eds-F. Hurtado, Encarna Segarra, Ferran Pla, Pascual Carrasco, Jos\u00e9-\u00c1ngel Gonz\u00e1lez: \"ELiRF-UPV at SemEval-2017 Task 7: Pun Detection and Interpretation\"", "SemEval: Dieke Oele, Kilian Evang: \"BuzzSaw at SemEval-2017 Task 7: Global vs. Local Context for Interpreting and Locating Homographic English Puns with Sense Embeddings\"", "SemEval: Ming Wang, Biao Chu, Qingxun Liu, Xiaobing Zhou: \"YNUDLG at SemEval-2017 Task 4: A GRU-SVM Model for Sentiment Classification and Quantification in Twitter\"", "SemEval: Fanqing Meng, Wenpeng Lu, Yuteng Zhang, Ping Jian, Shumin Shi, Heyan Huang: \"QLUT at SemEval-2017 Task 2: Word Similarity Based on Word Embedding and Knowledge Base\"", "SemEval: Steffen Eger, Erik-L\u00e2n Do Dinh, Ilia Kuznetsov, Masoud Kiaeeha, Iryna Gurevych: \"EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION\"", "SemEval: Hao Wu, Heyan Huang, Ping Jian, Yuhang Guo, Chao Su: \"BIT at SemEval-2017 Task 1: Using Semantic Information Space to Evaluate Semantic Textual Similarity\"", "SemEval: Hussein T. Al-Natsheh, Lucie Martinet, Fabrice Muhlenbach, Djamel Abdelkader ZIGHED: \"UdL at SemEval-2017 Task 1: Semantic Textual Similarity Estimation of English Sentence Pairs Using Regression Model over Pairwise Features\"", "SemEval: Johannes Bjerva, Robert \u00d6stling: \"ResSim at SemEval-2017 Task 1: Multilingual Word Representations for Semantic Textual Similarity\"", "SemEval: Wenjie Liu, Chengjie Sun, Lei Lin, Bingquan Liu: \"ITNLP-AiKF at SemEval-2017 Task 1: Rich Features Based SVR for Semantic Textual Similarity Computing\"", "SemEval: Amal Htait, S\u00e9bastien Fournier, Patrice Bellot: \"LSIS at SemEval-2017 Task 4: Using Adapted Sentiment Similarity Seed Words For English and Arabic Tweet Polarity Classification\"", "SemEval: Sheng Zhang, Jiajun Cheng, Hui Wang, Xin Zhang, Pei Li, Zhaoyun Ding: \"FuRongWang at SemEval-2017 Task 3: Deep Neural Networks for Selecting Relevant Answers in Community Question Answering\"", "SemEval: Isabel Segura-Bedmar, Crist\u00f3bal Col\u00f3n-Ruiz, Paloma Mart\u00ednez: \"LABDA at SemEval-2017 Task 10: Extracting Keyphrases from Scientific Publications by combining the BANNER tool and the UMLS Semantic Network\"", "SemEval: Lung-Hao Lee, Kuei-Ching Lee, Yuen-Hsien Tseng: \"The NTNU System at SemEval-2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications Using Multiple Conditional Random Fields\"", "SemEval: Simon Mille, Roberto Carlini, Alicia Burga, Leo Wanner: \"FORGe at SemEval-2017 Task 9: Deep sentence generation based on a sequence of graph transducers\"", "SemEval: Jos\u00e9-\u00c1ngel Gonz\u00e1lez, Ferran Pla, Llu\u00eds-F. Hurtado: \"ELiRF-UPV at SemEval-2017 Task 4: Sentiment Analysis using Deep Learning\"", "SemEval: Yazhou Hao, YangYang Lan, Yufei Li, Chen Li: \"XJSA at SemEval-2017 Task 4: A Deep System for Sentiment Classification in Twitter\"", "SemEval: WenLi Zhuang, Ernie Chang: \"Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model\"", "SemEval: Yassine El Adlouni, Imane Lahbari, Horacio Rodriguez, Mohammed Meknassi, Said Ouatik El Alaoui, Noureddine Ennahnahi: \"UPC-USMBA at SemEval-2017 Task 3: Combining multiple approaches for CQA for Arabic\"", "SemEval: Joosung Yoon, Kigon Lyu, Hyeoncheol Kim: \"Adullam at SemEval-2017 Task 4: Sentiment Analyzer Using Lexicon Integrated Convolutional Neural Networks with Attention\"", "SemEval: wang maoquan, Chen Shiyun, Xie yufei, Zhao lu: \"EICA at SemEval-2017 Task 4: A Simple Convolutional Neural Network for Topic-based Sentiment Classification\"", "SemEval: Normunds Gruzitis, Didzis Gosko, Guntis Barzdins: \"RIGOTRIO at SemEval-2017 Task 9: Combining Machine Learning and Grammar Engineering for AMR Parsing and Generation\"", "SemEval: Quanzhi Li, Armineh Nourbakhsh, Xiaomo Liu, Rui Fang, Sameena Shah: \"funSentiment at SemEval-2017 Task 4: Topic-Based Message Sentiment Classification by Exploiting  Word  Embeddings, Text Features and Target Contexts\"", "SemEval: Simone Filice, Giovanni Da San Martino, Alessandro Moschitti: \"KeLP at SemEval-2017 Task 3: Learning Pairwise Patterns in Community Question Answering\"", "SemEval: Andre Lamurias, Diana Sousa, Sofia Pereira, Luka Clarke, Francisco M Couto: \"ULISBOA at SemEval-2017 Task 12: Extraction and classification of temporal expressions and events\"", "SemEval: Lidia Pivovarova, Lloren\u00e7 Escoter, Arto Klami, Roman Yangarber: \"HCS at SemEval-2017 Task 5: Polarity detection in business news using convolutional neural networks\"", "SemEval: Christos Baziotis, Nikos Pelekis, Christos Doulkeridis: \"DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis\"", "SemEval: Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen: \"NLG301 at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News\"", "SemEval: Sijia Liu, Feichen Shen, Vipin Chaudhary, Hongfang Liu: \"MayoNLP at SemEval 2017 Task 10: Word Embedding Distance Pattern for Keyphrase Classification in Scientific Publications\"", "shortpapers: Changxing Wu, xiaodong shi, Yidong Chen, jinsong su, Boli Wang: \"Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings\"", "shortpapers: Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan Chen, Weipeng Zhao, Haiqing Chen, Jun Huang, Wei Chu: \"AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine\"", "shortpapers: Sam Wei, Igor Korostil, Joel Nothman, Ben Hachey: \"English Event Detection With Translated Language Features\"", "shortpapers: Raymond Hendy Susanto, Wei Lu: \"Neural Architectures for Multilingual Semantic Parsing\"", "shortpapers: Nedelina Teneva, Weiwei Cheng: \"Salience Rank: Efficient Keyphrase Extraction with Topic Modeling\"", "shortpapers: Ying Lin, Chin-Yew Lin, Heng Ji: \"List-only Entity Linking\"", "shortpapers: Ye Zhang, Matthew Lease, Byron C. Wallace: \"Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization\"", "shortpapers: Chenhui Chu, Raj Dabre, Sadao Kurohashi: \"An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation\"", "shortpapers: Michaeel Kazi, Brian Thompson: \"Implicitly-Defined Neural Networks for Sequence Labeling\"", "shortpapers: Xing Shi, Kevin Knight: \"Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary\"", "shortpapers: Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu, Feiyue Huang: \"Fast and Accurate Neural Word Segmentation for Chinese\"", "shortpapers: Spandana Gella, Frank Keller: \"An Analysis of Action Recognition Datasets for Language and Vision Tasks\"", "shortpapers: Glorianna Jagfeld, Patrick Ziering, Lonneke van der Plas: \"Evaluating Compound Splitters Extrinsically with Textual Entailment\"", "shortpapers: Lei Shu, Hu Xu, Bing Liu: \"Lifelong Learning CRF for Supervised Aspect Extraction\"", "shortpapers: Lena Reed, Jiaqi Wu, Shereen Oraby, Pranav Anand, Marilyn Walker: \"Learning Lexico-Functional Patterns for First-Person Affect\"", "shortpapers: Xiang Yu, Ngoc Thang Vu: \"Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages\"", "shortpapers: Jianpeng Cheng, Adam Lopez, Mirella Lapata: \"A Generative Parser with a Discriminative Recognition Algorithm\"", "shortpapers: Rui Wang, Andrew Finch, Masao Utiyama, Eiichiro Sumita: \"Sentence Embedding for Neural Machine Translation Domain Adaptation\"", "shortpapers: Yuya Yoshikawa, Yutaro Shigeto, Akikazu Takeuchi: \"STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset\"", "shortpapers: Terrence Szymanski: \"Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings\"", "shortpapers: Shervin Malmasi, Mark Dras: \"Feature Hashing for Language and Dialect Identification\"", "shortpapers: Akihiko Kato, Hiroyuki Shindo, Yuji Matsumoto: \"English Multiword Expression-aware Dependency Parsing Including Named Entities\"", "shortpapers: Benjamin Marie, Atsushi Fujita: \"Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings\"", "shortpapers: Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yansong Feng, Dongyan Zhao: \"How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models\"", "shortpapers: Paria Jamshid Lou, Mark Johnson: \"Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model\"", "shortpapers: Claudio Delli Bovi, Jose Camacho-Collados, Alessandro Raganato, Roberto Navigli: \"EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text\"", "shortpapers: \u017deljko Agi\u0107, Natalie Schluter: \"How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers\"", "shortpapers: Xueying Zhan, Yaowei Wang, Yanghui Rao, Haoran Xie, Qing Li, Fu Lee Wang, Tak-Lam Wong: \"A Network Framework for Noisy Label Aggregation in Social Media\"", "shortpapers: Jonathan Herzig, Jonathan Berant: \"Neural Semantic Parsing over Multiple Knowledge-bases\"", "shortpapers: Sanja \u0160tajner, Marc Franco-Salvador, Simone Paolo Ponzetto, Paolo Rosso, Heiner Stuckenschmidt: \"Sentence Alignment Methods for Improving Text Simplification Systems\"", "shortpapers: Quan Hung Tran, Gholamreza Haffari, Ingrid Zukerman: \"A Generative Attentional Neural Network Model for Dialogue Act Classification\"", "shortpapers: Thomas Kober, Julie Weeds, Jeremy Reffin, David Weir: \"Improving Semantic Composition with Offset Inference\"", "shortpapers: Swarnadeep Saha, Harinder Pal, {} Mausam: \"Bootstrapping for Numerical Open IE\"", "shortpapers: Mohammed Elrazzaz, Shady Elbassuoni, Khaled Shaban, Chadi Helwe: \"Methodical Evaluation of Arabic Word Embeddings\"", "shortpapers: Nafise Sadat Moosavi, Michael Strube: \"Lexical Features in Coreference Resolution: To be Used With Caution\"", "shortpapers: Long Zhou, Wenpeng Hu, Jiajun Zhang, Chengqing Zong: \"Neural System Combination for Machine Translation\"", "shortpapers: Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, Daniel Gildea: \"AMR-to-text Generation with Synchronous Node Replacement Grammar\"", "shortpapers: Andrey Malinin, Anton Ragni, Kate Knill, Mark Gales: \"Incorporating Uncertainty into Deep Learning for Spoken Language Assessment\"", "shortpapers: Shuming Ma, Xu Sun, Jingjing Xu, Houfeng WANG, Wenjie Li, Qi Su: \"Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization\"", "shortpapers: Henning Wachsmuth, Nona Naderi, Ivan Habernal, Yufang Hou, Graeme Hirst, Iryna Gurevych, Benno Stein: \"Argumentation Quality Assessment: Theory vs. Practice\"", "shortpapers: Chlo\u00e9 Braud, Oph\u00e9lie Lacroix, Anders S\u00f8gaard: \"Cross-lingual and cross-domain discourse segmentation of entire documents\"", "shortpapers: Bogdan Ludusan, Reiko Mazuka, Mathieu Bernard, Alejandrina Cristia, Emmanuel Dupoux: \"The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective\"", "shortpapers: Weiyue Wang, Tamer Alkhouli, Derui Zhu, Hermann Ney: \"Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation\"", "shortpapers: Marzieh Fadaee, Arianna Bisazza, Christof Monz: \"Learning Topic-Sensitive Word Representations\"", "shortpapers: Marzieh Fadaee, Arianna Bisazza, Christof Monz: \"Data Augmentation for Low-Resource Neural Machine Translation\"", "shortpapers: Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi Niu, Yang Zhao, Akiko Aizawa, Guoping Long: \"A Conditional Variational Framework for Dialog Generation\"", "shortpapers: Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy Puzikov, Ido Dagan, Iryna Gurevych: \"Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets\"", "shortpapers: Paramita Mirza, Simon Razniewski, Fariz Darari, Gerhard Weikum: \"Cardinal Virtues: Extracting Relation Cardinalities from Text\"", "shortpapers: Yizhong Wang, Sujian Li, Houfeng WANG: \"A Two-Stage Parsing Method for Text-Level Discourse Analysis\"", "shortpapers: Hongyu GUO: \"A Deep Network with Visual Text Composition Behavior\"", "shortpapers: Maxim Rabinovich, Dan Klein: \"Fine-Grained Entity Typing with High-Multiplicity Assignments\"", "shortpapers: Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho: \"Learning to Parse and Translate Improves Neural Machine Translation\"", "shortpapers: Youxuan Jiang, Jonathan K. Kummerfeld, Walter S. Lasecki: \"Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection\"", "shortpapers: William Yang Wang: \"''Liar, Liar Pants on Fire'': A New Benchmark Dataset for Fake News Detection\"", "shortpapers: Svetlana Kiritchenko, Saif Mohammad: \"Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation\"", "shortpapers: Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Ahmed Abdelali, Yonatan Belinkov, Stephan Vogel: \"Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging\"", "shortpapers: Oren Melamud, Jacob Goldberger: \"Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function\"", "shortpapers: Maxime Peyrard, Judith Eckle-Kohler: \"A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments\"", "shortpapers: Keisuke Sakaguchi, Matt Post, Benjamin Van Durme: \"Error-repair Dependency Parsing for Ungrammatical Texts\"", "shortpapers: Beata Beigman Klebanov, Binod Gyawali, Yi Song: \"Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?\"", "shortpapers: Samuel R\u00f6nnqvist, Niko Schenk, Christian Chiarcos: \"A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations\"", "shortpapers: Isabelle Augenstein, Anders S\u00f8gaard: \"Multi-Task Learning of Keyphrase Boundary Classification\"", "shortpapers: Mohammed Hasanuzzaman, Sabyasachi Kamila, Mandeep Kaur, Sriparna Saha, Asif Ekbal: \"Temporal Orientation of Tweets for Predicting Income of Users\"", "shortpapers: Jiaqi Mu, Suma Bhat, Pramod Viswanath: \"Representing Sentences as Low-Rank Subspaces\"", "shortpapers: Xinhao Wang, James Bruno, Hillary Molloy, Keelan Evanini, Klaus Zechner: \"Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework\"", "shortpapers: Julien Tourille, Olivier Ferret, Aurelie Neveol, Xavier Tannier: \"Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative Containers\"", "shortpapers: Fatemeh Almodaresi, Lyle Ungar, Vivek Kulkarni, Mohsen Zakeri, Salvatore Giorgi, H. Andrew Schwartz: \"On the Distribution of Lexical Features at Multiple Levels of Analysis\"", "shortpapers: Youngseo Son, Anneke Buffone, Joe Raso, Allegra Larche, Anthony Janocko, Kevin Zembroski, H. Andrew Schwartz, Lyle Ungar: \"Recognizing Counterfactual Thinking in Social Media Texts\"", "shortpapers: Azad Abad, Moin Nabi, Alessandro Moschitti: \"Self-Crowdsourcing Training for Relation Extraction\"", "shortpapers: Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, Nathan Hodas: \"Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter\"", "shortpapers: Lingzhen Chen, Carlo Strapparava, Vivi Nastase: \"Improving Native Language Identification by Using Spelling Errors\"", "shortpapers: Peng Qi, Christopher D. Manning: \"Arc-swift: A Novel Transition System for Dependency Parsing\"", "shortpapers: Meng Fang, Trevor Cohn: \"Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary\"", "shortpapers: Milo\u0161 Stanojevi\u0107, Khalil Sima'an: \"Alternative Objective Functions for Training MT Evaluation Metrics\"", "shortpapers: Xinyu Hua, Lu Wang: \"Understanding and Detecting Diverse Supporting Arguments on Controversial Issues\"", "shortpapers: Travis Wolfe, Mark Dredze, Benjamin Van Durme: \"Pocket Knowledge Base Population\"", "shortpapers: Afshin Rahimi, Trevor Cohn, Timothy Baldwin: \"A Neural Model for User Geolocation and Lexical Dialectology\"", "shortpapers: Yow-Ting Shiue, Hen-Hsen Huang, Hsin-Hsi Chen: \"Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM\"", "shortpapers: Tushar Khot, Ashish Sabharwal, Peter Clark: \"Answering Complex Questions Using Open Information Extraction\"", "shortpapers: Katsuhiko Hayashi, Masashi Shimbo: \"On the Equivalence of Holographic and Complex Embeddings for Link Prediction\"", "shortpapers: Hannah Rashkin, Eric Bell, Yejin Choi, Svitlana Volkova: \"Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast\"", "shortpapers: Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi: \"A Corpus of Natural Language for Visual Reasoning\"", "shortpapers: Alymzhan Toleu, Gulmira Tolegen, Aibek Makazhanov: \"Character-Aware Neural Morphological Disambiguation\"", "shortpapers: Hao Zhou, Zhaopeng Tu, Shujian Huang, Xiaohua Liu, Hang Li, Jiajun Chen: \"Chunk-Based Bi-Scale Decoder for Neural Machine Translation\"", "shortpapers: Emily Prud'hommeaux, Jan van Santen, Douglas Gliner: \"Vector space models for evaluating semantic fluency in autism\"", "shortpapers: Fei Cheng, Yusuke Miyao: \"Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths\"", "shortpapers: Kartik Goyal, Chris Dyer, Taylor Berg-Kirkpatrick: \"Differentiable Scheduled Sampling for Credit Assignment\"", "shortpapers: Sewon Min, Minjoon Seo, Hannaneh Hajishirzi: \"Question Answering through Transfer Learning from Large Fine-grained Supervision Data\"", "shortpapers: Rajarshi Das, Manzil Zaheer, Siva Reddy, Andrew McCallum: \"Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks\"", "shortpapers: Tsutomu Hirao, Masaaki Nishino, Masaaki Nagata: \"Oracle Summaries of Compressive Summarization\"", "shortpapers: Maria Ryskina, Hannah Alpert-Abrams, Dan Garrette, Taylor Berg-Kirkpatrick: \"Automatic Compositor Attribution in the First Folio of Shakespeare\"", "shortpapers: Krishna Chaitanya Sanagavarapu, Alakananda Vempala, Eduardo Blanco: \"Determining Whether and When People Participate in the Events They Tweet About\"", "shortpapers: Mingbo Ma, Liang Huang, Bing Xiang, Bowen Zhou: \"Group Sparse CNNs for Question Classification with Answer Sets\"", "shortpapers: Prashanth Vijayaraghavan, Soroush Vosoughi, Deb Roy: \"Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning\"", "shortpapers: Zheng Cai, Lifu Tu, Kevin Gimpel: \"Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task\"", "shortpapers: Sergiu Nisioi, Sanja \u0160tajner, Simone Paolo Ponzetto, Liviu P. Dinu: \"Exploring Neural Text Simplification Models\"", "shortpapers: Denis Savenkov, Eugene Agichtein: \"EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering\"", "shortpapers: Daniel Dahlmeier: \"On the Challenges of Translating NLP Research into Commercial Products\"", "shortpapers: Pablo Loyola, Edison Marrese-Taylor, Yutaka Matsuo: \"A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes\"", "shortpapers: David Jurgens, Yulia Tsvetkov, Dan Jurafsky: \"Incorporating Dialectal Variability for Socially Equitable Language Identification\"", "shortpapers: Shun Hasegawa, Yuta Kikuchi, Hiroya Takamura, Manabu Okumura: \"Japanese Sentence Compression with a Large Training Dataset\"", "shortpapers: Rob van der Goot, Gertjan van Noord: \"Parser Adaptation for Social Media by Integrating Normalization\"", "shortpapers: Roee Aharoni, Yoav Goldberg: \"Towards String-To-Tree Neural Machine Translation\"", "shortpapers: Sunghwan Mac Kim, Qiongkai Xu, Lizhen Qu, Stephen Wan, Cecile Paris: \"Demographic Inference on Twitter using Recursive Neural Networks\"", "shortpapers: Alexandros Komninos, Suresh Manandhar: \"Feature-Rich Networks for Knowledge Base Completion\"", "shortpapers: Jind\u0159ich Libovick\u00fd, Jind\u0159ich Helcl: \"Attention Strategies for Multi-Source Sequence-to-Sequence Learning\"", "shortpapers: Daniel Fried, Mitchell Stern, Dan Klein: \"Improving Neural Parsing by Disentangling Model Combination and Reranking Effects\"", "SRW: Facundo Carrillo: \"Computational Characterization of Mental States: A Natural Language Processing Approach\"", "SRW: Nandan Sukthankar, Sanket Maharnawar, Pranay Deshmukh, Yashodhara Haribhakta, Vibhavari Kamble: \"nQuery - A Natural Language Statement to SQL Query Generator\"", "SRW: Nihal V. Nayak, Tanmay Chinchore, Aishwarya Hanumanth Rao, Shane Michael Martin, Sagar Nagaraj Simha, G. M. Lingaraju, H. S. Jamadagni: \"V for Vocab: An Intelligent Flashcard Application\"", "SRW: Sudha Rao: \"Are You Asking the Right Questions? Teaching Machines to Ask Clarification Questions\"", "SRW: Yui Suzuki, Tomoyuki Kajiwara, Mamoru Komachi: \"Building a Non-Trivial Paraphrase Corpus Using Multiple Machine Translation Systems\"", "SRW: Vasu Sharma, Ankita Bishnu, Labhesh Patel: \"Segmentation Guided Attention Networks for Visual Question Answering\"", "SRW: Kaixin Ma, Catherine Xiao, Jinho D. Choi: \"Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks\"", "SRW: Hang Li, Haozheng Wang, Zhenglu Yang, Masato Odagaki: \"Variation Autoencoder Based Network Representation Learning for Classification\"", "SRW: Paul Michel, Okko R\u00e4s\u00e4nen, Roland Thiolliere, Emmanuel Dupoux: \"Blind Phoneme Segmentation With Temporal Prediction Errors\"", "SRW: Srishti Aggarwal, Radhika Mamidi: \"Automatic Generation of Jokes in Hindi\"", "SRW: Haoran Zhang, Diane Litman: \"Word Embedding for Response-To-Text Assessment of Evidence\"", "SRW: Katira Soleymanzadeh: \"Domain Specific Automatic Question Generation from Text\"", "SRW: Jose Ramirez, Matthew Garber, Xinhao Wang: \"SoccEval: An Annotation Schema for Rating Soccer Players\"", "SRW: Matthew Garber, Meital Singer, Christopher Ward: \"Accent Adaptation for the Air Traffic Control Domain\"", "SRW: Tina Fang, Martin Jaggi, Katerina Argyraki: \"Generating Steganographic Text with LSTMs\"", "SRW: Misato Hiraga: \"Predicting Depression for Japanese Blog Text\"", "SRW: Petr Babkin, Sergei Nirenburg: \"Fast Forward Through Opportunistic Incremental Meaning Representation Construction\"", "SRW: Shoetsu Sato, Naoki Yoshinaga, Masashi Toyoda, Masaru Kitsuregawa: \"Modeling Situations in Neural Chat Bots\"", "SRW: Kurt Junshean Espinosa: \"An Empirical Study on End-to-End Sentence Modelling\"", "SRW: Noa Naaman, Hannah Provenza, Orion Montoya: \"Varying Linguistic Purposes of Emoji in (Twitter) Context\"", "SRW: Nan Wang: \"Negotiation of Antibiotic Treatment in Medical Consultations: A Corpus Based Study\"", "SRW: Ganesh Jawahar: \"Improving Distributed Representations of Tweets - Present and Future\"", "SRW: Jeenu Grover, Pabitra Mitra: \"Bilingual Word Embeddings with Bucketed CNN for Parallel Sentence Extraction\"", "starSEM: Ao Chen, Maosong Sun: \"Domain-Specific New Words Detection in Chinese\"", "starSEM: Sorcha Gilroy, Adam Lopez, Sebastian Maneth: \"Parsing Graphs with Regular Graph Grammars\"", "starSEM: Efsun Sarioglu Kayi, Mona Diab, Luca Pauselli, Michael Compton, Glen Coppersmith: \"Predictive Linguistic Features of Schizophrenia\"", "starSEM: Abhijeet Gupta, Gemma Boleda, Sebastian Pad\u00f3: \"Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible\"", "starSEM: Tibor Kiss, Francis Jeffry Pelletier, Halima Husic, Johanna Poppek: \"Issues of Mass and Count: Dealing with `Dual-Life' Nouns\"", "starSEM: Ekaterina Shutova, Andreas Wundsam, Helen Yannakoudakis: \"Semantic Frames and Visual Scenes: Learning Semantic Role Inventories from Image and Video Descriptions\"", "starSEM: Vered Shwartz, Gabriel Stanovsky, Ido Dagan: \"Acquiring Predicate Paraphrases from News Tweets\"", "starSEM: Saif Mohammad, Felipe Bravo-Marquez: \"Emotion Intensities in Tweets\"", "starSEM: Mrinmaya Sachan, Eric Xing: \"Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks\"", "starSEM: Simon Ostermann, Michael Roth, Stefan Thater, Manfred Pinkal: \"Aligning Script Events with Narrative Texts\"", "starSEM: Alon Talmor, Mor Geva, Jonathan Berant: \"Evaluating Semantic Parsing against a Simple Web-based Question Answering Model\"", "starSEM: Jo\u00e3o Ant\u00f3nio Rodrigues, Chakaveh Saedi, Vladislav Maraev, Jo\u00e3o Silva, Ant\u00f3nio Branco: \"Ways of Asking and Replying in Duplicate Question Detection\"", "starSEM: Maria Becker, Michael Staniek, Vivi Nastase, Alexis Palmer, Anette Frank: \"Classifying Semantic Clause Types: Modeling Context and Genre Characteristics with Recurrent Neural Networks and Attention\"", "starSEM: Sujay Kumar Jauhar, Eduard Hovy: \"Embedded Semantic Lexicon Induction with Joint Global and Local Optimization\"", "starSEM: Zoran Medi\u0107, Jan \u0160najder, Sebastian Pad\u00f3: \"Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for Croatian\"", "starSEM: Yukun Feng, Dong Yu, Jian Xu, Chunhua Liu: \"Semantic Frame Labeling with Target-based Neural Model\"", "starSEM: Emmanuele Chersoni, Alessandro Lenci, Philippe Blache: \"Logical Metonymy in a Distributional Model of Sentence Comprehension\"", "starSEM: Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi, Stefan Thater, Manfred Pinkal: \"A Mixture Model for Learning Multi-Sense Word Embeddings\"", "starSEM: Waseem Gharbieh, Virendrakumar Bhavsar, Paul Cook: \"Deep Learning Models For Multiword Expression Identification\"", "starSEM: Anna Rogers, Aleksandr Drozd, Bofang Li: \"The (too Many) Problems of Analogical Reasoning with Word Vectors\"", "starSEM: Jena D. Hwang, Archna Bhatia, Na-Rae Han, Tim O'Gorman, Vivek Srikumar, Nathan Schneider: \"Double Trouble: The Problem of Construal in Semantic Annotation of Adpositions\"", "starSEM: Anne Cocos, Marianna Apidianaki, Chris Callison-Burch: \"Mapping the Paraphrase Database to WordNet\"", "starSEM: Kathrin Eichler, Feiyu Xu, Hans Uszkoreit, Sebastian Krause: \"Generating Pattern-Based Entailment Graphs for Relation Extraction\"", "starSEM: Edoardo Maria Ponti, Ivan Vuli\u0107, Anna Korhonen: \"Decoding Sentiment from Distributed Representations of Sentences\"", "starSEM: Angel Maredia, Kara Schechtman, Sarah Ita Levitan, Julia Hirschberg: \"Comparing Approaches for Automatic Question Identification\"", "starSEM: Francis Ferraro, Adam Poliak, Ryan Cotterell, Benjamin Van Durme: \"Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles\"", "starSEM: Nabiha Asghar, Pascal Poupart, Xin Jiang, Hang Li: \"Deep Active Learning for Dialogue Generation\"", "starSEM: Yogarshi Vyas, Marine Carpuat: \"Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection\"", "starSEM: Sneha Rajana, Chris Callison-Burch, Marianna Apidianaki, Vered Shwartz: \"Learning Antonyms with Paraphrases and a Morphology-Aware Neural Network\"", "starSEM: Gregory Finley, Stephanie Farmer, Serguei Pakhomov: \"What Analogies Reveal about Word Vectors and their Compositionality\"", "TextGraphs-11: Jer\u00f3nimo Hern\u00e1ndez-Gonz\u00e1lez, Estevam R. Hruschka Jr., Tom M. Mitchell: \"Merging knowledge bases in different languages\"", "TextGraphs-11: Sakhar Alkhereyf, Owen Rambow: \"Work Hard, Play Hard: Email Classification on the Avocado and Enron Corpora\"", "TextGraphs-11: Vanessa Queiroz Marinho, Henrique Ferraz de Arruda, Thales Sinelli, Luciano da Fontoura Costa, Diego Raphael Amancio: \"On the ``Calligraphy'' of Books\"", "TextGraphs-11: Binny Mathew, Suman Kalyan Maity, Pratip Sarkar, Animesh Mukherjee, Pawan Goyal: \"Adapting predominant and novel sense discovery algorithms for identifying corpus-specific sense differences\"", "TextGraphs-11: Collin Baker, Michael Ellsworth: \"Graph Methods for Multilingual FrameNets\"", "TextGraphs-11: Amrith Krishna, Pavankumar Satuluri, Harshavardhan Ponnada, Muneeb Ahmed, Gulab Arora, Kaustubh Hiware, Pawan Goyal: \"A Graph Based Semi-Supervised Approach for Analysis of Derivational Nouns in Sanskrit\"", "TextGraphs-11: Jan Wira Gotama Putra, Takenobu Tokunaga: \"Evaluating text coherence based on semantic similarity graph\"", "TextGraphs-11: Kazuki Fukui, Takamasa Oshikiri, Hidetoshi Shimodaira: \"Spectral Graph-Based Method of Multimodal Word Embedding\"", "TextGraphs-11: Thomas Alexander Trost, Dietrich Klakow: \"Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings\"", "TextGraphs-11: Mir Tafseer Nayeem, Yllias Chali: \"Extract with Order for Coherent Multi-Document Summarization\""]}}; }
buildViz(1000,undefined,null,null,true,false,false,false,false,true,false,false,true,0.05,false,undefined,undefined);
</script>

